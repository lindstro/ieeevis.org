<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Paper: The Impact of Visual Segmentation on Lexical Word Recognition"><meta name="twitter:description" content="When a reader encounters a word in English, they split the word into smaller orthographic units in the process of recognizing
its meaning. For example, “rough”, when split according to phonemes, is decomposed as r-ou-gh (not as r-o-ugh or r-ough), where
each group of letters corresponds to a sound. Since there are many ways to segment a group of letters, this constitutes a computational
operation that has to be solved by the reading brain, many times per minute, in order to achieve the recognition of words in text
necessary for reading. In English, the irregular relationships between groups of letters and sounds, and the wide variety of possible
groupings make this operation harder than in more regular languages such as Italian. If this segmentation takes a significant amount of
time in the process of recognizing a word, it is conceivable that providing segmentation information in the text itself could help the
reading process by reducing its computational cost. In this paper we explore whether and how different visual interventions from the
visualization literature could communicate segmentation information for reading and word recognition. We ran a series of pre-registered
lexical decision experiments with 192 participants that tested five main types of visual segmentations: outlines, spacing, connections,
underlines and color. The evidence indicates that, even with a moderate amount of training, these visual interventions always slow
down word identification, but each to a different extent (between 32.7ms—color technique—and 70.7ms—connection technique).
These findings are important because they indicate that, at least for typical adult readers with a moderate amount of specific training in
these visual interventions, accelerating the lexical decision task is unlikely. Importantly, the results also offer an empirical measurement
of the cost of a common set of visual manipulations of text, which can be useful for practitioners seeking to visualize alongside or within
text without impacting reading performance. Finally, the interaction between typographically encoded information and visual variables
presented unique patterns that deviate from existing theories, suggesting new directions for future inquiry."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2025/paper_images/bf10121c-1a25-4391-b766-450e53b2348e_Image.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2025/paper_images/bf10121c-1a25-4391-b766-450e53b2348e_Image.png"><meta name="description" property="og:description" content="When a reader encounters a word in English, they split the word into smaller orthographic units in the process of recognizing
its meaning. For example, “rough”, when split according to phonemes, is decomposed as r-ou-gh (not as r-o-ugh or r-ough), where
each group of letters corresponds to a sound. Since there are many ways to segment a group of letters, this constitutes a computational
operation that has to be solved by the reading brain, many times per minute, in order to achieve the recognition of words in text
necessary for reading. In English, the irregular relationships between groups of letters and sounds, and the wide variety of possible
groupings make this operation harder than in more regular languages such as Italian. If this segmentation takes a significant amount of
time in the process of recognizing a word, it is conceivable that providing segmentation information in the text itself could help the
reading process by reducing its computational cost. In this paper we explore whether and how different visual interventions from the
visualization literature could communicate segmentation information for reading and word recognition. We ran a series of pre-registered
lexical decision experiments with 192 participants that tested five main types of visual segmentations: outlines, spacing, connections,
underlines and color. The evidence indicates that, even with a moderate amount of training, these visual interventions always slow
down word identification, but each to a different extent (between 32.7ms—color technique—and 70.7ms—connection technique).
These findings are important because they indicate that, at least for typical adult readers with a moderate amount of specific training in
these visual interventions, accelerating the lexical decision task is unlikely. Importantly, the results also offer an empirical measurement
of the cost of a common set of visual manipulations of text, which can be useful for practitioners seeking to visualize alongside or within
text without impacting reading performance. Finally, the interaction between typographically encoded information and visual variables
presented unique patterns that deviate from existing theories, suggesting new directions for future inquiry."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Paper: The Impact of Visual Segmentation on Lexical Word Recognition"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: The Impact of Visual Segmentation on Lexical Word Recognition</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-12"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="event_.html"></a></li><li class="breadcrumb-item"><a href="session_.html"></a></li><li class="breadcrumb-item active text-truncate" aria-current="page">The Impact of Visual Segmentation on Lexical Word Recognition</li></ol></nav><h1 class="paper-title">The Impact of Visual Segmentation on Lexical Word Recognition</h1><div class="checkbox-bookmark fas" style="font-size: 24pt;position: absolute; top:10px; right:20px;" data-tippy-content="(un-)bookmark this paper"> &#xf02e; </div><h4 class="paper-authors pb-2 mt-2"><span class="fas mr-1">&#xf183;</span> Matthew Termuende - </h4><h4 class="paper-authors pb-2 mt-2"><span class="fas mr-1">&#xf183;</span> Kevin Larson - </h4><h4 class="paper-authors pb-2 mt-2"><span class="fas mr-1">&#xf183;</span> Miguel Nacenta - </h4><h5><span class="fas" title="The authors made this paper screen-reader accessible in the IEEE Digital Library.">&#xf29a;</span> Screen-reader Accessible PDF </h5><h5 class="paper-link pb-2"><a href="https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1789-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc4OS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.cCIeKxG8EIJjp6eEwrcD6XGjlAyO2a32RTsz7A_ohyM"><span class="fas mr-1">&#xf15c;</span> Download camera-ready PDF </a></h5><h5 class="paper-link pb-2"><a href="https://doi.org/10.17605/OSF.IO/79Y5S"><span class="fas mr-1" title="This paper has additional material, like demos or experimental data, available online.">&#xf0c6;</span> Download Supplemental Material </a></h5></div></div><div class="row my-3"><div class="col-md-8"></div></div><div class="row my-3"><div class="col-md-8"><h5 class="paper-details-heading">Keywords</h5><p>Reading, Word Recognition, Text Visualization, Text Interaction, Phonological Cues</p><h5 class="paper-details-heading">Abstract</h5><p>When a reader encounters a word in English, they split the word into smaller orthographic units in the process of recognizing its meaning. For example, “rough”, when split according to phonemes, is decomposed as r-ou-gh (not as r-o-ugh or r-ough), where each group of letters corresponds to a sound. Since there are many ways to segment a group of letters, this constitutes a computational operation that has to be solved by the reading brain, many times per minute, in order to achieve the recognition of words in text necessary for reading. In English, the irregular relationships between groups of letters and sounds, and the wide variety of possible groupings make this operation harder than in more regular languages such as Italian. If this segmentation takes a significant amount of time in the process of recognizing a word, it is conceivable that providing segmentation information in the text itself could help the reading process by reducing its computational cost. In this paper we explore whether and how different visual interventions from the visualization literature could communicate segmentation information for reading and word recognition. We ran a series of pre-registered lexical decision experiments with 192 participants that tested five main types of visual segmentations: outlines, spacing, connections, underlines and color. The evidence indicates that, even with a moderate amount of training, these visual interventions always slow down word identification, but each to a different extent (between 32.7ms—color technique—and 70.7ms—connection technique). These findings are important because they indicate that, at least for typical adult readers with a moderate amount of specific training in these visual interventions, accelerating the lexical decision task is unlikely. Importantly, the results also offer an empirical measurement of the cost of a common set of visual manipulations of text, which can be useful for practitioners seeking to visualize alongside or within text without impacting reading performance. Finally, the interaction between typographically encoded information and visual variables presented unique patterns that deviate from existing theories, suggesting new directions for future inquiry.</p></div></div><script lang="js">
      const paperID = "bf10121c-1a25-4391-b766-450e53b2348e"
      $(document).ready(() => {
        tippy('[data-tippy-content]');

        const allBookmarks =
          d3.selectAll('.checkbox-bookmark')
            .on("click", function () {
              const newValue = !d3.select(this).classed('selected');
              API.markSet(API.storeIDs.bookmarked, paperID, newValue);
              d3.select(this).classed('selected', newValue);
            })
        API.markGet(API.storeIDs.bookmarked, paperID).then(is_bookmarked => {
          is_bookmarked = !!is_bookmarked;
          allBookmarks.classed('selected', is_bookmarked);
        })
        API.markSet(API.storeIDs.visited, paperID, true);

      })

    </script><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script></body></html>