[{"UID":"a93e5d17-6b6f-48f8-a943-8abfdd53c3c1","abstract":"Scatterplots are among the most popular visualization methods for bivariate or multivariate data. While scatterplots scale well with the number of samples, visual clutter cannot be avoided with increasing data size. Density regularization is a common approach to declutter scatterplots. However, most existing density-equalizing algorithms are restricted to rectangular domains, which limits their applicability. In particular, they cannot operate within interactive lenses, a widely used approach for interactive data exploration. We present a numerical approach that generalizes density-equalizing transformations to domains of arbitrary shape, including concave regions. The definition of the regions of interest can be data-driven or interactive. We demonstrate the effectiveness of our method by implementing adaptive and flexible interactive lenses for enhanced data exploration in scatterplots, showcasing its versatility and potential for broader application.","accessible_pdf":null,"authors":[{"affiliation":"University of M\u00fcnster","email":"molchano@uni-muenster.de","name":"Vladimir Molchanov"},{"affiliation":"University of M\u00fcnster","email":"hennes.rave@uni-muenster.de","name":"Hennes Rave"},{"affiliation":"University of M\u00fcnster","email":"linsen@uni-muenster.de","name":"Lars Linsen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"a93e5d17-6b6f-48f8-a943-8abfdd53c3c1","image_caption":"This paper may be of interest to data scientists who develop interactive visual analytics systems that use scatterplots to display and explore complex data.","keywords":["Clutter reduction","scatterplots","density equalization","virtual lenses."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1338-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMzgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.PP_Jb1Fsu3bSMhRSpFrN_DBYeyCr6xBK6ybnuqYcDfE","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1338","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"a93e5d17-6b6f-48f8-a943","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"A Decluttering Lens for Scatterplots","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"09fe1b80-d4a6-454e-bd5f-d49b53eae041","abstract":"Designing multiscale visualizations, particularly when the ratio between the largest scale and the smallest item is large, can be challenging, and designers have developed many approaches to overcome this challenge. We present a design space for visualization with multiple scales. The design space includes three dimensions, with eight total subdimensions. We demonstrate its descriptive power by using it to code approaches from a corpus we compiled of 52 examples, created by a mix of academics and practitioners. We demonstrate descriptive power by analyzing and partitioning these examples into four high-level strategies for designing multiscale visualizations, which are shared approaches with respect to design space dimension choices. We demonstrate generative power by analyzing missed opportunities within the corpus of examples, identified through analysis of the design space, where we note how certain examples could have benefited from different choices. We discuss patterns in the use of different dimension and strategy choices in the different visualization contexts of analysis and presentation. \n\nSupplemental materials: https://osf.io/wbrdm/\n\nDesign space website: https://marasolen.github.io/multiscale-vis-ds/","accessible_pdf":"Accessible","authors":[{"affiliation":"The University of British Columbia","email":"marasolen@gmail.com","name":"Mara Solen"},{"affiliation":"The University of British Columbia","email":"moddo@eoas.ubc.ca","name":"Matt Oddo"},{"affiliation":"University of British Columbia","email":"tmm@cs.ubc.ca","name":"Tamara Munzner"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"09fe1b80-d4a6-454e-bd5f-d49b53eae041","image_caption":"Visualization designers working on large or multiscale datasets may be interested in the paper as they could learn about different design options and strategies for constructing effective visualizations.","keywords":["Visualization","design space","multiscale."],"open_access_supplemental_link":"https://osf.io/wbrdm/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1085-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA4NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYyLCJleHAiOjE3OTI0Nzk2NjJ9.W0kqN9YKAInjxiiCZX0x-etAzhYVHXsxF2c8acOwJjE","preprint_link":"https://arxiv.org/abs/2404.01485","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1085","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"09fe1b80-d4a6-454e-bd5f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"A Design Space for Multiscale Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"c9fcc45e-0f81-49be-817e-b813e0b61c89","abstract":"Research on affective visualization design has shown that color is an especially powerful feature for influencing the emotional connotation of visualizations. Associations between colors and emotions are largely driven by lightness (e.g., lighter colors are associated with positive emotions, whereas darker colors are associated with negative emotions). Designing visualizations to have all light or all dark colors to convey particular emotions may work well for visualizations in which colors represent categories and spatial channels encode data values. However, this approach poses a problem for visualizations that use color to represent spatial patterns in data (e.g., colormap data visualizations) because lightness contrast is needed to reveal fine details in spatial structure. In this study, we found it is possible to design colormaps that have strong lightness contrast to support spatial vision while communicating clear affective connotation. We also found that affective connotation depended not only on the color scales used to construct the colormaps, but also the frequency with which colors appeared in the map, as determined by the underlying dataset (data-dependence hypothesis). These results emphasize the importance of data-aware design, which accounts for not only the design features that encode data (e.g., colors, shapes, textures), but also how those design features are instantiated in a visualization, given the properties of the data.","accessible_pdf":null,"authors":[{"affiliation":"University of Wisconsin - Madison","email":"hbraun5@wisc.edu","name":"Halle Braun"},{"affiliation":"University of Wisconsin-Madison","email":"kushinm11@gmail.com","name":"Kushin Mukherjee"},{"affiliation":"Woodwell Climate Research Center","email":"sgorelik@woodwellclimate.org","name":"Seth Gorelik"},{"affiliation":"University of Wisconsin - Madison","email":"kschloss@wisc.edu","name":"Karen Schloss"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"c9fcc45e-0f81-49be-817e-b813e0b61c89","image_caption":"Practitioners who use color in visualization design may be interested in our findings on factors that influence the affective (emotional) connotation of colormap data visualizations. They may also be interested to learn about how affective connotation is influenced not only by the colors selected but also the underlying data distribution, emphasizing a need for data-aware design.","keywords":["Visual reasoning","visual communication","color cognition","affective science","emotion","scalar field","data-aware design"],"open_access_supplemental_link":"https://github.com/SchlossVRL/color_scales_affect","open_access_supplemental_question":"We have posted all of our data and analysis code on GitHub and have documented our code.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1682-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY4Mi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTU3LCJleHAiOjE3OTI0ODA1NTd9.NQ8_r_0OAM9vZ5qljk65-IDP0m8AHBZG_YWYL2lYePo","preprint_link":"https://osf.io/preprints/osf/p3bva_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1682","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"c9fcc45e-0f81-49be-817e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Affective color scales for colormap data visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ff31e8af-e918-47c4-a7f9-123a2ac6e98c","abstract":"This work introduces AGri (Adaptive Thumbnails for Grid-based Visualizations), a method for dynamically adjusting thumbnails of spatiotemporal data\u2014such as videos\u2014to varying screen footprints in grid-based layouts. AGri aims to maximize thumbnail expressiveness, which quantifies how well similarity relationships among data members (e.g., video frames) are preserved. Thumbnails are generated via cropping, with crop windows optimized based on cumulative salience images. By modeling the trade-off between expressiveness and footprint size, AGri defines a curve\u2014the AGri curve\u2014representing Pareto-optimal visual representations. This curve enables dynamic selection of thumbnails suited to different grid sizes and resolutions. The approach is demonstrated on two datasets: a spatiotemporal ensemble from scientific experiments and an animated short film.","accessible_pdf":null,"authors":[{"affiliation":"University of Groningen","email":"s.d.frey@rug.nl","name":"Steffen Frey"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ff31e8af-e918-47c4-a7f9-123a2ac6e98c","image_caption":"The work is relevant for practitioners aiming to analyze spatiotemporal data, e.g., collections of videos. It may be of interest to scientists across various domains, including simulation sciences, natural sciences, journalism, and the arts. AGri provides a method for generating expressive thumbnails corresponding to different screen footprints when viewing data in a grid-based visualization, such as gridified scatterplots or storyboards.","keywords":["Grid-based Visualization","Thumbnails","Spatiotemporal Data","Video Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1357-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNTctZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU3NSwiZXhwIjoxNzkyNDgyNTc1fQ.5jH95G_q9VY1v-A0bWmC_6Bszc_sZavHxh3E0kS3Mkk","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1357","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ff31e8af-e918-47c4-a7f9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"AGri: Adaptive Thumbnails For Grid-based Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"dde3f095-6834-4f0c-b006-206d9e2e9e69","abstract":"This research discusses the figurative tensions that arise when using portraits to represent individuals behind a dataset. In the broader effort to communicate European data related to depression, the Kiel Science Communication Network (KielSCN) team attempted to engage a wider audience by combining interactive data graphics with AI-generated images of people. This article examines the project\u2019s decisions and results, reflecting on the reaction from the audience when information design incorporates figurative representations of individuals within the data.","accessible_pdf":null,"authors":[{"affiliation":"Kiel Science Communication Network","email":"jahrend@hfk-bremen.de","name":"Julia Ahrend"},{"affiliation":"Muthesius University  of Fine Arts & Design","email":"bdoege@kielscn.de","name":"Bj\u00f6rn D\u00f6ge"},{"affiliation":"Kiel Science Communication Network","email":"td@muthesius.de","name":"Tom Duscher"},{"affiliation":"University of Groningen","email":"d.rodighiero@rug.nl","name":"Dario Rodighiero"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"dde3f095-6834-4f0c-b006-206d9e2e9e69","image_caption":null,"keywords":["Visual Science Communication; Interactive Information Design; AI-Generated Images"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1103-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzExMDMtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1MiwiZXhwIjoxNzkyNDc1ODUyfQ.2bkzwSR7allWYphVk1HHCIZvm0GUQ4zQSB_1CRwc9g0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1103","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"dde3f095-6834-4f0c-b006","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"AI-Generated Images for Representing Individuals \u2013 Navigating the Thin Line Between Care and Bias","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f53bf658-7b1b-4417-9c0c-0b74e7455702","abstract":"London's famous tube map is an iconic piece of design and perhaps represents the schematic visualization style most well-known to the general public: its octolinearity has become the de facto standar for transit maps around the world. Making a good schematic transit map is challenging and labour-intensive, and has attracted the attention of the optimization community. Much of the literature has focused on mathematically defining an optimal drawing and algorithms to compute one. However, achieving these optimal layouts is computationally challenging, often requiring multiple minutes of runtime. Crucially, what it means for a map to be good is actually highly dependent on factors that evade a general formal definition, like unique landmarks within the network, the context in which a map will be displayed, and the preference of the designer and client. Rather than attempting to make an algorithm that produces a single high-quality and ready-to-use metro map at great cost, we propose it is more fruitful to support rapid layout iteration by a human designer, providing a workflow that enables efficient exploration of a wider range of designs than could be done by hand, and iterating on these designs. To this end we identify steps in the design process of schematic maps that are tedious to do by hand but are algorithmically feasible, and present a framework around a simple linear program that computes network layouts almost instantaneously given a fixed direction for every connection. These connection directions are decided by a designer in a graphical user interface with several interaction methods and a number of quality-of-life features demonstrating the flexibility of the framework; the implementation is available as open source.","accessible_pdf":null,"authors":[{"affiliation":"TU Eindhoven","email":"tvdmaps@gmail.com","name":"Thomas van Dijk"},{"affiliation":"TU Eindhoven","email":"s.d.terziadis@tue.nl","name":"Soeren Terziadis"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f53bf658-7b1b-4417-9c0c-0b74e7455702","image_caption":"Graphic designers and information designers might find our tool useful to design schematic layouts of (transit) networks. More generally, we hope that interaction designers may find inspiration in how tightly a user and an algorithm can cooperate on a design task.","keywords":["Geospatial Data","Graph/Network Data","Algorithms","Interaction Design","Software Prototype"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1550-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU1MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.SymWgI0YA4oAXynv49fMOBeIEGLlh7R001UnnUdp9E0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1550","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f53bf658-7b1b-4417-9c0c","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Algorithmically-Assisted Schematic Transit Map Design: A System and Algorithmic Core for Fast Layout Iteration","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"18f5b9fb-e80f-415d-a701-6c7256d9711b","abstract":"An essential task in analyzing collaborative design processes, such as those that are part of workshops in design studies,\nis identifying design outcomes and understanding how the collaboration between participants formed the results and led to decision-\nmaking. However, findings are typically restricted to a consolidated textual form based on notes from interviews or observations. A\nchallenge arises from integrating different sources of observations, leading to large amounts and heterogeneity of collected data. To\naddress this challenge we propose a practical, modular, and adaptable framework of workshop setup, multimodal data acquisition,\nAI-based artifact extraction, and visual analysis. Our interactive visual analysis system, reCAPit, allows the flexible combination of\ndifferent modalities, including video, audio, notes, or gaze, to analyze and communicate important workshop findings. A multimodal\nstreamgraph displays activity and attention in the working area, temporally aligned topic cards summarize participants\u2019 discussions,\nand drill-down techniques allow inspecting raw data of included sources. As part of our research, we conducted six workshops across\ndifferent themes ranging from social science research on urban planning to a design study on band-practice visualization. The latter\ntwo are examined in detail and described as case studies. Further, we present considerations for planning workshops and challenges\nthat we derive from our own experience and the interviews we conducted with workshop experts. Our research extends existing\nmethodology of collaborative design workshops by promoting data-rich acquisition of multimodal observations, combined AI-based\nextraction and interactive visual analysis, and transparent dissemination of results.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Stuttgart","email":"maurice.koch@visus.uni-stuttgart.de","name":"Maurice Koch"},{"affiliation":"University of Stuttgart","email":"nelusa.pathmanathan@visus.uni-stuttgart.de","name":"Nelusa Pathmanathan"},{"affiliation":"University of Stuttgart","email":"weiskopf@visus.uni-stuttgart.de","name":"Daniel Weiskopf"},{"affiliation":"University of Stuttgart","email":"kuno.kurzhals@visus.uni-stuttgart.de","name":"Kuno Kurzhals"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"18f5b9fb-e80f-415d-a701-6c7256d9711b","image_caption":"Our work can be valuable for organizers of collaborative workshops and other design processes, sometimes conducted as part of design studies and co-design. Our approach offers a practical solution in the form of a user interface for analyzing workshop outcomes, along with guidance on how to conduct and instrument workshops with a variety of data modalities, including audio, video, eye tracking, and notes.","keywords":["Collaborative workshops","multimodal analysis of design processes","combination of AI and interactive visualization","design study methodology","eye tracking","reCAPit"],"open_access_supplemental_link":"https://doi.org/10.18419/DARUS-5166","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1683-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY4My1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTQyLCJleHAiOjE3OTI0ODA1NDJ9.j-94_CNmlMC5nByMQXVKWdvISEs2udgDoIY09G8QNdk","preprint_link":"https://arxiv.org/abs/2508.06117","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1683","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"18f5b9fb-e80f-415d-a701","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"A Multimodal Framework for Understanding Collaborative Design Processes","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"cdb6dc71-86fb-401f-a559-fa6c53347af5","abstract":"Morse-Cerf theory considers a one-parameter family of smooth\nfunctions defined on a manifold and studies the evolution of their\ncritical points with the parameter. This paper presents an adaptation\nof Morse-Cerf theory to a family of piecewise-linear (PL) functions.\nThe vertex diagram and Cerf diagram are introduced as representations\nof the evolution of critical points of the PL function. The\ncharacterization of a crossing in the vertex diagram based on the\nhomology of the lower links of vertices leads to the definition of a\ntopological descriptor for time-varying scalar fields. An algorithm\nfor computing the Cerf diagram and a measure for comparing two\nCerf diagrams are also described together with experimental results\non time-varying scalar fields.","accessible_pdf":null,"authors":[{"affiliation":"Indian Institute of Science, Bangalore","email":"amritendud@iisc.ac.in","name":"Amritendu Dhar"},{"affiliation":"TCG CREST","email":"apratimn@gmail.com","name":"Apratim Chakraborty"},{"affiliation":"Indian Institute of Science","email":"vijayn@iisc.ac.in","name":"Vijay Natarajan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"cdb6dc71-86fb-401f-a559-fa6c53347af5","image_caption":"Scientists and engineers who work with time varying data could benefit from this research.\nThe paper introduces PL Morse Cerf theory, a novel theoretical framework to study the evolution of features in time-varying scalar fields. Through practical examples, it also demonstrates the potential for a new visualization framework for the evolution of features by introducing and computing a novel descriptor, the PL Cerf diagram, and a topology-aware distance measure to compare time-varying fields.","keywords":["Time-varying scalar fields","topological descriptors","critical points","Morse theory"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1268-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNjgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU3OCwiZXhwIjoxNzkyNDgyNTc4fQ.erO2B2jd2i555sLC4kFF0Ntn-VFrn5NmbI-VDUUMEPc","preprint_link":"https://arxiv.org/abs/2507.00725","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1268","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"cdb6dc71-86fb-401f-a559","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9188996c-0f8f-420b-95eb-1cf466e3fa0d","abstract":"Text is an integral but understudied component of visualization design. Although recent studies have examined how text elements (e.g., titles and annotations) influence comprehension, preferences, and predictions, many questions remain about textual design and use in practice. This paper introduces a framework for understanding text functions in information visualizations, building on and filling gaps in prior classifications and taxonomies. Through an analysis of 120 real-world visualizations and 804 text elements, we identified ten distinct text functions, ranging from identifying data mappings to presenting valenced subtext. We further identify patterns in text usage and conduct a factor analysis, revealing four overarching text-informed design strategies: Attribution and Variables, Annotation-Centric Design, Visual Embellishments, and Narrative Framing. In addition to these factors, we explore features of title rhetoric and text multifunctionality, while also uncovering previously unexamined text functions, such as text replacing visual elements. Our findings highlight the flexibility of text, demonstrating how different text elements in a given design can combine to communicate, synthesize, and frame visual information. This framework adds important nuance and detail to existing frameworks that analyze the diverse roles of text in visualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of California Berkeley","email":"chase_stokes@berkeley.edu","name":"Chase Stokes"},{"affiliation":"Northeastern University","email":"aarunku5@asu.edu","name":"Anjana Arunkumar"},{"affiliation":"UC Berkeley","email":"hearst@berkeley.edu","name":"Marti Hearst"},{"affiliation":"Northeastern University","email":"l.padilla@northeastern.edu","name":"Lace Padilla"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9188996c-0f8f-420b-95eb-1cf466e3fa0d","image_caption":"This paper would be interesting to data journalists, consultants, analysts, and any other roles which create charts for communication purposes. This paper presents four design patterns found in data visualizations in the wild and analyzes their components and potential use cases.","keywords":["Visualization","text","language","text function","factor analysis","design patterns."],"open_access_supplemental_link":"https://osf.io/swqfc/","open_access_supplemental_question":"We provide all analysis code necessary for the rigorous validation we completed on our factor anlysis results.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1611-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTYxMS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.YBhwKt_pYfRgtWhj8XpCtYvejtc0ebm_zVg9o0Hptss","preprint_link":"https://arxiv.org/abs/2507.12334","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1611","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9188996c-0f8f-420b-95eb","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"An Analysis of Text Functions in Information Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"7c5cabc5-b76c-4eba-be3a-c6fcfd508888","abstract":"We contribute an autoethnographic reflection on the complexity of defining and measuring visualization literacy (i.e., the ability to interpret and construct visualizations) to expose our tacit thoughts that often exist in-between polished works and remain unreported in individual research papers. Our work is inspired by the growing number of empirical studies in visualization research that rely on visualization literacy as a basis for developing effective data representations or educational interventions. Researchers have already made various efforts to assess this construct, yet it is often hard to pinpoint either what we want to measure or what we are effectively measuring. In this autoethnography, we gather insights from 14 internal interviews with researchers who are users or designers of visualization literacy tests. We aim to identify what makes visualization literacy assessment a wicked problem. We further reflect on the fluidity of visualization literacy and discuss how this property may lead to misalignment between what the construct is and how measurements of it are used or designed. We also examine potential threats to measurement validity from conceptual, operational, and methodological perspectives. Based on our experiences and reflections, we propose several calls to action aimed at tackling the wicked problem of visualization literacy measurement, such as by broadening test scopes and modalities, improving test ecological validity, making it easier to use tests, seeking interdisciplinary collaboration, and drawing from continued dialogue on visualization literacy to expect and be more comfortable with its fluidity.","accessible_pdf":"Accessible","authors":[{"affiliation":"Northwestern University","email":"wanqian.ge@northwestern.edu","name":"Lily Ge"},{"affiliation":"Universit\u00e9 Paris-Saclay, Inria, Institut Polytechnique de Paris, CNRS, LISN, i3","email":"acabouat@gmail.com","name":"Anne-Flore Cabouat"},{"affiliation":"Worcester Polytechnic Institute","email":"kbonilla@wpi.edu","name":"Karen Bonilla"},{"affiliation":"Northwestern University","email":"yuancui2025@u.northwestern.edu","name":"Yuan Cui"},{"affiliation":"Worcester Polytechnic Institute","email":"yding5@wpi.edu","name":"Yiren Ding"},{"affiliation":"Worcester Polytechnic Institute","email":"ntrakotondravony@wpi.edu","name":"No\u00eblle Rakotondravony"},{"affiliation":"Northeastern University","email":"18mcreamer@gmail.com","name":"Mackenzie Creamer"},{"affiliation":"UC Santa Cruz","email":"jtotto@ucsc.edu","name":"Jasmine Otto"},{"affiliation":"Northwestern University","email":"maryam.hedayati@u.northwestern.edu","name":"Maryam Hedayati"},{"affiliation":"IBM Research","email":"bumchul.kwon@us.ibm.com","name":"Bum Chul Kwon"},{"affiliation":"Universit\u00e0 degli Studi di Brescia","email":"angela.locoro@unibs.it","name":"Angela Locoro"},{"affiliation":"Worcester Polytechnic Institute","email":"ltharrison@wpi.edu","name":"Lane Harrison"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"petra.isenberg@inria.fr","name":"Petra Isenberg"},{"affiliation":"Northeastern University","email":"m.correll@northeastern.edu","name":"Michael Correll"},{"affiliation":"Northwestern University","email":"matthew.kay@u.northwestern.edu","name":"Matthew Kay"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"7c5cabc5-b76c-4eba-be3a-c6fcfd508888","image_caption":"Practitioners who wish to assess the visualization literacy of their employees (e.g., during onboarding process) or their audience could be interested in reading this paper. This paper can inform them of the considerations in using and designing visualization literacy assessments.","keywords":["Visualization literacy","autoethnography","measurement","validity"],"open_access_supplemental_link":"https://osf.io/xwr4c/","open_access_supplemental_question":"We provided detailed supplemental materials for our autoethnography, including the interview protocol with the semi-structured questions and the full Miro board that we used during analysis.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1406-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQwNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQxLCJleHAiOjE3OTI0ODAzNDF9.Il0Xbe-s_aTEfEnYY1sj4Qyvlim35dGZBaylUTEmYSs","preprint_link":"https://osf.io/dfr4p_v2 (pending moderator approval)","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1406","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"7c5cabc5-b76c-4eba-be3a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"An Autoethnography on Visualization Literacy: A Wicked Measurement Problem","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"fb8fb932-9800-4cfe-9ad5-4d7504df2c00","abstract":"We explore the effects of data and design considerations through the example case of part-to-whole data relationships. Standard part-to-whole representations like pie charts and stacked bar charts make the relationships of parts to the whole explicit. Value estimation in these charts benefits from two perceptual mechanisms: anchoring, where the value is close to a reference value with an easily recognized shape, and alignment where the beginning or end of the shape is aligned with a marker. In an online study, we explore how data and design factors such as value, position, and encoding together impact these effects in making estimations in part-to-whole charts. The results show how salient values and alignment to positions on a scale affect task performance. This demonstrates the need for informed visualization design based around how data properties and design factors affect perceptual mechanisms.","accessible_pdf":null,"authors":[{"affiliation":"University of Wisconsin - Madison","email":"cbailey9@wisc.edu","name":"Connor Bailey"},{"affiliation":"University of Wisconsin - Madison","email":"gleicher@cs.wisc.edu","name":"Michael Gleicher"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"fb8fb932-9800-4cfe-9ad5-4d7504df2c00","image_caption":"This paper demonstrates the effects of data and design factors on perceptual mechanisms, and the resulting effects on task performance. Our work has three types of ramifications. First, in terms of visualization design, it provides evidence to support a nuanced approach to making design decisions based on data properties. Therefore, our paper should be of interest to chart designers. Second, it shows the need to consider data and design factors and perceptual mechanisms in designing visualization experiments. This should be of interest to data visualization researchers. Third, it contributes to our understanding of the mechanisms of how basic charts are perceived by viewers, showing how understanding of anchoring, and alignment relate to the large variations across different conditions within chart types. This should make our paper applicable in data visualization, psychology, and other areas in which the understanding of perceptual mechanisms is important.","keywords":["part-to-whole","estimation","graphical perception","anchoring","alignment","rounding","perceptual mechanisms."],"open_access_supplemental_link":"https://osf.io/e36au, https://github.com/uwgraphics/PartToWhole","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1307-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMDctZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.laGAgh5gV49CW0ID6Cp52XRuYWcTLH-FYkhPhs30-Zo","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1307","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"fb8fb932-9800-4cfe-9ad5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Anchoring and Alignment: Data Factors in Part-to-Whole Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"135b9aea-b3cc-4bf7-880f-117393bf1f70","abstract":"Encoding uncertainty in timelines can provide more precise and informative visualizations (e.g., visual representations of unsure times or locations in event planning timelines). To evaluate the effectiveness of different temporal and categorical uncertainty representations on timelines, we conducted a mixed-methods user study with 81 participants on uncertainty in activity recall timelines (ARTs). We find that participants' accuracy is better when temporal uncertainty is encoded using transparency instead of dashing, and that a participant's visual encoding preference does not always align with their performance (e.g., they performed better with a less-preferred visual encoding technique). Additionally, qualitative findings show that existing biases of an individual alter their interpretation of ARTs. A copy of our study materials is available at https://osf.io/98p6m/.","accessible_pdf":null,"authors":[{"affiliation":"Northeastern University","email":"potter.v@northeastern.edu","name":"Veronika Potter"},{"affiliation":"Northeastern University","email":"le.ha1@northeastern.edu","name":"Ha Le"},{"affiliation":"Northeastern University","email":"syeda.u@northeastern.edu","name":"Uzma Haque Syeda"},{"affiliation":"Northeastern University","email":"s.intille@northeastern.edu","name":"Stephen Intille"},{"affiliation":"Northeastern University","email":"m.borkin@neu.edu","name":"Michelle Borkin"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"135b9aea-b3cc-4bf7-880f-117393bf1f70","image_caption":"As our paper is a case study on embedding uncertainty in activity recall timelines, researchers who work with activity recall or longitudinal behavioral data may be able to apply techniques used in this work for better sense-making of their data or to inform design decisions when making visualizations. Additionally, anyone who works with timelines or temporal data that includes uncertainty may be interested in our work.","keywords":["Timelines","Uncertainty Visualization","Evaluation Study."],"open_access_supplemental_link":"https://osf.io/98p6m/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1068-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwNjgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.UZcz6vB6fVR0hJCV1ZpmPGU5935-tbZH3p-l8-StAbI","preprint_link":"https://osf.io/98p6m/","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1068","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"135b9aea-b3cc-4bf7-880f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"An Evaluation of Temporal and Categorical Uncertainty on Timelines: A Case Study in Human Activity Recall Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"dfb38658-4e3c-42b8-9333-ac8f04df3106","abstract":"Game-Based Learning has proven to be an effective method for enhancing engagement with educational material. However, gaining a deeper understanding of player strategies remains challenging. Sequential game-state and action-based tracking tools often gather extensive data that can be difficult to interpret as long-term strategy. This data presents unique problems to visualization, as it can be fairly natural, noisy data but is constrained within synthetic, controlled environments, leading to issues such as overplotting which can make interpretation complicated. We propose an animated visual encoding tool that utilizes kinetic visualization to address these issues. This tool enables researchers to construct animated data narratives through the configuration of parameter interpolation curves and blending layers. Finally, we demonstrate the usefulness of the tool while addressing specific interests as outlined by a domain expert collaborator.","accessible_pdf":"Accessible","authors":[{"affiliation":"The University of Oklahoma","email":"bradenroper@ou.edu","name":"Braden Roper"},{"affiliation":"The University of Oklahoma","email":"will.thompson@ou.edu","name":"William Thompson"},{"affiliation":"University of Oklahoma","email":"cweaver@ou.edu","name":"Chris Weaver"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"dfb38658-4e3c-42b8-9333-ac8f04df3106","image_caption":"This work will be of interest to researchers who wish to explore animation as a means to encode information into visualizations. Additionally, those interested in game-based learning may find the domain-specific analysis interesting.","keywords":["Kinetic visualization","kinetic queries","animated encoding","game-based learning"],"open_access_supplemental_link":"https://osf.io/tf245","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1277-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNzctZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.Kcd3CnRN2YViuHUNA-ZEnR2DsCgt1U5p3ncpEZgW7YU","preprint_link":"https://arxiv.org/abs/2507.01134","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1277","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"dfb38658-4e3c-42b8-9333","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Animated Visual Encoding and Layer Blending for Identification of Educational Game Strategies","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3ca02fd1-d387-44fa-b576-413ea269d949","abstract":"Pathology images are crucial for cancer diagnosis and treatment. Although artificial intelligence has driven rapid advancements in pathology image analysis, the interpretation of ultra-large and multi-scale pathology images in clinical practice still heavily relies on physicians' experience. Clinicians need to repeatedly zoom in and out on individual slides to compare and assess pathological details \u2014 a process that is both time-consuming and prone to visual fatigue. The system first employs a diffusion model to perform tissue segmentation on pathology images, then calculates pathological tissue proportions and morphological metrics. Finally, through multi-scale dynamic comparison and multi-level visual evaluation, the system facilitates comprehensive and precise analysis of pathology images. The system provides clinicians with an intelligent and interactive tool for pathology image interpretation, enabling efficient visualization and precise analysis of pathological details, thereby reducing the effort require for detailed analysis.","accessible_pdf":null,"authors":[{"affiliation":"Hangzhou City University","email":"xucq@hzcu.edu.cn","name":"Chaoqing Xu"},{"affiliation":"Zhejiang University","email":"rickyyang0113@gmail.com","name":"Ruiqi Yang"},{"affiliation":"Zhejiang University","email":"leewh@zju.edu.cn","name":"Weihan Li"},{"affiliation":"School of Computer and Computing Science, Hangzhou City University","email":"xinyuanfu0421@gmail.com","name":"Xinyuan Fu"},{"affiliation":"School of Computer and Computing Science, Hangzhou City University","email":"flting.amanda@gmail.com","name":"Liting Fang"},{"affiliation":"Zhejiang University","email":"zunleifeng@zju.edu.cn","name":"Zunlei Feng"},{"affiliation":"Zhejiang University","email":"wcan@zju.edu.cn","name":"Can Wang"},{"affiliation":"Hangzhou City University","email":"songml@zju.edu.cn","name":"Mingli Song"},{"affiliation":"Zhejiang University","email":"chenvis@zju.edu.cn","name":"Wei Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3ca02fd1-d387-44fa-b576-413ea269d949","image_caption":"pathologist, clinical doctors, data scientists.\n\nThey could benefit from using our visualization tool.","keywords":["Pathology Image","Diffusion Model","Large-Scale","Visual Analytics","Interactive Exploration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1787-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc4Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.QKMHVMEExppgVF7fyHvG9HV262eDMQWO79MDUDmQuvQ","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1787","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3ca02fd1-d387-44fa-b576","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"An Intelligent Interactive Visual Analytics System for Exploring Large and Multi-Scale Pathology Images","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f8d762ae-1228-45b3-a314-a7a550e6b725","abstract":"Annotations are central to effective data communication, yet most visualization tools treat them as secondary constructs---manually defined, difficult to reuse, and loosely coupled to the underlying visualization grammar. We propose a declarative extension to Wilkinson's Grammar of Graphics that reifies annotations as first-class design elements, enabling structured specification of annotation targets, types, and positioning strategies. To demonstrate the utility of our approach, we develop a prototype extension called Vega-Lite Annotation. Through comparison with eight existing tools, we show that our approach enhances expressiveness, reduces authoring effort, and enables portable, semantically integrated annotation workflows.","accessible_pdf":null,"authors":[{"affiliation":"University of Utah","email":"dilshadur@sci.utah.edu","name":"Md Dilshadur Rahman"},{"affiliation":"University of Utah","email":"rahatzamancse@gmail.com","name":"Md Rahat-uz- Zaman"},{"affiliation":"University of Utah","email":"mcnutt.andrew@gmail.com","name":"Andrew McNutt"},{"affiliation":"University of Utah","email":"paul.rosen@utah.edu","name":"Paul Rosen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f8d762ae-1228-45b3-a314-a7a550e6b725","image_caption":"Type of practitioners: data journalists, data scientists, visualization designers, and visualization developers\n\nPractitioners can use AnnoGram to declaratively specify annotations that remain consistent and well-positioned as data or chart types change. Although the current system is a prototype, it already supports annotation of basic charts like bar, line, and scatterplots. Visualization tool developers can build on this framework to integrate structured annotation support into new or existing tools, improving authoring efficiency, reusability, and semantic integration.","keywords":["annotation","visualization grammar"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1199-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExOTktZG9jLnBkZiIsImlhdCI6MTc2MDk0NTk0MCwiZXhwIjoxNzkyNDgxOTQwfQ.yH0FegbDwJgv1e2o6fuazqUPhrAxuYbZkjFXA9mqq9A","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1199","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f8d762ae-1228-45b3-a314","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"AnnoGram: An Annotative Grammar of Graphics Extension","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"2b485865-8584-417a-bd55-5a961dcff5ef","abstract":"Annotation is often a time-consuming but fruitful activity in data analysis contexts. The manual labor required to create useful annotations is a barrier that keeps users from documenting their analysis, especially intermediate results. To address the needs of exploration and annotation alike, we propose integrating annotation with lens-based interactions, combining both with guidance. We investigate the exploration-annotation requirement space, identifying challenges and extracting five design requirements for annotation in exploration contexts. Based on this investigation, we designed ANNOLENS\u2014a concrete instantiation of such a system that lets users explore and annotate dimensionality-reduced multivariate data. It employs a dual-lens approach for contrastive exploration, using guidance to steer users toward interesting data subsets and attributes. Annotation is directly integrated into the lenses, letting users quickly annotate hunches and discoveries. Automated merging and linking serve to simplify annotation management and reduce disruptions. In a pilot study, we conducted a preliminary evaluation of our approach, which indicated that users find it easy to annotate data and were able to incorporate their knowledge and unique perspective into the process. A free copy of this paper and all supplemental materials are available at https://osf.io/zpu6c/.","accessible_pdf":null,"authors":[{"affiliation":"University of Stuttgart","email":"franziska.becker@vis.uni-stuttgart.de","name":"Franziska Becker"},{"affiliation":"University of Stuttgart","email":"steffen.koch@vis.uni-stuttgart.de","name":"Steffen Koch"},{"affiliation":"University of Stuttgart","email":"research@blascheck.eu","name":"Tanja Blascheck"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"2b485865-8584-417a-bd55-5a961dcff5ef","image_caption":"It could be interesting for data scientists that do exploratory data analysis and want to start documenting their insights while exploring, but do not like heavy-handed annotations as they are conventionally available.","keywords":["Annotation","exploration","guidance","visual analytics."],"open_access_supplemental_link":"https://osf.io/zpu6c/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1333-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMzMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.wEI9PodzqvtWF1ijlyLfl5GYhC9VbfSuXPrBF06r1dU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1333","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"2b485865-8584-417a-bd55","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"AnnoLens: Exploration and Annotation through Lens-Based Guidance","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"8330bc4e-c9eb-41e0-8fa0-8043f0518241","abstract":"Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Notre Dame","email":"dan3@nd.edu","name":"Delin An"},{"affiliation":"University of Notre Dame","email":"pdu@nd.edu","name":"Pan Du"},{"affiliation":"Cornell University","email":"jw2837@cornell.edu","name":"Jian-Xun Wang"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"8330bc4e-c9eb-41e0-8fa0-8043f0518241","image_caption":"Researchers whose work focuses on medical image processing, computational fluid dynamics simulation, and aortic visualization will find this paper of interest. They can apply this model to construct an aorta mesh from the CT image.","keywords":["Conditional diffusion model","volume-guided surface generation","multi-branch vessel modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1206-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTIwNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MjQ4LCJleHAiOjE3OTI0ODAyNDh9.XeRxA-Gh5GMy__KJKKe2QfJUGSNxvEnDimt8z7Cb3Ho","preprint_link":"http://arxiv.org/abs/2507.13404","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1206","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"8330bc4e-c9eb-41e0-8fa0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"72c11016-caee-44a4-82cf-1939cf5d21d3","abstract":"We present a data-domain sampling regime for quantifying CNNs\u2019 graphic perception behaviors. This regime lets us evaluate CNNs\u2019 ratio estimation ability in bar charts from three perspectives: sensitivity to training-test distribution discrepancies, stability to limited samples, and relative expertise to human observers. After analyzing 16 million trials from 800 CNN models and 6,825 trials from 113 human participants, we arrived at a simple and actionable conclusion: CNNs can outperform humans and their biases simply depend on the training-test distance. We show evidence of this simple, elegant behavior of the machines when they interpret visualization images. osf.io/gfqc3 provides registration, the code for our sampling regime, and experimental results.","accessible_pdf":null,"authors":[{"affiliation":"The Ohio State University","email":"jiang.2126@osu.edu","name":"Shuning Jiang"},{"affiliation":"The Ohio State University","email":"weilunchao760414@gmail.com","name":"Wei-Lun Chao"},{"affiliation":"University of Massachusetts Boston","email":"daniel.haehn@umb.edu","name":"Daniel Haehn"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"},{"affiliation":"The Ohio State University","email":"chen.8028@osu.edu","name":"Jian Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"72c11016-caee-44a4-82cf-1939cf5d21d3","image_caption":"This paper is for practitioners such as machine learning engineers, data scientists, and product managers who build or deploy models for visual analysis. Our research provides actionable strategies to create more robust, stable, and cost-effective systems. The key takeaway is to adopt a Coverage (COV) sampling strategy\u2014spreading training samples evenly across the data's range\u2014which achieves higher stability with significantly fewer samples than standard methods, thus reducing training costs. Furthermore, practitioners can diagnose model failures by analyzing the training-test distance, as our findings show a strong correlation between this distance and inference error, helping to predict and explain out-of-distribution failures. By applying these sampling insights and documenting them in model cards, teams can significantly improve model reliability and transparency.","keywords":["Quantification","convolutional neural network","sampling","graphical perception","evaluation"],"open_access_supplemental_link":"https://osf.io/gfqc3/","open_access_supplemental_question":"Our work features thoroughly documented source code and interactive Google Colab notebooks, enabling immediate, zero-setup replication of our analyses online.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/2070-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMjA3MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQwLCJleHAiOjE3OTI0ODA5NDB9.77YNLfbMYLDiBY37bzwyzyxFbFuaB-zQTZ9s05YBUIA","preprint_link":"https://www.arxiv.org/abs/2507.03866","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2070","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"72c11016-caee-44a4-82cf","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3ee80ea0-1bd7-49c1-82f5-fd9b9dd2a2ba","abstract":"This paper presents a typology of concepts and contexts to inform the design of medical data visualizations for clinician-patient communication. As the accessibility, ubiquity and complexity of personal medical data grows, so does the need for patients to understand and interpret it correctly. Based on interviews with 19 healthcare professionals and analysis of current patient communication materials, our typology captures i) the diverse types of medical data communicated; ii) the features clinicians need patients to recognise and understand about these data; iii) contextual information required; iv) motivation for communicating data to patients; and v) current common visualization techniques. In doing so it identifies and defines the key dimensions of data-communication scenarios relevant to data visualizers working with patient data. We apply the typology to classify real world clinical scenarios and through doing so, illustrate how the typology can inform data visualization when designing communication aids for clinicians.","accessible_pdf":null,"authors":[{"affiliation":"University of Edinburgh","email":"s0678276@ed.ac.uk","name":"Sarah Dunn"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3ee80ea0-1bd7-49c1-82f5-fd9b9dd2a2ba","image_caption":"This paper would be of interest to data visualizers working with healthcare data, in particular those producing materials for patient communication. It would also be of interest to medical researchers working on visualizing healthcare data. The paper defines a typology of medical data for the first time, highlighting dimensions of  i) the diverse types of medical data communicated; ii) the features clinicians need patients to recognise and understand about these data; iii) contextual information required; iv) motivation for communicating data to patients; and v) current common visualization techniques. This informs the design of data visualizations for patient communication through highlighting the many factors a data visualizer must take into account when applying visualisation techniques to the unique domain of medical data.","keywords":["Data visualization","health communication","typologies"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1352-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNTItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NSwiZXhwIjoxNzkyNDgyNTY1fQ.bJPzCQsh6VZPtwtw2yToWh8y4KysAcpukVjut5Chtnk","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1352","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3ee80ea0-1bd7-49c1-82f5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"A Typology of Data Concepts and Contexts in Clinician-Patient Communication","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"d207baab-2afc-4d7a-bbfd-cf81768ec83c","abstract":"Explorative flow visualization allows domain experts to analyze complex flow structures by interactively investigating flow patterns. However, traditional visual interfaces often rely on specialized graphical representations and interactions, which require additional effort to learn and use. Natural language interaction offers a more intuitive alternative, but teaching machines to recognize diverse scientific concepts and extract corresponding structures from flow data poses a significant challenge. In this paper, we introduce an automated framework that aligns flow pattern representations with the semantic space of large language models (LLMs), eliminating the need for manual labeling. Our approach encodes streamline segments using a denoising autoencoder and maps the generated flow pattern representations to LLM embeddings via a projector layer. This alignment empowers semantic matching between textual embeddings and flow representations through an attention mechanism, enabling the extraction of corresponding flow patterns based on textual descriptions. To enhance accessibility, we develop an interactive interface that allows users to query and visualize flow structures using natural language. Through case studies, we demonstrate the effectiveness of our framework in enabling intuitive and intelligent flow exploration.","accessible_pdf":null,"authors":[{"affiliation":"Sun Yat-sen University","email":"zhangwh79@mail2.sysu.edu.cn","name":"Weihan Zhang"},{"affiliation":"Sun Yat-sen University","email":"taoj23@mail.sysu.edu.cn","name":"Jun Tao"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"d207baab-2afc-4d7a-bbfd-cf81768ec83c","image_caption":"Practitioners such as scientific data visualization researchers, tool developers, and domain scientists working with flow data would be interested in this paper.","keywords":["Flow visualization","natural language","streamlines"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1349-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTM0OS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQxLCJleHAiOjE3OTI0ODAzNDF9.d7dYMhp-Uu-tkqZQ1ig5gA93c_Q0FNir7YOBF4qaong","preprint_link":"https://arxiv.org/abs/2508.06300","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1349","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"d207baab-2afc-4d7a-bbfd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"67904347-7281-4081-b903-1114c27844ee","abstract":"Balaton Borders translates ecological data from Lake Balaton into ceramic tableware that represents human impact on the landscape, from reedbed reduction to shoreline modification and land erosion. Designed for performative dining, the pieces turn shared meals into multisensory encounters where food and data ceramics spark collective reflection on ecological disruption.","accessible_pdf":null,"authors":[{"affiliation":"Independent Artist","email":"gyeviki.hajnal@gmail.com","name":"Hajnal Gyeviki"},{"affiliation":"Moholy-Nagy University of Art and Design","email":"minko.mihaly@mome.hu","name":"Mih\u00e1ly Mink\u00f3"},{"affiliation":"Moholy-Nagy University of Art and Design","email":"karyda@mome.hu","name":"Mary Karyda"},{"affiliation":"Moholy-Nagy University of Art and Design","email":"damla.cay@mome.hu","name":"Damla \u00c7ay"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"67904347-7281-4081-b903-1114c27844ee","image_caption":null,"keywords":["data physicalization","data edibilization","commensality","ecological design","environmental data","ceramic design","performative dining"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1000-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzEwMDAtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1NSwiZXhwIjoxNzkyNDc1ODU1fQ.1V6HG4tVzPNiNwMbEgCFJLzzJH3DoePkyGI6OHvr2M8","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1000","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"67904347-7281-4081-b903","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Balaton Borders: Data Ceramics for Ecological Reflection","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e8f720e9-fa87-43c1-9d57-b8cc150f3cd3","abstract":"Biomedical data harmonization is essential for enabling exploratory analyses and meta-studies, but the process of schema matching\u2014identifying semantic correspondences between elements of disparate datasets (schemas)\u2014remains a labor-intensive and error-prone task. Even state-of-the-art automated methods often yield low accuracy when applied to biomedical schemas due to the large number of attributes and nuanced semantic differences between them. We present BDIViz, a novel visual analytics system designed to streamline the schema matching process for biomedical data. Through formative studies with domain experts, we identified key requirements for an effective solution and developed interactive visualization techniques that address both scalability challenges and semantic ambiguity. BDIViz employs an ensemble approach that combines multiple matching methods with LLM-based validation, summarizes matches through interactive heatmaps, and provides coordinated views that enable users to quickly compare attributes and their values. Our method-agnostic design allows the system to integrate various schema matching algorithms and adapt to application-specific needs. Through two biomedical case studies and a within-subject user study with  domain experts, we demonstrate that BDIViz significantly improves matching accuracy while reducing cognitive load and curation time compared to baseline approaches.","accessible_pdf":"Accessible","authors":[{"affiliation":"New York University","email":"eden.wu@nyu.edu","name":"Eden Wu"},{"affiliation":"New York University","email":"d.turakhia@nyu.edu","name":"Dishita Turakhia"},{"affiliation":"New York University","email":"guandewu@nyu.edu","name":"Guande Wu"},{"affiliation":"New York University","email":"christos.koutras@nyu.edu","name":"Christos Koutras"},{"affiliation":"New York University School of Medicine","email":"sarah.keegan@nyulangone.org","name":"Sarah Keegan"},{"affiliation":"New York University School of Medicine","email":"wenke.liu@nyulangone.org","name":"Wenke Liu"},{"affiliation":"NYU Grossman School of Medicine","email":"beata.szeitz@nyulangone.org","name":"Beata Szeitz"},{"affiliation":"NYU Grossman School of Medicine","email":"david.fenyo@nyulangone.org","name":"David Fenyo"},{"affiliation":"New York University","email":"csilva@nyu.edu","name":"Claudio Silva"},{"affiliation":"New York University","email":"juliana.freire@nyu.edu","name":"Juliana Freire"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"e8f720e9-fa87-43c1-9d57-b8cc150f3cd3","image_caption":"Data Scientist, Biomedical Researchers","keywords":["Schema matching","Biomedical data harmonization","Data visualization","User-in-the-loop","LLM-based schema matching"],"open_access_supplemental_link":"https://github.com/VIDA-NYU/bdi-viz","open_access_supplemental_question":"Our work provides a reproducible pipeline, including thoroughly documented source code, public Docker images for streamlined deployment, and a live server that allows users to directly interact with the system. We also share source code, experiment datasets, and other supplemental materials to ensure transparency and enable others to re-analyze or extend our results.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1204-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTIwNC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.sChBdqVV330LhozQbt5shMVj9LIty1l75ccWs4q24oA","preprint_link":"https://arxiv.org/abs/2507.16117","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1204","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e8f720e9-fa87-43c1-9d57","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"80f2d2e8-877c-469a-8ce2-bb3e5bea1f93","abstract":"In this work, we challenge the dominant use of logarithmic scales to communicate values spanning multiple orders of magnitude\u2014Orders of Magnitude Values (OMVs)\u2014to the general public. Focusing on bar charts, we incorporate cognitive insights into visualization design to better align with how humans perceive OMVs. Studies in cognitive psychology suggest that, for large numerical ranges such as millions and billions, people do not think logarithmically. Instead, they perceive numbers in a piecewise linear manner, grouping values into scale words (e.g., millions) and applying linear reasoning within each group. We build upon a recently introduced piecewise linear scale, EplusM, and validate its use in bar charts, which we refer to as EplusM bar charts. We also introduce two novel variants of the EplusM bar chart informed by findings in numerical perception: Bricks, which builds on the concepts of round numbers and subitizing, and Multi-Magnitude, which leverages categorical perception of large numbers. In a crowdsourced experiment, we evaluate four bar chart designs: 1) Log, 2) EplusM, 3) Bricks, and 4) Multi-Magnitude, across value retrieval and quantitative comparison tasks. Our results show that EplusM bar charts are significantly preferred over logarithmic designs, increase user confidence, and reduce perceived mental demand, while maintaining task performance. These findings suggest that EplusM bar charts can serve as effective alternatives to logarithmic ones when visualizing OMVs for general audiences.","accessible_pdf":"Accessible","authors":[{"affiliation":"Berger-Levrault","email":"kbatziakoudi@gmail.com","name":"Katerina Batziakoudi"},{"affiliation":"Berger-Levrault","email":"reystef@gmail.com","name":"St\u00e9phanie Rey"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"jean-daniel.fekete@inria.fr","name":"Jean-Daniel Fekete"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"80f2d2e8-877c-469a-8ce2-bb3e5bea1f93","image_caption":"This paper would be valuable to any practitioner who needs to visualize quantitative data spanning multiple orders of magnitude for general audiences\u2014a common challenge in domains such as finance, healthcare, public policy, environmental science, and science communication. Relevant practitioners include, but are not limited to, data journalists creating public-facing visualizations, data analysts preparing reports for non-technical stakeholders, and educators or scientists aiming to make complex quantitative findings more accessible.\n\nPractitioners can directly apply the work by using the open-source implementation of the EplusM scale, available in both D3 and Observable (see supplemental material). Beyond the technical implementation itself, the experimental findings offer practical design guidance\u2014highlighting, for example, the benefits and tradeoffs of using techniques such as subitizing for mantissa representation or categorical encodings for communicating scale.","keywords":["orders of magnitude","comparisons","exponent","mantissa","logarithmic scale","bar charts"],"open_access_supplemental_link":"https://osf.io/ hybvp/?view_only=5cd17943e9ba46deb66a8f7f4eeeb4da","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1497-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ5Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQzLCJleHAiOjE3OTI0ODAzNDN9._boubZw1aZ23gLJdfzE6ZM7nM9JnzzkE_AFA090VsJM","preprint_link":"https://hal.science/hal-05171203","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1497","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"80f2d2e8-877c-469a-8ce2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Beyond Log Scales: Toward Cognitively Informed Bar Charts for Orders of Magnitude Values","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"930062ee-90b2-4c48-9896-626047d01771","abstract":"Visualization design is often described as a process of solving a well-defined problem by navigating a design space. While existing visualization design models have provided valuable structure and guidance, they tend to foreground technical problem-solving and underemphasize the interpretive, judgment-based aspects of design. In contrast, research in other design disciplines has emphasized the importance of framing---how designers define and redefine what the problem is---and the co-evolution of problem and solution spaces through reflective practice. These dimensions remain underexplored in visualization research, particularly from the perspective of expert practitioners. This paper investigates how visualization designers frame problems and navigate the interplay between problem understanding and solution development. We conducted a mixed-methods study with 11 expert design practitioners using design challenges, diary entries, and semi-structured interviews. Through reflexive thematic analysis, we identified key strategies that participants used to frame design problems, reframe them in response to evolving constraints or insights, and construct bridges between problem and solution spaces. These included the use of metaphors, heuristics, sketching, primary generators, and reflective evaluation of failed or incomplete ideas. Our findings contribute an empirically grounded account of visualization design as a reflective, co-evolutionary practice. We show that framing is not a preliminary step, but a continuous activity embedded in the act of designing. Participants frequently shifted their understanding of the problem based on solution attempts, feedback from tools, and ethical or narrative concerns. These insights extend current visualization design models and highlight the need for frameworks that better account for framing and interpretive judgment. We conclude with implications for visualization research, education, and practice. In particular, we discuss how design education can better support framing and co-evolutionary thinking, and how visualization research can benefit from greater attention to the cognitive strategies and reflective processes that underpin expert design.","accessible_pdf":null,"authors":[{"affiliation":"Purdue University","email":"parsonsp@purdue.edu","name":"Paul Parsons"},{"affiliation":"Purdue University","email":"shukla37@purdue.edu","name":"Prakash Chandra Shukla"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"930062ee-90b2-4c48-9896-626047d01771","image_caption":"Practitioners such as professional data visualization designers, UX designers, data journalists, and design consultants would find this paper valuable. The insights on framing and problem\u2013solution co-evolution are especially relevant for those working on complex, client-facing visualization projects where problems and solutions must be iteratively shaped in collaboration with stakeholders.","keywords":["Data visualization design","data visualization practice","problem framing","problem-solution co-evolution","design cognition"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1971-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTk3MS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.en6rzZX5epd4w4FgsAwBfkdenEToEPX6riXh1i6zpzU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1971","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"930062ee-90b2-4c48-9896","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Beyond Problem Solving: Framing and Problem\u2013Solution Co-Evolution in Data Visualization Design","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"d2d68e75-88a1-4f4a-a1c5-7d8cb6706524","abstract":"Virtual Reality (VR) broadcasting has emerged as a promising medium for providing immersive viewing experiences of major sports events such as tennis. However, current VR broadcast systems often lack an effective camera language and do not adequately incorporate dynamic, in-game visualizations, limiting viewer engagement and narrative clarity. To address these limitations, we analyze 400 out-of-play segments from eight major tennis broadcasts to develop a tennis-specific design framework that effectively combines cinematic camera movements with embedded visualizations. We further refine our framework by examining 25 cinematic VR animations, comparing their camera techniques with traditional tennis broadcasts to identify key differences and inform adaptations for VR. Based on data extracted from the broadcast videos, we reconstruct a simulated game that captures the players' and ball's motion and trajectories. Leveraging this design framework and processing pipeline, we develope Beyond the Broadcast, a VR tennis viewing system that integrates embedded visualizations with adaptive camera motions to construct a comprehensive and engaging narrative. Our system dynamically overlays tactical information and key match events onto the simulated environment, enhancing viewer comprehension and narrative engagement while ensuring perceptual immersion and viewing comfort. A user study involving tennis viewers demonstrate that our approach outperforms traditional VR broadcasting methods in delivering an immersive, informative viewing experience.","accessible_pdf":"Accessible","authors":[{"affiliation":"Fudan University","email":"peter650059@hotmail.com","name":"Jun-Hsiang Yao"},{"affiliation":"Fudan University","email":"23110980031@m.fudan.edu.cn","name":"Jielin Feng"},{"affiliation":"Hunan University","email":"xinfangtian0206@163.com","name":"Xinfang Tian"},{"affiliation":"University of Nottingham","email":"kai.xu@nottingham.ac.uk","name":"Kai Xu"},{"affiliation":"Al-Farabi Kazakh National University","email":"gulshat.aa@gmail.com","name":"Gulshat Amirkhanova"},{"affiliation":"Fudan University","email":"simingchen3@gmail.com","name":"Siming Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"d2d68e75-88a1-4f4a-a1c5-7d8cb6706524","image_caption":"Target Practitioners:\nSports Broadcasting Professionals, VR/AR Developers, Sports Data Visualization Specialists. \n\nPractitioners can apply this research to:\nDesign Immersive VR Broadcasts, Optimize Viewer Engagement, Streamline Production Pipelines.","keywords":["Tennis","VR Broadcasting","Embedded Visualization","Camera Motion","Immersive Experience"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1732-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTczMi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.PXNtMYtKHjqSYOYZg3IqeoOVL4fcc3XyzmecM7z0tX8","preprint_link":"https://arxiv.org/abs/2507.20006","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1732","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"d2d68e75-88a1-4f4a-a1c5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Beyond the Broadcast: Enhancing VR Tennis Broadcasting through Embedded Visualizations and Camera Techniques","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"362f56c3-81b6-4af1-aeb4-022ee36079d5","abstract":"Bayesian Neural Networks (BNNs) offer a principled approach to modeling uncertainty in addition to providing predictions, making them particularly valuable for high-stake domains where uncertainty quantification is required. However, their adoption remains low, partly due to the difficulty in tuning and interpreting these models and their results. To address this limitation, we introduce BNNVis, a visual analytics tool designed to visualize BNNs and their results. BNNVis allows the user to understand the architecture and learned posterior weight distributions of their BNN at a glance and how these distributions differ from their prior. Additionally, the system helps them understand the distribution and magnitude of the accompanying uncertainties of the model's predictions. BNNVis provides insight into the final predictions and the model, helping practitioners tune and interpret BNNs and their results. We describe a usage scenario to demonstrate how the features of BNNVis come together to support a practitioner in using a BNN.","accessible_pdf":null,"authors":[{"affiliation":"National Renewable Energy Laboratory","email":"gabriel.appleby@gmail.com","name":"Gabriel Appleby"},{"affiliation":"National Renewable Energy Laboratory","email":"malik.hassanaly@nrel.gov","name":"Malik Hassanaly"},{"affiliation":"Idaho National Lab","email":"jennifer.rogers@inl.gov","name":"Jen Rogers"},{"affiliation":"NREL","email":"juliane.mueller@nrel.gov","name":"Juliane Mueller"},{"affiliation":"National Renewable Energy Laboratory","email":"kristi.potter@nrel.gov","name":"Kristi Potter"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"362f56c3-81b6-4af1-aeb4-022ee36079d5","image_caption":"Scientists interested applying Bayesian Neural Networks to their domain.","keywords":["Visual Analytics","Visualization","Machine Learning","Bayesian Neural-Networks","Uncertainty Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1239-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMzktZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.AIQWek9YrcSlfVZZflOWDuYWqBKAWM12JWbmglrTteY","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1239","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"362f56c3-81b6-4af1-aeb4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"BNNVis: Towards Visual Analytics for Bayesian Neural Networks","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"981234e2-bdbe-42ef-863d-00d6ea28561d","abstract":"This application paper investigates the stability of hydrogen bonds (H-bonds), as characterized by the Quantum Theory of Atoms in Molecules (QTAIM). First, we contribute a database of 4544 electron densities associated to four isomers of water hexamers (the so-called Ring, Book, Cage and Prism), generated by distorting their equilibrium geometry under various structural perturbations, modeling the natural dynamic behavior of molecular systems. Second, we present a new stability measure, called bond occurrence rate, associating each bond path present at equilibrium with its rate of occurrence within the input ensemble. We also provide an algorithm, called BondMatcher, for its automatic computation, based on a tailored, geometry-aware partial isomorphism estimation between the extremum graphs of the considered electron densities. Our new stability measure allows for the automatic identification of densities lacking H-bond paths, enabling further visual inspections. Specifically, the topological analysis enabled by our framework corroborates experimental observations and provides refined geometrical criteria for characterizing the disappearance of H-bond paths. Our electron density database and our C++ implementation are available at this address: https://github.com/thom-dani/BondMatcher.","accessible_pdf":null,"authors":[{"affiliation":"CNRS (LIP6)","email":"thomas.daniel@lip6.fr","name":"Thomas Daniel"},{"affiliation":"Center of New Technologies","email":"malgorzata.olejniczak@cent.uw.edu.pl","name":"Malgorzata Olejniczak"},{"affiliation":"CNRS","email":"julien.tierny@sorbonne-universite.fr","name":"Julien Tierny"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"981234e2-bdbe-42ef-863d-00d6ea28561d","image_caption":"Chemists, simulation scientists, data scientists.","keywords":["Quantum chemistry","topological data analysis","discrete Morse theory","ensemble data."],"open_access_supplemental_link":null,"open_access_supplemental_question":"Replicable experiments (data, code and documentation are provided).","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1440-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ0MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.qP-0K1Jj51q3E8pZMkO7JSa_hfhS8pZVguWK0OTsKUI","preprint_link":"https://arxiv.org/abs/2504.03205","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1440","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"981234e2-bdbe-42ef-863d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"BondMatcher: H-Bond Stability Analysis in Molecular Systems","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e61c0f73-21a5-4e16-bdb6-4038e74770ae","abstract":"Chinese calligraphy is a quintessential element of Chinese cultural heritage. Analyzing and comparing calligraphic styles not only enhances the appreciation, learning, and advancement of calligraphy but also provides valuable insights into ancient China. However, such analysis remains challenging due to the limited scalability and possible inconsistencies of qualitative methods, as well as usability and misalignment issues in conventional quantitative approaches. We propose Calli-VA, a visual analytics system, to address these challenges. Calli-VA extracts character images and their corresponding strokes from original works and characterizes each character using systematic criteria. During analysis, the system defines the analysis scope by overview and uncovers relationships between characters. Explanation and recommendation mechanisms are integrated to help users understand patterns and guide further exploration. A documentation feature allows users to record and share their findings. We demonstrate the effectiveness of Calli-VA through three case studies and expert feedback.","accessible_pdf":null,"authors":[{"affiliation":"Beijing Normal University","email":"jinchengli@bnu.edu.cn","name":"Jincheng Li"},{"affiliation":"\u5317\u4eac\u5e08\u8303\u5927\u5b66 Beijing Normal University (\u4e2d\u56fd\u5927\u9646 Mainland, China)","email":"202422081016@mail.bnu.edu.cn","name":"Jinpeng Wu"},{"affiliation":"Peking University","email":"tanshaocong0108@gmail.com","name":"Shaocong Tan"},{"affiliation":"Beijing Normal University","email":"202431081039@mail.bnu.edu.cn","name":"Lin Du"},{"affiliation":"University of Oxford","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"Peking University","email":"chaofanyang@pku.edu.cn","name":"Chaofan Yang"},{"affiliation":"Peking University","email":"2206595287@pku.edu.cn","name":"Jiadi Zhang"},{"affiliation":"Syracuse University","email":"rxu@syr.edu","name":"Rebecca Ruige Xu"},{"affiliation":"Peking University","email":"shirui@pku.edu.cn","name":"Rui Shi"},{"affiliation":"Beijing Normal University","email":"bailu@bnu.edu.cn","name":"Lu Bai"},{"affiliation":"Peking University","email":"xiaoru.yuan@pku.edu.cn","name":"Xiaoru Yuan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"e61c0f73-21a5-4e16-bdb6-4038e74770ae","image_caption":"Humanists, especially those engaged in style analysis, can benefit from our approach. We provide a complete pipeline for conducting such analysis: extracting features through preprocessing, identifying objects of interest via an overview, performing detailed analysis with the aid of explanation and recommendation mechanisms, and recording findings for documentation and communication.","keywords":["Chinese calligraphy style analysis","image analysis","digital humanities","visual analytics."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1335-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTMzNS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.nR9KJIHpOqRE5JOM0XEDbPg8cKqYznRd8RNHDgNtC4I","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1335","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e61c0f73-21a5-4e16-bdb6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Calli-VA: A Visual Analytics System for Analyzing and Comparing Chinese Calligraphic Styles","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"32bd145e-06bf-411a-af2e-109192108a7e","abstract":"While many visualizations are build for domain users (e.g., biologists, machine learning developers), understanding how visualizations are used in the domain has long been a challenging task. Previous research has relied on either interviewing a limited number of domain users or reviewing relevant application papers in the visualization community, neither of which provides comprehensive insight into visualizations in the wild of a specific domain. This paper aims to fill this gap by examining the potential of using Large Language Models (LLM) to analyze visualization usage in domain literature. We use high-dimension (HD) data visualization in sing-cell transcriptomics as a test case, analyzing 1,203 papers that describe 2,056 HD visualizations with highly specialized domain terminologies (e.g., biomarkers, cell lineage). To facilitate this analysis, we introduce a multi-step, human-in-the-loop LLM workflow. Instead of relying solely on LLMs for end-to-end analysis, our workflow enhances analytical quality through 1) integrating image processing and traditional NLP methods to prepare well-structured inputs for three targeted LLM subtasks (i.e., translating domain terminology, summarizing analysis tasks, and performing categorization), and 2) establishing checkpoints for human involvement and validation throughout the process.\nThe analysis results was validated with expert interviews and a test set, revealing three often overlooked aspects in HD visualization: trajectories in HD spaces, inter-cluster relationships, and dimension clustering.\nThis research provides a stepping stone for future studies seeking to use LLMs to bridge the gap between visualization design and domain-specific usage.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"},{"affiliation":"Northeastern University","email":"xinyi.liu@utexas.edu","name":"Xinyi Liu"},{"affiliation":"Harvard Medical School","email":"nils@hms.harvard.edu","name":"Nils Gehlenborg"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"32bd145e-06bf-411a-af2e-109192108a7e","image_caption":"Biologists and other domain practitioners new to visualization can use this paper to learn how visualizations are applied in their field and how to create more effective ones.\n\nVisualization designers can use the findings to develop tools that better align with the real-world needs and practices of domain users.","keywords":["High dimensional visualization; LLM-supported literature review; Visualization in the wild"],"open_access_supplemental_link":"https://hdvis.github.io","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1905-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkwNS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.TswtYR2y7Q1Rq18g6yRk_vbMycBldnYtcwz7XLgUbQQ","preprint_link":"https://osf.io/preprints/osf/qtsak_v2?view_only=","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1905","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"32bd145e-06bf-411a-af2e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Can LLMs Bridge Domain and Visualization? A Case Study on High-Dimension Data Visualization in Single-Cell Transcriptomics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"6d474cda-8152-40b9-ba2f-7a28f9bbe0bd","abstract":"Prior natural language datasets for data visualization have focused on tasks such as visualization literacy assessment, insight generation, and visualization generation from natural language instructions. These studies often rely on controlled setups with purpose-built visualizations and artificially constructed questions. As a result, they tend to prioritize the interpretation of visualizations, focusing on decoding visualizations rather than understanding their encoding. In this paper, we present a new dataset and methodology for probing visualization design rationale through natural language. We leverage a unique source of real-world visualizations and natural language narratives: literate visualization notebooks created by students as part of a data visualization course. These notebooks combine visual artifacts with design exposition, in which students make explicit the rationale behind their design decisions. We also use large language models (LLMs) to generate and categorize question-answer-rationale triples from the narratives and articulations in the notebooks. We then carefully validate the triples and curate a dataset that captures and distills the visualization design choices and corresponding rationales of the students.","accessible_pdf":null,"authors":[{"affiliation":"City, University of London","email":"maeve.hutchinson@city.ac.uk","name":"Maeve Hutchinson"},{"affiliation":"City, University of London","email":"radu.jianu@city.ac.uk","name":"Radu Jianu"},{"affiliation":"City, University of London","email":"a.slingsby@city.ac.uk","name":"Aidan Slingsby"},{"affiliation":"City, University of London","email":"j.d.wood@city.ac.uk","name":"Jo Wood"},{"affiliation":"City, University of London","email":"pranava.madhyastha@city.ac.uk","name":"Pranava Madhyastha"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"6d474cda-8152-40b9-ba2f-7a28f9bbe0bd","image_caption":"This paper will be of interest to data visualization educators, data scientists, visual analytics researchers, UX designers, and HCI practitioners who engage in or support visualization design. It is especially relevant to those who mentor, teach, or evaluate visualization work, such as university instructors, industry trainers, and visualization system designers.","keywords":["Design","Literate Visualization","Natural Language"],"open_access_supplemental_link":"https://github.com/maevehutch/DesignQAR; https://maevehutch.github.io/DesignQAR/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1218-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMTgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.OzkjXvqcADR99SjbEsGj51KYw8ae3kSln-3f1yD1hl8","preprint_link":"https://arxiv.org/abs/2506.16571","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1218","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"6d474cda-8152-40b9-ba2f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Capturing Visualization Design Rationale","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"bf2f93ab-ab67-4a81-8726-6819e84553be","abstract":"Sentiment contagion occurs when attitudes toward one topic are influenced by attitudes toward others. \nDetecting and understanding this phenomenon is essential for analyzing topic evolution and informing social policies.\nPrior research has developed models to simulate the contagion process through hypothesis testing and has visualized user\u2013topic correlations to aid comprehension. \nNevertheless, the vast volume of topics and the complex interrelationships on social media present two key challenges: (1) efficient construction of large-scale sentiment contagion networks, and (2) in-depth explorations of these networks.\nTo address these challenges, we introduce a causality-based framework that efficiently constructs and explains sentiment contagion.\nWe further propose a map-like visualization technique that encodes time using a horizontal axis, enabling efficient visualization of causality-based sentiment flow while maintaining scalability through limitless spatial segmentation. \nBased on the visualization, we develop CausalMap, a system that supports analysts in tracing sentiment contagion pathways and assessing the influence of different demographic groups.\nFurthermore, we conduct comprehensive evaluations\u2014\u2014including two use cases, a task-based user study, an expert interview, and an algorithm evaluation\u2014\u2014to validate the usability and effectiveness of our approach.","accessible_pdf":null,"authors":[{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"renzhongli@zju.edu.cn","name":"Renzhong Li"},{"affiliation":"Zhejiang University","email":"sn_ye@zju.edu.cn","name":"Shuainan Ye"},{"affiliation":"Zhejiang University","email":"yuchenlin@zju.edu.cn","name":"Yuchen Lin"},{"affiliation":"Zhejiang University","email":"zhoubuwei@zju.edu.cn","name":"Buwei Zhou"},{"affiliation":"Zhejiang University","email":"kang264@zju.edu.cn","name":"Zhining Kang"},{"affiliation":"Michigan State University","email":"pengtaiq@msu.edu","name":"Tai-Quan Peng"},{"affiliation":"independent researcher","email":"723981485@qq.com","name":"Wenhao Fu"},{"affiliation":"Zhejiang University","email":"tangtan@zju.edu.cn","name":"Tan Tang"},{"affiliation":"Zhejiang University","email":"ycwu@zju.edu.cn","name":"Yingcai Wu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"bf2f93ab-ab67-4a81-8726-6819e84553be","image_caption":"Practitioners analyzing public sentiment (e.g., policy regulators, brand managers) will benefit from our framework. \nThey can adopt the three-stage workflow (explore-interpret-intervene) or directly apply the causal framework and visual system. \nFor instance:\n1) Policy regulators can use CausalMap to trace negative sentiment origins and simulate interventions;\n2) Brand managers can identify cross-product sentiment propagation and design demographic-based strategies.","keywords":["Social Media","Causal Analysis","Visual Analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1075-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA3NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYxLCJleHAiOjE3OTI0Nzk2NjF9.vVLT3OiHMrpzgAt7F83ldoW119WoIBqjir39mUEqy-w","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1075","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"bf2f93ab-ab67-4a81-8726","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Causality-based Visual Analytics of Sentiment Contagion in Social Media Topics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"4161480d-1e23-4eed-be76-3953fac207b7","abstract":"Large-scale scientific simulations require significant resources to generate high-resolution time-varying data (TVD). While super-resolution is an efficient post-processing strategy to reduce costs, existing methods rely on a large amount of HR training data, limiting their applicability to diverse simulation scenarios. To address this constraint, we proposed CD-TVD, a novel framework that combines contrastive learning and an improved diffusion-based super-resolution model to achieve accurate 3D super-resolution from limited time-step high-resolution data. During pre-training on historical simulation data, the contrastive encoder and diffusion super-resolution modules learn degradation patterns and detailed features of high-resolution and low-resolution samples. In the training phase, the improved diffusion model with a local attention mechanism is fine-tuned using only one newly generated high-resolution timestep, leveraging the degradation knowledge learned by the encoder. This design minimizes the reliance on large-scale high-resolution datasets while maintaining the capability to recover fine-grained details. Experimental results on fluid and atmospheric simulation datasets confirm that CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a significant advancement in data augmentation for large-scale scientific simulations. The code is available at https://github.com/Xin-Gao-private/CD-TVD.","accessible_pdf":"Accessible","authors":[{"affiliation":"Tianjin University","email":"bichongke@tju.edu.cn","name":"Chongke Bi"},{"affiliation":"Tianjin University","email":"gao_xin_private@163.com","name":"Xin Gao"},{"affiliation":"Tianjin University","email":"closernh@163.com","name":"Jiakang Deng"},{"affiliation":"Computer Network Information Center, Chinese Academy of Sciences","email":"liguan@sccas.cn","name":"Guan Li"},{"affiliation":"The Hong Kong University of Science and Technology","email":"junhanvis@outlook.com","name":"Jun Han"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"4161480d-1e23-4eed-be76-3953fac207b7","image_caption":"This paper would be of particular interest to practitioners in scientific computing, computational fluid dynamics, and climate and atmospheric modeling, especially those working with large-scale time-varying simulations under resource constraints. By leveraging contrastive learning and diffusion-based super-resolution, the proposed framework enables accurate reconstruction of high-resolution spatiotemporal fields using minimal high-resolution input. Practitioners can apply this method to enhance the resolution of simulation outputs post hoc, reducing storage and computation costs while preserving fine-grained structures critical for downstream analysis and visualization.","keywords":["Time-varying data visualization","deep learning","super-resolution","diffusion model"],"open_access_supplemental_link":null,"open_access_supplemental_question":"This work introduces a two-stage framework for 3D super-resolution of time-varying scientific data, which integrates contrastive representation learning with a diffusion-based reconstruction module. Notably, it achieves accurate reconstruction using only a single high-resolution timestep in the fine-tuning phase, enabled by degradation-aware feature encoding during pretraining. This design significantly reduces reliance on high-resolution data while maintaining generalizability across diverse physical simulation scenarios.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1782-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc4Mi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTU2LCJleHAiOjE3OTI0ODA5NTZ9.sYr5jmI1bzrE98MWsmFGL2l6NfkVO-CesxrkkbYlRtQ","preprint_link":"https://arxiv.org/abs/2508.08173","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1782","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"4161480d-1e23-4eed-be76","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"CD-TVD:Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"973882c9-093a-4edf-b7b0-9157aabaa93b","abstract":"Machine learning algorithms may be chosen for their effectiveness at predictions, but they lose impact when human decision makers do not understand the predictions well enough to trust them. Though machine learning algorithms that produce interpretable models are available, many factors influence the selection for a particular application, and many tools exist for building understanding post-hoc. One way to develop insight is to probe through a concept humans use in making decisions -- an examination of alternate scenarios that might change the outcome called counterfactuals. In this short paper, we build on work using counterfactuals for comprehending machine learning models by proposing a technique wherein users explore counterfactuals as paths through a tree of possible data attribute changes. We extend the technique to groups of data points and consequently to groups of counterfactuals. We provide a prototype implementation and and evaluation with four users from different fields of expertise who were able to apply CFTree to their own domain data and discover interesting attribute relationships.","accessible_pdf":null,"authors":[{"affiliation":"DePaul University","email":"caroline.cao39@gmail.com","name":"Fang Cao"},{"affiliation":"DePaul University","email":"ebrown80@cdm.depaul.edu","name":"Eli Brown"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"973882c9-093a-4edf-b7b0-9157aabaa93b","image_caption":"data scientists, technical domain experts using machine learning. \nPractitioners could use this technology in tuning or understanding their models","keywords":["counterfactuals","human-centered AI","interpretable machine learning","visual analytics"],"open_access_supplemental_link":"https://osf.io/j2wgx/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1355-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNTUtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NSwiZXhwIjoxNzkyNDgyNTY1fQ.7PUuH30XXB7RwNpafoBEYUb91V1PqKOc_ojPr__5Ko0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1355","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"973882c9-093a-4edf-b7b0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"CFTree: Exploring Paths Through Counterfactuals","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"605afb22-7f43-443f-9183-77604f4577f3","abstract":"Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.","accessible_pdf":null,"authors":[{"affiliation":"University of Washington","email":"snyderl@cs.washington.edu","name":"Luke Snyder"},{"affiliation":"Microsoft Research","email":"clwang15uw@gmail.com","name":"Chenglong Wang"},{"affiliation":"Microsoft Research","email":"sdrucker@microsoft.com","name":"Steven Drucker"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"605afb22-7f43-443f-9183-77604f4577f3","image_caption":"Data journalists, data scientists, visualization designers","keywords":["Visualization","Large Language Models","Retargeting"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1128-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMjgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.8bsAgjD9yPXSulsjKPsevGJN_h8Lhyc9jRZ00aiQgLI","preprint_link":"http://arxiv.org/abs/2507.01436","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1128","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"605afb22-7f43-443f-9183","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Challenges & Opportunities with LLM-Assisted Visualization Retargeting","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"8aa31c36-374d-4155-b9c3-fd20d8deeec2","abstract":"Understanding how people perceive visualizations is crucial for designing effective visual data representations; however, many heuristic design guidelines are derived from specific tasks or visualization types, without considering the constraints or conditions under which those guidelines hold. In this work, we aimed to assess existing design heuristics for categorical visualization using well-established psychological knowledge. Specifically, we examine the impact of the subitizing phenomenon in cognitive psychology\u2014people\u2019s ability to automatically recognize a small set of objects instantly without counting\u2014in data visualizations. We conducted three experiments with multi-class scatterplots\u2014between 2 and 15 classes with varying design choices\u2014across three different tasks\u2014class estimation, correlation comparison, and clustering judgments\u2014to understand how performance changes as the number of classes (and therefore set size) increases. Our results indicate if the category number is smaller than six, people tend to perform well at all tasks, providing empirical evidence of subitizing in visualization. When category numbers increased, performance fell, with the magnitude of the performance change depending on task and encoding. Our study bridges the gap between heuristic guidelines and empirical evidence by applying well-established psychological theories, suggesting future opportunities for using psychological theories and constructs to characterize visualization perception.","accessible_pdf":null,"authors":[{"affiliation":"University of North Carolina-Chapel Hill","email":"zeyuwang@cs.unc.edu","name":"Arran Zeyu Wang"},{"affiliation":"University of Oklahoma","email":"quadri@ou.edu","name":"Ghulam Jilani Quadri"},{"affiliation":"The University of North Carolina at Chapel Hill","email":"gisellez@ad.unc.edu","name":"Mengyuan Zhu"},{"affiliation":"University of North Carolina-Chapel Hill","email":"chint@cs.unc.edu","name":"Chin Tseng"},{"affiliation":"University of North Carolina-Chapel Hill","email":"danielle.szafir@cs.unc.edu","name":"Danielle Szafir"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"8aa31c36-374d-4155-b9c3-fd20d8deeec2","image_caption":"Designer, Visualization Psychologist, Empirical Researcher","keywords":["Visualization Perception","Psychology","Subitizing","Fechner\u2019s Law","Dual-System Theory","Categorical Data","Color","Shape"],"open_access_supplemental_link":"https://osf.io/y3z2b/?view_only=7f6569187b344fadbd11cc09a6e63d24","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1566-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU2Ni1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.GKAvgeJVw-gUBxJ1l3xCFigzU-0UTuswvm4WeQXv58k","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1566","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"8aa31c36-374d-4155-b9c3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Characterizing Visualization Perception with Psychological Phenomena: Uncovering the Role of Subitizing in Data Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"5b91b14a-3bf6-41db-9687-a9c0406af811","abstract":"Chart annotations enhance visualization accessibility but suffer from fragmented, non-standardized representations that limit cross-platform reuse. We propose ChartMark, a structured grammar that separates annotation semantics from visualization implementations. ChartMark features a hierarchical framework mapping onto annotation dimensions (e.g., task, chart context), supporting both abstract intents and precise visual details. Our toolkit demonstrates converting ChartMark specifications into Vega-Lite visualizations, highlighting its flexibility, expressiveness, and practical applicability.","accessible_pdf":"Accessible","authors":[{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"ychen081@connect.hkust-gz.edu.cn","name":"Yiyu Chen"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"ywu012@connect.hkust-gz.edu.cn","name":"Yifan Wu"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"sshen190@connect.hkust-gz.edu.cn","name":"Shuyu Shen"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yxie740@connect.hkust-gz.edu.cn","name":"Yupeng Xie"},{"affiliation":"The Hong Kong University of Science and Technology","email":"lshenaj@connect.ust.hk","name":"Leixian Shen"},{"affiliation":"The Hong Kong University of Science and Technology \uff08Guangzhou\uff09","email":"xionghui@hkust-gz.edu.cn","name":"Hui Xiong"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yuyuluo@hkust-gz.edu.cn","name":"Yuyu Luo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5b91b14a-3bf6-41db-9687-a9c0406af811","image_caption":"Practitioners who would benefit from this work include data visualization researchers, data journalists, human\u2013computer interaction (HCI) designers, and applied data scientists who routinely work with complex or high-density visualizations. In particular, those interested in improving the interpretability, accessibility, and communicative effectiveness of data graphics will find the proposed annotation framework valuable. Data journalists could apply the techniques to better highlight key trends, comparisons, or anomalies in public-facing charts. HCI designers and developers of visualization tools could adopt the formal grammar and annotation structures introduced in this work to support interactive authoring or automated annotation features. Simulation scientists and analysts could also benefit by using the annotation system to more clearly convey important events or state transitions in time-series or group-based data. Ultimately, the proposed framework enables a more structured, semantically meaningful, and reusable approach to chart annotation\u2014supporting both manual and machine-generated visual explanations across domains.","keywords":["Chart Annotation","Grammar Language-agnostic"],"open_access_supplemental_link":"1. https://chartmark.github.io/        2. https://github.com/HKUSTDial/ChartMark","open_access_supplemental_question":"Thoroughly documented source code","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1349-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNDktZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.ZJjLVV-urouNiAWTCjr1Sng0oaLEnoyzQwUVeh7qZTI","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1349","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"5b91b14a-3bf6-41db-9687","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"ChartMark: A Structured Grammar for Chart Annotation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"fcafb37e-70f2-4d68-b9a4-e92902d9d2bd","abstract":"This paper evaluates the visualization literacy of modern Large Language Models (LLMs) and introduces a novel prompting technique called Charts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet, GPT-4.5-preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment Test (VLAT) using standard prompts and our structured approach. The Charts-of-Thought method guides LLMs through a systematic data extraction, verification, and analysis process before answering visualization questions. Our results show Claude-3.7-sonnet achieved a score of 50.17 using this method, far exceeding the human baseline of 28.82. This approach improved performance across all models, with score increases of 21.8% for GPT-4.5, 9.4% for Gemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The performance gains were consistent across original and modified VLAT charts, with Claude correctly answering 100% of questions for several chart types that previously challenged LLMs. Our study reveals that modern multimodal LLMs can surpass human performance on visualization literacy tasks when given the proper analytical framework. These findings establish a new benchmark for LLM visualization literacy and demonstrate the importance of structured prompting strategies for complex visual interpretation tasks. Beyond improving LLM visualization literacy, Charts-of-Thought could also enhance the accessibility of visualizations, potentially benefiting individuals with visual impairments or lower visualization literacy.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"amitkumar.das@stonybrook.edu","name":"Amit Kumar Das"},{"affiliation":"East West University","email":"md.tarun005@gmail.com","name":"Mohammad Tarun"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"fcafb37e-70f2-4d68-b9a4-e92902d9d2bd","image_caption":"Type of Practitioners:\nData scientists, AI/ML engineers, visualization researchers, educational technologists, accessibility specialists, business intelligence analysts, and researchers working with multimodal LLMs.\n\nHow they could apply this work:\nData scientists and AI/ML engineers can integrate the Charts-of-Thought prompting technique into their existing LLM workflows to enhance automated chart analysis and data interpretation capabilities significantly, achieving human-level performance on visualization literacy tasks. Visualization researchers could adopt this structured prompting methodology to develop better evaluation systems for chart design and to create automated accessibility tools that generate accurate descriptions for visually impaired users. Educational technologists could implement Charts-of-Thought in learning platforms to provide intelligent tutoring for data literacy, helping students systematically approach chart interpretation through guided data extraction and analysis steps. Business intelligence professionals could deploy this approach in automated dashboard evaluation systems to ensure data visualizations communicate accurately and flag potential misinterpretations before publication. Accessibility specialists can use this methodology to develop more effective screen reader technologies that provide structured, step-by-step chart descriptions, following the same cognitive process that humans use. Researchers working with multimodal LLMs across domains could adapt the Charts-of-Thought framework for other visual interpretation tasks, applying the principles of structured data extraction, verification, and analysis to improve model performance on domain-specific visual content beyond standard charts and graphs.","keywords":["Visualization Literacy","Large Language Models","Charts-of-Thoughts","Data Extraction"],"open_access_supplemental_link":"https://github.com/vhcailab/Charts-of-Thought","open_access_supplemental_question":"We provide comprehensive supplementary materials including all source code, experimental data, prompts, and detailed results to ensure full reproducibility. All materials are publicly accessible through both the submission system and a dedicated GitHub repository (https://github.com/vhcailab/Charts-of-Thought), enabling researchers to replicate our Charts-of-Thought prompting methodology and build upon our structured approach to enhancing LLM visualization literacy.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1997-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTk5Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.KWQYXIp-fUs0bgEUQ09-UFpzuveP-EctaGSUgXR8Kqo","preprint_link":"https://arxiv.org/abs/2508.04842","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1997","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"fcafb37e-70f2-4d68-b9a4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3d33e0d2-031b-4bd9-beae-dd6146407196","abstract":"Many real-world datasets \u2014 from an artist's body of work to a person's social media history \u2014 exhibit meaningful semantic changes over time that are difficult to capture with existing dimensionality reduction methods. To address this gap, we introduce a visualization technique that combines force-based projection and streaming clustering methods to build a spatial-temporal map of embeddings. Applying this technique, we create Chronotome, a tool for interactively exploring evolving themes in time-based data \u2014 in real time. We demonstrate the utility of our approach through use cases on text and image data, showing how it offers a new lens for understanding the aesthetics and semantics of temporal datasets.","accessible_pdf":"Accessible","authors":[{"affiliation":"Harvard University","email":"mattelim@gsd.harvard.edu","name":"Matte Lim"},{"affiliation":"Harvard University","email":"catherineyeh@g.harvard.edu","name":"Catherine Yeh"},{"affiliation":"Harvard","email":"wattenberg@gmail.com","name":"Martin Wattenberg"},{"affiliation":"Harvard","email":"viegas@google.com","name":"Fernanda Viegas"},{"affiliation":"Harvard University","email":"pmichala@gsd.harvard.edu","name":"Panagiotis Michalatos"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3d33e0d2-031b-4bd9-beae-dd6146407196","image_caption":"We think our work would interest a variety of practitioners interested in working with time-based data, including data scientists, machine learning researchers, digital humanities scholars, and journalists. By visualizing how semantic structures evolve over time, practitioners can use Chronotome to uncover thematic shifts, track emerging trends in real-time, or explore the aesthetics of changing content \u2014 offering new ways to analyze, interpret, and tell stories with temporal datasets.","keywords":["Dynamic topic modeling","embedding visualization","clustering methods","temporal data","spring force models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1131-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMzEtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.zv5vQGF-FUGiYaAHVmT6aFQ7fQJQI_Ebqrb_6_7S9C4","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1131","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3d33e0d2-031b-4bd9-beae","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"55c2bdfa-142e-44ed-989c-76c9b32b3d66","abstract":"Communicating climate change remains challenging, as climate reports, though rich in data and visualizations, often feel too abstract or technical for the public. Although personalization can enhance communication, most tools still lack the narrative and visualization tailoring needed to connect with individual experiences. We present CLAImate, an AI-enabled prototype that personalizes conversation narratives and localizes visualizations based on users' climate knowledge and geographic location. We evaluated CLAImate through internal verification of factual correctness, a formative study with experts, and a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70% FACTSCORE. Visualization experts appreciated its clarity and personalization, and seven out of ten UK participants reported better understanding and local relevance of climate risks with CLAImate. We also discuss design challenges in personalization, accuracy, and scalability, and outline future directions for integrating visualizations in personalized conversational interfaces.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Massachusetts Amherst","email":"mrashik@cs.umass.edu","name":"Mashrur Rashik"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"jean-daniel.fekete@inria.fr","name":"Jean-Daniel Fekete"},{"affiliation":"University of Massachusetts Amherst","email":"nmahyar@cs.umass.edu","name":"Narges Mahyar"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"55c2bdfa-142e-44ed-989c-76c9b32b3d66","image_caption":"Practitioners, including data scientists, climate communication experts, environmental scientists, visualization researchers, educators, and developers of AI-powered communication tools, will find this paper highly relevant. CLAImate\u2019s personalized, localized, and interactive approach shows how complex scientific data, such as climate information, can be transformed into content that is accessible, relatable, and actionable for diverse audiences. Practitioners can use these strategies to design or improve their own systems by utilizing narrative-driven visualizations, adaptive explanations tailored to user backgrounds, and region-specific data.","keywords":["Other Application Areas; Communication/Presentation","Storytelling; General Public; Software Prototype."],"open_access_supplemental_link":"https://osf.io/9vgf2/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1139-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMzktZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.SSpnnG1Xbnts_ZbOEM1VUjYOGiRQNq2JXdG2fej-kNc","preprint_link":"https://arxiv.org/abs/2507.11677","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1139","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"55c2bdfa-142e-44ed-989c","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"34e579da-1d6a-4ba7-a6e1-4644a0d2c0b7","abstract":"Ensemble datasets are ever more prevalent in various scientific domains. In climate science, ensemble datasets are used to capture variability in projections under plausible future conditions including greenhouse and aerosol emissions. Each ensemble model run produces projections that are fundamentally similar yet meaningfully distinct. Understanding this variability among ensemble model runs and analyzing its magnitude and patterns is a vital task for climate scientists. In this paper, we present ClimateSOM, a visual analysis workflow that leverages a self organizing map (SOM) and Large Language Models (LLMs) to support interactive exploration and interpretation of climate ensemble datasets. The workflow abstracts climate ensemble model runs\u2014spatiotemporal time series\u2014into a distribution over a 2D space that captures the variability among the ensemble model runs using a SOM. LLMs are integrated to assist in sensemaking of this SOM-defined 2D space, the basis for the visual analysis tasks. In all, ClimateSOM enables users to explore the variability among ensemble model runs, identify patterns, compare and cluster the ensemble model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to an ensemble dataset of precipitation projections over California and the Northwestern United States. Furthermore, we conduct a short evaluation of our LLM integration, and conduct an expert review of the visual workflow and the insights from the case studies with six domain experts to evaluate our approach and its utility.","accessible_pdf":null,"authors":[{"affiliation":"University of California, Davis","email":"ykawakami@ucdavis.edu","name":"Yuya Kawakami"},{"affiliation":"Scripps Institution of Oceanography","email":"dcayan@ucsd.edu","name":"Daniel Cayan"},{"affiliation":"University of California at Davis","email":"dyuliu@ucdavis.edu","name":"Dongyu Liu"},{"affiliation":"University of California at Davis","email":"ma@cs.ucdavis.edu","name":"Kwan-Liu Ma"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"34e579da-1d6a-4ba7-a6e1-4644a0d2c0b7","image_caption":"Climate scientists in particular would be interested to explore visual methods for analysis alongside often-used statistical methods. Explicitly considering climate-related time series as a distribution in addition to aggregate metrics can uncover potential insights related to to the ingredients of such time series and we identified cases where providing that additional lens can be insightful.","keywords":["Climate visual analytics","ensemble visualization","self-organizing maps"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1773-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc3My1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.sIf_YCaRJ_o_33FDmafkuXv_2VgsOS-bOPjIOSDdo_o","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1773","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"34e579da-1d6a-4ba7-a6e1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"d3492f20-d356-4c55-992d-dbc0fa0fe0af","abstract":"Random forests are a machine learning method used to automatically classify datasets and consist of a multitude of decision trees. While these random forests often have higher performance and generalize better than a single decision tree, they are also harder to interpret. This paper presents a visualization method and system to increase interpretability of random forests. We cluster similar trees which enables users to interpret how the model performs in general without needing to analyze each individual decision tree in detail, or interpret an oversimplified summary of the full forest. To meaningfully cluster the decision trees, we introduce a new distance metric that takes into account both the decision rules as well as the predictions of a pair of decision trees. We also propose two new visualization methods that visualize both clustered and individual decision trees: (1) The Feature Plot, which visualizes the topological position of features in the decision trees, and (2) the Rule Plot, which visualizes the decision rules of the decision trees. We demonstrate the efficacy of our approach through a case study on the \u201cGlass\u201d dataset, which is a relatively complex standard machine learning dataset, as well as a small user study.","accessible_pdf":"Accessible","authors":[{"affiliation":"Maastricht University","email":"max.sondag@hotmail.com","name":"Max Sondag"},{"affiliation":"Leipzig University","email":"cmeinecke@informatik.uni-leipzig.de","name":"Christofer Meinecke"},{"affiliation":"Eindhoven University of Technology","email":"d.a.c.collaris@tue.nl","name":"Dennis Collaris"},{"affiliation":"University of Cologne","email":"landesberger@cs.uni-koeln.de","name":"Tatiana von Landesberger"},{"affiliation":"Eindhoven University of Technology","email":"s.j.v.d.elzen@tue.nl","name":"Stef van den Elzen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"d3492f20-d356-4c55-992d-dbc0fa0fe0af","image_caption":"Modelers from various domains who aim to interpret whether their decision forests.","keywords":["Random Forest","Decision Tree","Tree clustering"],"open_access_supplemental_link":"https://github.com/maxie12/RandomForestVis","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1432-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQzMi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.ifIYYGGEmHRxXCWdYZXNT_KVX8HbJUrUaHH_frPfDsg","preprint_link":"https://arxiv.org/abs/2507.22665","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1432","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"d3492f20-d356-4c55-992d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Cluster-Based Random Forest Visualization and Interpretation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"c7fde6c0-1dc4-4ad4-8413-50cc367ed451","abstract":"Visualization design study is a widely adopted approach for developing tailored visual solutions to domain-specific problems through close interdisciplinary collaboration. While the visualization community has proposed generalizable frameworks, there is a growing need for domain-aware methodologies that address discipline-specific challenges and refine design study practices. To investigate how domain characteristics and collaborator roles influence the design study process, we conducted interviews with 15 experts, including domain specialists from the humanities, arts, applied sciences, and artificial intelligence, as well as visualization researchers and developers, with direct experience in design studies. Our findings reveal tensions and opportunities that arise from differing expectations, communication styles, and levels of engagement among collaborators at various stages of the design process, including problem formulation, co-design, and evaluation. We highlight how domain-specific norms and role dynamics shape collaboration and influence the trajectory of visualization projects. Based on these insights, we offer practical considerations to help visualization researchers anticipate domain-specific challenges, foster mutual understanding, and adapt their methods accordingly. Our study contributes to ongoing efforts to support more context-sensitive, sustainable, and inclusive design study practices across diverse application domains.","accessible_pdf":null,"authors":[{"affiliation":"King's College London","email":"yiwen.xing@kcl.ac.uk","name":"Yiwen Xing"},{"affiliation":"King's College London","email":"maria-teresa.ortoleva@kcl.ac.uk","name":"Maria Teresa Ortoleva"},{"affiliation":"Kings College London","email":"rita.borgo@kcl.ac.uk","name":"Rita Borgo"},{"affiliation":"King's College London","email":"alfie.abdulrahman@gmail.com","name":"Alfie Abdul-Rahman"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"c7fde6c0-1dc4-4ad4-8413-50cc367ed451","image_caption":"This paper will be of interest to visualization researchers and practitioners who conduct or plan to conduct design studies, including those working in interdisciplinary settings. By reading this paper, practitioners can gain a multi-perspective understanding of current practices in visualization design studies, including insights into methodological choices, collaborative dynamics, and expectations from visualization researchers, software engineers, and domain experts across different fields. The findings can serve as a reference when planning and executing their own design studies, helping them anticipate challenges, navigate collaboration, and improve the overall rigor and impact of their work.","keywords":["Design study","collaboration","interview study"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1584-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU4NC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.8h2zWp66DF_mGhfEuWF-MvYM7kTpMmDWy5bmn-ME-Qk","preprint_link":"https://kclpure.kcl.ac.uk/portal/en/publications/collaborating-across-domains-and-roles-an-interview-study-of-visu","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1584","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"c7fde6c0-1dc4-4ad4-8413","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Collaborating across Domains and Roles: An Interview Study of Visualization Design Practices","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"60b8dd96-21c8-4a3d-8e8b-b50374e1230e","abstract":"In this paper, we evaluated the use of three pre-attentive visual variables, namely, area, color hue, and flicker, to highlight elements on four charts: bar chart, line chart, linear progress chart, and radial progress chart. We exposed 48 participants---using short time lapses of 250 ms---to a set of stimuli showing one of the four charts with the three visual variables. Then we examined the accuracy of their answers in identifying the highlighted target element under sedentary and walking conditions. Our results show that all three visual variables can be perceived with a medium to high accuracy rate, though performance varies depending on the mobility condition, the visual variable, and even the chart type. Among the visual variables, color and flicker yield the best target identification accuracy rates, while the area proves to be the least effective. The findings from our first investigation contribute to the design of efficient smartwatch micro visualizations that support glanceable and in-motion data reading.","accessible_pdf":null,"authors":[{"affiliation":"University of Stuttgart","email":"fairouz.grioui@vis.uni-stuttgart.de","name":"Fairouz Grioui"},{"affiliation":"Universit\u00e4t Stuttgart","email":"st166324@stud.uni-stuttgart.de","name":"Yasmin Amzir"},{"affiliation":"University of Stuttgart","email":"nina.doerr@visus.uni-stuttgart.de","name":"Nina Doerr"},{"affiliation":"University of Stuttgart","email":"research@blascheck.eu","name":"Tanja Blascheck"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"60b8dd96-21c8-4a3d-8e8b-b50374e1230e","image_caption":"Practitioners who might find this paper interesting are: \n- Researchers in the field of data visualization in general and micro and mobile visualizations specifically. \n- UI and UX designers of mobile applications.\n\nApplying our study findings regarding the three tested pre-attentive visual variables can be helpful in highlighting important information for glanceable visualizations (in particular, progress charts, bar charts, and line charts) on small and mobile devices.","keywords":["pre-attentive visual variables","micro visualization","mobile visualizations","smartwatch."],"open_access_supplemental_link":"https://osf.io/n348a/?view_only=139b008328dd44708945ed6fab310a1a","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1272-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNzItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.CxcgDqMFaWLT7aCQtOKHg4GALIrAK4vcmnLIOiVPsp0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1272","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"60b8dd96-21c8-4a3d-8e8b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Comparing Pre-attentive Visual Variables in a Target Identification Task for Glanceable Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1f23efb5-deca-4391-a7c4-e455fce74cb9","abstract":"Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification \u21d2 Interpretation \u21d2 Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at https://github.com/Happy-Hippo209/ConceptViz.","accessible_pdf":null,"authors":[{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"lihaoxuan@zju.edu.cn","name":"Haoxuan Li"},{"affiliation":"Zhejiang University","email":"wenzhen@zju.edu.cn","name":"Zhen Wen"},{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"jiangqiqi348284@gmail.com","name":"Qiqi Jiang"},{"affiliation":"Zhejiang University, Hangzhou, China","email":"3220101835@zju.edu.cn","name":"Chenxiao Li"},{"affiliation":"Zhejiang University","email":"wuyw0701@foxmail.com","name":"Yuwei Wu"},{"affiliation":"Zhejiang University","email":"yyc_yang@zju.edu.cn","name":"Yuchen Yang"},{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"wangyiyao@zju.edu.cn","name":"Yiyao Wang"},{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"huangxiuqi@zju.edu.cn","name":"Xiuqi Huang"},{"affiliation":"Zhejiang University","email":"minfeng_zhu@zju.edu.cn","name":"Minfeng Zhu"},{"affiliation":"Zhejiang University","email":"chenvis@zju.edu.cn","name":"Wei Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"1f23efb5-deca-4391-a7c4-e455fce74cb9","image_caption":"AI researchers, interpretability engineers, and visualization designers would be interested in this paper. They can apply the Identification-Interpretation-Validation workflow and the concept-based feature exploration techniques to design their own interpretability tools or extend similar visual analytics approaches to other neural network architectures.","keywords":["Large Language Models","Mechanistic Interpretability","Visual Analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1254-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI1NC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.su28JmkRpVRfX7s2xWqnS4nJTQrC2eMAVheZEKBaOAI","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1254","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1f23efb5-deca-4391-a7c4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"da138859-6a6c-4df2-813b-6a70e20d04a1","abstract":"In-depth analysis of competitive debates is essential for participants to develop argumentative skills and refine strategies, and further improve their debating performance. However, manual analysis of unstructured and unlabeled textual records of debating is time-consuming and ineffective, as it is challenging to reconstruct contextual semantics and track logical connections from raw data. To address this, we propose Conch, an interactive visualization system that systematically analyzes both what is debated and how it is debated. In particular, we propose a novel parallel spiral visualization that compactly traces the multidimensional evolution of clash points and participant interactions throughout debate process. In addition, we leverage large language models with well-designed prompts to automatically identify critical debate elements such as clash points, disagreements, viewpoints, and strategies, enabling participants to understand the debate context comprehensively. Finally, through two case studies on real-world debates and a carefully-designed user study, we demonstrate Conch's effectiveness and usability for competitive debate analysis.","accessible_pdf":"Accessible","authors":[{"affiliation":"Huazhong University of Science and Technology","email":"qianhechen01@gmail.com","name":"Qianhe Chen"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"},{"affiliation":"Huazhong University of Science and Technology","email":"sakaaanayu@gmail.com","name":"Yixin Yu"},{"affiliation":"Huazhong University of Science and Technology","email":"addinistrator.fallindepart@gmail.com","name":"Xiyuan Zhu"},{"affiliation":"Huazhong University of Science and Technology","email":"2874849065@qq.com","name":"Xuerou Yu"},{"affiliation":"School of Journalism and Information Communication, Huazhong University of Science and Technology","email":"rex_wang@hust.edu.cn","name":"Ran Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"da138859-6a6c-4df2-813b-6a70e20d04a1","image_caption":"This paper will interest debate coaches, educators, and NLP or visualization practitioners who work with structured argumentation. They can apply the proposed system, Conch, to analyze how debates evolve, identify key clash points and strategies, and improve training or research in argument analysis. The methods and visualizations may also transfer to fields like legal reasoning or educational discourse analysis.","keywords":["Competitive debate","debate analysis","clash point","visual analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Our work introduces Conch, an interactive system that analyzes competitive debates through a novel spiral visualization and a hierarchical semantic framework. We provide all prompts, the full strategy framework, and the mathematical model of Conch in the supplemental materials, and we plan to open-source the implementation after further organization and documentation.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1157-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTE1Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.j9d28l1hKK7GTKIofpD3_pc_Ra9oyUjb8qOUwhb0qQs","preprint_link":"https://arxiv.org/abs/2507.14482/","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1157","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"da138859-6a6c-4df2-813b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"25ece7fd-1a08-463e-b526-9e6a16a62b59","abstract":"Volumetric medical imaging technologies produce detailed 3D representations of anatomical structures. However, effective medical data visualization and exploration pose significant challenges, especially for individuals with limited medical expertise. We introduce a novel XR-based system with two key innovations: (1) a coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models and (2) a multimodal interaction framework combining hand gestures with LLM-enabled voice commands. We conduct preliminary evaluations, including a 15-participant user study and expert interviews, to demonstrate the system's abilities to enhance spatial understanding and reduce cognitive load. Experimental results show notable improvements in task completion times, usability metrics, and interaction effectiveness enhanced by LLM-driven voice control. While identifying areas for future refinement, our findings highlight the potential of this immersive visualization system to advance medical training and clinical practice. Our demo application and supplemental materials are available for download at: https://osf.io/bpjq5/.","accessible_pdf":null,"authors":[{"affiliation":"CUHK","email":"1155203213@link.cuhk.edu.hk","name":"Qixuan Liu"},{"affiliation":"CUHK","email":"shiqiu@cuhk.edu.hk","name":"Shi Qiu"},{"affiliation":"CUHK","email":"yqwang@cse.cuhk.edu.hk","name":"Yinqiao Wang"},{"affiliation":"CUHK","email":"xiwenwu@cuhk.edu.hk","name":"Xiwen Wu"},{"affiliation":"CUHK","email":"kennethchok@surgery.cuhk.edu.hk","name":"Kenneth Siu Ho Chok"},{"affiliation":"The Chinese University of Hong Kong","email":"cwfu@cse.cuhk.edu.hk","name":"Chi-Wing Fu"},{"affiliation":"The Chinese University of Hong Kong","email":"pheng@cse.cuhk.edu.hk","name":"Pheng Ann Heng"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"25ece7fd-1a08-463e-b526-9e6a16a62b59","image_caption":"Simulation scientists and medical practitioners would be interested in reading this paper. They may try our XR-based system design and multimodal interaction paradigm for effective visualization of 3D medical data such as CT/MRI volumes.","keywords":["Medical Visualization","Medical XR","Multimodal Interaction"],"open_access_supplemental_link":"https://osf.io/bpjq5/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1094-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwOTQtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.SZLZV-FmjJAkMZn3IqmUvmZ8rKE-cKOhvX3zR8Ed9k4","preprint_link":"http://arxiv.org/abs/2506.22926","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1094","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"25ece7fd-1a08-463e-b526","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ad323fdf-97c1-4e09-a814-a643243d280b","abstract":"Political sectarianism is fueled in part by misperceptions of political opponents: People commonly overestimate the support for extreme policies among members of the other party. These misperceptions inflame partisan animosity and may be used to justify extremism among one\u2019s own party. Research suggests that correcting partisan misperceptions\u2014by informing people about the actual views of outparty members\u2014may reduce one\u2019s own expressed support for political extremism, including partisan violence and anti-democratic actions. However, there remains a limited understanding of how the design of correction interventions drives these effects. The present study investigated how correction effects depend on different representations of outparty views communicated through data visualizations. Building on prior interventions that present the average outparty view, we consider the impact of visualizations that more fully convey the range of views among outparty members. We conducted an experiment with U.S.-based participants from Prolific (N=239 Democrats, N=244 Republicans). Participants made predictions about support for political violence and undemocratic practices among members of their political outparty. They were then presented with data from an earlier survey on the actual views of outparty members. Some participants viewed only the average response (Mean-Only condition), while other groups were shown visual representations of the range of views from 75% of the outparty (Mean+Interval condition) or the full distribution of responses (Mean+Points condition). Compared to a control group that was not informed about outparty views, we observed the strongest correction effects (i.e., lower support for political violence and undemocratic practices) among participants in the Mean-only and Mean+Points condition, while correction effects were weaker in the Mean+Interval condition. In addition, participants who observed the full distribution of out-party views (Mean+Points condition) were most accurate at later recalling the degree of support among the outparty. Our findings suggest that data visualizations can be an important tool for correcting pervasive distortions in beliefs about other groups. However, the way in which variability in outparty views is visualized can significantly shape how people interpret and respond to corrective information.","accessible_pdf":null,"authors":[{"affiliation":"University of North Carolina at Charlotte","email":"dmarkant@uncc.edu","name":"Doug Markant"},{"affiliation":"University of North Carolina at Charlotte","email":"ssah1@uncc.edu","name":"Subham Sah"},{"affiliation":"Simon Fraser University","email":"akarduni@sfu.ca","name":"Alireza Karduni"},{"affiliation":"University of North Carolina at Charlotte","email":"mrogha@uncc.edu","name":"Milad Rogha"},{"affiliation":"University of Florida","email":"mythai@cise.ufl.edu","name":"My Thai"},{"affiliation":"UNC Charlotte","email":"wdou1@uncc.edu","name":"Wenwen Dou"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"ad323fdf-97c1-4e09-a814-a643243d280b","image_caption":"This paper might be of interest to people who broadly work on topics in the social and political sciences, including the use of data visualizations to communicate public opinion on divisive or controversial issues. This work could provide some useful insights into how people interpret data about other social groups and how different visual representations might impact the persuasiveness of such communications.","keywords":["uncertainty visualization","political communication","partisan misperception"],"open_access_supplemental_link":"https://osf.io/8crsp/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1695-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY5NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.ShXOq5J3ifh-_RJYMcTKDBrO77jHZsmYAqpZgPfJJSU","preprint_link":"https://arxiv.org/pdf/2508.00233","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1695","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ad323fdf-97c1-4e09-a814","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"70fe9706-bf4e-4d28-a8e4-fc5b25dca854","abstract":"We present the Critical Design Strategy (CDS)---a structured method designed to facilitate the examination of visualisation designs through reflection and critical thought. The CDS helps designers think critically and make informed improvements using heuristic evaluation. When developing a visual tool or pioneering a novel visualisation approach, identifying areas for enhancement can be challenging. Critical thinking is particularly crucial for visualisation designers and tool developers, especially those new to the field, such as studying visualisation in higher education. The CDS consists of three stages across six perspectives: Stage 1 captures the essence of the idea by assigning an indicative title and selecting five adjectives (from twenty options) to form initial impressions of the design. Stage 2 involves an in-depth critique using 30 heuristic questions spanning six key perspectives---user, environment, interface, components, design, and visual marks. Stage 3 focuses on synthesising insights, reflecting on design decisions, and determining the next steps forward. We introduce the CDS and explore its use across three visualisation modules in both undergraduate and postgraduate courses. Our longstanding experience with the CDS has allowed us to refine and develop it over time: from its initial creation through workshops in 2017/18 to improvements in wording and the development of two applications by 2020, followed by the expansion of support notes and refinement of heuristics through 2023; while using it in our teaching each year. This sustained use allows us to reflect on its practical application and offer guidance on how others can incorporate it into their own work.","accessible_pdf":"Accessible","authors":[{"affiliation":"Bangor University","email":"j.c.roberts@bangor.ac.uk","name":"Jonathan Roberts"},{"affiliation":"Basrah University","email":"hananalnjar@yahoo.com","name":"Hanan Alnjar"},{"affiliation":"Bangor University","email":"aron.e.owen@bangor.ac.uk","name":"Aron Owen"},{"affiliation":"Bangor University","email":"p.ritsos@bangor.ac.uk","name":"Panagiotis Ritsos"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"70fe9706-bf4e-4d28-a8e4-fc5b25dca854","image_caption":"The Critical Design Strategy (CDS) will be of interest to practitioners who design visualisations, evaluate them, or teach visualisation. Practitioners such as data scientists, information designers, user-experience (UX) specialists, visual analytics researchers, interface designers, data journalists, and educators in design or computer science.\n\nIn professional practice, the CDS can be applied as a structured, heuristic evaluation framework to guide the critical review of visualisation designs at any stage of development. It allows practitioners to:\n\n- Break down a visualisation into six distinct perspectives (such as data, visual encoding, interaction, etc.) for targeted critique.\n- Identify strengths, weaknesses, and trade-offs early in the design process, enabling iterative improvement before deployment.\n- Provide a consistent evaluation approach across team members, improving communication and shared understanding in collaborative projects.\n- Adapt the CDS process for specific project contexts from internal tool development to public-facing visualisations.\n\nBy following the CDS, practitioners can produce more considered, credible, and effective visualisations, supported by clear rationale and a repeatable critique process.","keywords":["Visualisation design","Design critique","Pedagogy","Visualisation theory","Information visualisation","Teaching visualisation"],"open_access_supplemental_link":"https://arxiv.org/abs/2508.05325","open_access_supplemental_question":"Novel heuristic evaluation method with detailed supplemental notes available via IEEE TVCG and the arXiv preprint (https://arxiv.org/abs/2508.05325). A supporting website hosts an online CDS tool, extended guidance, and direct links to all supplemental materials (https://cds-critical-design-strategy.github.io/index.html).","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1737-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTczNy1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.zusaz9jpJbGPU9CSJFdcW8SsTVg382pAjmHK3nmgxIA","preprint_link":"https://arxiv.org/abs/2508.05325","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1737","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"70fe9706-bf4e-4d28-a8e4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"c663a254-dd80-4021-88aa-f0840fc3024e","abstract":"The interactive visual analysis of set-typed data, i.e., data with attributes that are of type set, is a rewarding area of research and applications. Valuable prior work has contributed solutions that enable the study of such data with individual set-typed dimensions. In this paper, we present CrossSet, a novel method for the joint study of two set-typed dimensions and their interplay. Based on a task analysis, we describe a new, multi-scale approach to the interactive visual exploration and analysis of such data. Two set-typed data dimensions are jointly visualized using a hierarchical matrix layout, enabling the analysis of the interactions between two set-typed attributes at several levels, in addition to the analysis of individual such dimensions. CrossSet is anchored at a compact, large-scale overview that is complemented by drill-down opportunities to study the relations between and within the set-typed dimensions, enabling an interactive visual multi-scale exploration and analysis of bivariate set-typed data. Such an interactive approach makes it possible to study single set-typed dimensions in detail, to gain an overview of the interaction and association between two such dimensions,\nto refine one of the dimensions to gain additional details at several levels, and to drill down to the specific interactions of individual set-elements from the set-typed dimensions. To demonstrate the effectiveness and efficiency of CrossSet, we have evaluated the new method in the context of several application scenarios.","accessible_pdf":null,"authors":[{"affiliation":"VRVis Research Center","email":"matkovic@vrvis.at","name":"Kresimir Matkovic"},{"affiliation":"VRVis Research Center","email":"splechtna@vrvis.at","name":"Rainer Splechtna"},{"affiliation":"Virginia Tech","email":"gracanin@vt.edu","name":"Denis Gracanin"},{"affiliation":"University of Bergen","email":"helwig.hauser@uib.no","name":"Helwig Hauser"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"c663a254-dd80-4021-88aa-f0840fc3024e","image_caption":"The CrossSet method and the accompanying paper are highly interesting for experts and practitioners working with set-typed data. Our examples demonstrate its wide applicability across various domains. Traditionally, biologists have used set-typed data primarily in univariate analyses. With CrossSet, they can now extend their investigations to bivariate cases. Additionally, data journalists and data scientists can explore bivariate analyses of set-typed data more effectively.","keywords":["set-typed data","bivariate visual data exploration and analysis"],"open_access_supplemental_link":"https://arxiv.org/abs/2508.00424","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1622-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTYyMi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.kT9kNo8qowH59UwHs8te8LAd9mbgMYaDBUYtFz108D8","preprint_link":"https://arxiv.org/abs/2508.00424","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1622","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"c663a254-dd80-4021-88aa","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"cf7e7577-abbd-426a-bf21-3f2fbffe5aad","abstract":"Visualization knowledge bases enable computational reasoning and recommendation over a visualization design space. These systems evaluate design trade-offs using numeric weights assigned to different features (e.g., binning a variable). Feature weights can be learned automatically by fitting a model to a collection of chart pairs, in which one chart is deemed preferable to the other. To date, labeled chart pairs have been drawn from published empirical research results; however, such pairs are not comprehensive, resulting in a training corpus that lacks many design variants and fails to systematically assess potential trade-offs. To improve knowledge base coverage and accuracy, we contribute data augmentation techniques for generating and labeling chart pairs. We present methods to generate novel chart pairs based on design permutations and by identifying under-assessed features\u2014leading to an expanded corpus with thousands of new chart pairs, now in need of labels. Accordingly, we next compare varied methods to scale labeling efforts to annotate chart pairs, in order to learn updated feature weights. We evaluate our methods in the context of the Draco knowledge base, demonstrating improvements to both feature coverage and chart recommendation performance.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Washington","email":"hyeokk@uw.edu","name":"Hyeok Kim"},{"affiliation":"University of Washington","email":"jheer@uw.edu","name":"Jeffrey Heer"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"cf7e7577-abbd-426a-bf21-3f2fbffe5aad","image_caption":"Visualization researchers could best benefit from our work by replicating our approaches with their empirical study stimuli.","keywords":["Visualization design knowledge base","data augmentation"],"open_access_supplemental_link":"https://osf.io/fqpdh/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1016-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTAxNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYwLCJleHAiOjE3OTI0Nzk2NjB9.5w9lvXKwKhYtssONqicI2QmCpIiZGfVQJza0ySxy8AI","preprint_link":"https://arxiv.org/abs/2508.02216","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1016","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"cf7e7577-abbd-426a-bf21","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Data Augmentation for Visualization Design Knowledge Bases","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"2b8f4be2-cade-4f52-8af3-5d874bdb4b77","abstract":"We present interactive data-driven compute overlays for native and\nweb-based 3D geographic map applications based on WebGPU.\nOur data-driven overlays are generated in a multi-step compute\nworkfow from multiple data sources on the GPU. We demonstrate\ntheir potential by showing results from snow cover and avalanche\nsimulations, where simulation parameters can be adjusted interac-\ntively and results are visualized instantly. Benchmarks show that\nour approach can compute large-scale avalanche simulations in mil-\nliseconds to seconds, depending on the size of the terrain and the\nsimulation parameters, which is multiple orders of magnitude faster\nthan a state-of-the-art Python implementation.","accessible_pdf":"Accessible","authors":[{"affiliation":"TU Wien","email":"e11808210@student.tuwien.ac.at","name":"Patrick Komon"},{"affiliation":"TU Vienna","email":"e1326608@student.tuwien.ac.at","name":"Gerald Kimmersdorfer"},{"affiliation":"TU Wien","email":"celarek@cg.tuwien.ac.at","name":"Adam Celarek"},{"affiliation":"TU Wien","email":"waldner@cg.tuwien.ac.at","name":"Manuela Waldner"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"2b8f4be2-cade-4f52-8af3-5d874bdb4b77","image_caption":"This paper is aimed at practitioners who work with 3D geographic visualizations and simulations. Our API can either be used directly to implement custom interactive map overlays or can serve as a reference to guide the design and implementation of a system for generating such overlays.\nAs demonstrated with snow cover and avalanches, weBIGeo\u2019s API can be used to define compute workflows for various simulations, allowing for interactive exploration of the outcomes. This is especially relevant for scientists working in the domains of natural hazards (e.g., avalanches, landslides), meteorology (e.g., drainage, snow cover) or geology (e.g., erosion), allowing them to better understand and communicate their models.\nGenerally, this approach can be applied to interactive visualizations of any geographic dataset, like for instance, creating a heatmap from millions of user-selected points. Data scientists, developers, and data journalists with programming experience can use weBIGeo to create interactive overlays, enabling both expert and non-expert users to explore large geographic datasets interactively.","keywords":["3D geographic visualization","geographic simulation","WebGPU"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1251-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNTEtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.-bsemwowFvpxhFBOumm8QGM4fxH_ZyDdC4Xmt6bdIR4","preprint_link":"https://arxiv.org/abs/2506.23364","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1251","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"2b8f4be2-cade-4f52-8af3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Data-Driven Compute Overlays for Interactive Geographic Simulation and Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"200b7b6d-abf5-4cb3-b8b4-3da334833db3","abstract":"Data Humanism is a human-centered design approach that emphasizes the personal, contextual, and imperfect nature of data. Despite its growing influence among practitioners, the 13 principles outlined in Giorgia Lupi\u2019s visual manifesto remain loosely defined in research contexts, creating a gap between design practice and systematic application. Through a mixed-methods approach, including a systematic literature review, multimedia analysis, and expert interviews, we present a characterization of Data Humanism principles for visualization researchers. Our characterization provides concrete definitions that maintain interpretive flexibility in operationalizing design choices. We  validate our work through direct consultation with Lupi. Moreover, we leverage the characterization to decode a visualization work, mapping Data Humanism principles\nto specific visual design choices. Our work creates a common language for human-centered visualization, bridging the gap between practice and research for future applications and evaluations.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Zurich","email":"alhazwani@ifi.uzh.ch","name":"Ibrahim Al-Hazwani"},{"affiliation":"University of Bergen","email":"ke.zhang@uib.no","name":"Ke Er Zhang"},{"affiliation":"University of Bergen","email":"laura.garrison@uib.no","name":"Laura Garrison"},{"affiliation":"University of Zurich","email":"bernard@ifi.uzh.ch","name":"J\u00fcrgen Bernard"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"200b7b6d-abf5-4cb3-b8b4-3da334833db3","image_caption":"Data visualization designers & researchers would use this paper to systematically apply data humanism principles in their projects, moving beyond intuitive design choices to evidence-based human-centered approaches. Moreover, they could leverage the characterization framework to evaluate and justify design decisions in academic or commercial contexts.\n\nUX/UI designers working with data products could integrate these principles into dashboard design, analytics platforms, and data-driven applications to prioritize user needs over technical efficiency. The design-driven data principle would guide participatory design processes.\n\nEducators teaching data literacy could incorporate the data drawing and inspiring data principles to help students develop critical thinking about data sources and representation.","keywords":["Data Humanism","Critical Data Visualization","Human-Centered Visualization"],"open_access_supplemental_link":"https://osf.io/tmnra/?view_only=06edc12deed64382a354dac3a7a62049","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1208-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMDgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NSwiZXhwIjoxNzkyNDgyNTY1fQ._areJbJxVhNO4j8-TBr4nRfgfc-dz9UxyCO9uMpJ2QE","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1208","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"200b7b6d-abf5-4cb3-b8b4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Data Humanism Decoded: A Characterization of its Principles to Bridge Data Visualization Researchers and Practitioners","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3d30c2f2-5e29-4f5f-a3bc-05c9fb3e4ea0","abstract":"Selecting the appropriate dimensionality reduction (DR) technique and determining its optimal hyperparameter settings that maximize the accuracy of the output projections typically involves extensive trial and error, often resulting in unnecessary computational overhead. To address this challenge, we propose a dataset-adaptive approach to DR optimization guided by structural complexity metrics. These metrics quantify the intrinsic complexity of a dataset, predicting whether higher-dimensional spaces are necessary to represent it accurately. Since complex datasets are often inaccurately represented in two-dimensional projections, leveraging these metrics enables us to predict the maximum achievable accuracy of DR techniques for a given dataset, eliminating redundant trials in optimizing DR. We introduce the design and theoretical foundations of these structural complexity metrics. We quantitatively verify that our metrics effectively approximate the ground truth complexity of datasets and confirm their suitability for guiding dataset-adaptive DR workflow. Finally, we empirically show that our dataset-adaptive workflow significantly enhances the efficiency of DR optimization without compromising accuracy.","accessible_pdf":null,"authors":[{"affiliation":"Seoul National University","email":"hj@hcil.snu.ac.kr","name":"Hyeon Jeon"},{"affiliation":"Seoul National University","email":"parkjeong02@snu.ac.kr","name":"Jeongin Park"},{"affiliation":"Seoul National University","email":"dtngus0111@gmail.com","name":"Soohyun Lee"},{"affiliation":"Yonsei University","email":"dhkim16@yonsei.ac.kr","name":"Dae Hyun Kim"},{"affiliation":"Inria-Saclay, Universit\u00e9 Paris-Saclay","email":"sbshin90@cs.umd.edu","name":"Sungbok Shin"},{"affiliation":"Seoul National University","email":"jseo@snu.ac.kr","name":"Jinwook Seo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3d30c2f2-5e29-4f5f-a3bc-05c9fb3e4ea0","image_caption":"Domain researchers using dimensionality reduction","keywords":["imensionality reduction","Structural complexity","High-dimensional data","Optimization","Dataset-adaptive workflow"],"open_access_supplemental_link":null,"open_access_supplemental_question":"We publicized the source code","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1019-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTAxOS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjU5LCJleHAiOjE3OTI0Nzk2NTl9.UB8ZANe4CoT2L33Oqbu0v70o6cWrP8laaV0jhba2YC8","preprint_link":"https://arxiv.org/abs/2507.11984","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1019","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3d30c2f2-5e29-4f5f-a3bc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Dataset-Adaptive Dimensionality Reduction","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9e9a15b7-6ccb-4419-b391-e82e3bd8b907","abstract":"Data-driven news articles combine narrative storytelling with data visualizations to inform and influence public opinion on pressing societal issues. These articles often employ persuasive strategies, which are rhetorical techniques in narrative framing, visual rhetoric, or data presentation, to influence audience interpretation and opinion formation regarding information communication. While previous research has examined whether and when data visualizations persuade, the strategic choices made by persuaders remain largely unexplored. Addressing this gap, our work presents a taxonomy of persuasive strategies grounded in psychological theories and expert insights, categorizing 15 strategies across five dimensions: Credibility, Guided Interpretation, Reference-based Framing, Emotional Appeal, and Participation Invitation. To facilitate large-scale analysis, we curated a dataset of 936 data-driven news articles annotated with both persuasive strategies and their perceived effects. Leveraging this corpus, we developed a multimodal, multi-task learning model that jointly predicts the presence of persuasive strategies and their persuasive effects by incorporating both embedded (text and visualization) and explicit (visual narrative and psycholinguistic) features. Our evaluation demonstrates that our model outperforms state-of-the-art baselines in identifying persuasive strategies and measuring their effects.","accessible_pdf":null,"authors":[{"affiliation":"Tongji University","email":"2411927@tongji.edu.cn","name":"Zikai Li"},{"affiliation":"Tongji University","email":"chuyizheng02@gmail.com","name":"Chuyi Zheng"},{"affiliation":"Tongji University","email":"ankerlee168@gmail.com","name":"Ziang Li"},{"affiliation":"College of Design and Innovation, Tongji University","email":"shiyang1230@gmail.com","name":"Yang Shi"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9e9a15b7-6ccb-4419-b391-e82e3bd8b907","image_caption":"data scientists, data journalists, journalists, psychologists","keywords":["Data-driven storytelling","persuasive strategies","taxonomy","computational modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1168-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTE2OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQzLCJleHAiOjE3OTI0Nzk3NDN9.9dJ0jbSx-SFbEKNdCbH7ueistzrGOHAxo4HYtzhV0t8","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1168","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9e9a15b7-6ccb-4419-b391","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Data Speaks, But Who Gives It a Voice? Understanding Persuasive Strategies in Data-Driven News Articles","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"30cf9291-b159-4a33-95bd-47f194b893a0","abstract":"Creating aesthetically pleasing data visualizations remains challenging for users without design expertise or familiarity with visualization tools. To address this gap, we present DataWink, a system that enables users to create custom visualizations by adapting high-quality examples. Our approach combines large multimodal models (LMMs) to extract data encoding from existing SVG-based visualization examples, featuring an intermediate representation of visualizations that bridges primitive SVG and visualization programs. Users may express adaptation goals to a conversational agent and control the visual appearance through widgets generated on demand. With an interactive interface, users can modify both data mappings and visual design elements while maintaining the original visualization's aesthetic quality. To evaluate DataWink, we conduct a user study (N=12) with replication and free-form exploration tasks. As a result, DataWink is recognized for its learnability and effectiveness in personalized authoring tasks. Our results demonstrate the potential of example-driven approaches for democratizing visualization creation.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology","email":"liwenhan.xie@connect.ust.hk","name":"Liwenhan Xie"},{"affiliation":"The Hong Kong University of Science and Technology","email":"ylindg@connect.ust.hk","name":"Yanna Lin"},{"affiliation":"Nanyang Technological University","email":"can.liu.1996@gmail.com","name":"Can Liu"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"},{"affiliation":"Newcastle University","email":"xinhuan.shu@gmail.com","name":"Xinhuan Shu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"30cf9291-b159-4a33-95bd-47f194b893a0","image_caption":"Data journalists and everyday users can be interested in reading this paper to learn about new ways to create aesthetically pleasing charts by reusing and adapting existing examples.","keywords":["Visualization template","Lazy data binding","Visualization by example","Dynamic abstractions"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1029-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTAyOS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYxLCJleHAiOjE3OTI0Nzk2NjF9.dEpY5zlnsveS-4ZjBVUDvjRfGTb1fndaWrLKL7Xqf5I","preprint_link":"https://arxiv.org/abs/2507.17734","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1029","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"30cf9291-b159-4a33-95bd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9d0c979c-02fc-47d5-9f8f-50fc27cf9c35","abstract":"We conduct a deconstructive reading of a qualitative interview study with 17 visual data journalists from newsrooms across the globe. We borrow a deconstruction approach from literary critique to explore the instability of meaning in language and reveal implicit beliefs in words and ideas. Through our analysis we surface two sets of opposing implicit beliefs in visual data journalism: objectivity/subjectivity and humanism/mechanism. We contextualize these beliefs through a genealogical analysis, which brings deconstruction theory into practice by providing a historic backdrop for these opposing perspectives. Our analysis shows that these beliefs held within visual data journalism are not self-enclosed but rather a product of external societal forces and paradigm shifts over time. Through this work, we demonstrate how thinking with critical theories such as deconstruction and genealogy can reframe success in visual data storytelling and diversify visualization research outcomes. These efforts push the ways in which we as researchers produce domain knowledge to examine the sociotechnical issues of today's values towards datafication and data visualization. All supplemental materials for this work are available at osf.io/5fr48.","accessible_pdf":null,"authors":[{"affiliation":"University of Bergen","email":"ke.zhang@uib.no","name":"Ke Er Zhang"},{"affiliation":"University of Toronto","email":"j.jenkinson@utoronto.ca","name":"Jodie Jenkinson"},{"affiliation":"University of Bergen","email":"laura.garrison@uib.no","name":"Laura Garrison"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9d0c979c-02fc-47d5-9f8f-50fc27cf9c35","image_caption":"Visual data journalists as well as visualization practitioners (designers and developers) can better understand their own attitudes towards data and design using insights into the design philosophies and practices of other data journalists from newsrooms across the globe.","keywords":["Visualization","visual data journalism","epistemology","critical theory","poststructuralism","genealogy","deconstruction"],"open_access_supplemental_link":"https://osf.io/5fr48/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1666-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY2Ni1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTQwLCJleHAiOjE3OTI0ODA1NDB9.87azw5EAWeX18UeKmjHVVid3xnftvM35BLmvJrK1NRM","preprint_link":"https://arxiv.org/abs/2507.12377","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1666","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9d0c979c-02fc-47d5-9f8f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f4073f80-c0bd-4d83-9c72-3dcfd060ef7f","abstract":"Although data visualization is powerful for revealing patterns and communicating insights, creating effective visualizations requires familiarity with authoring tools and often disrupts the analysis flow. While large language models show promise for automatically converting analysis intent into visualizations, existing methods function as black boxes without transparent reasoning processes, which prevents users from understanding design rationales and refining suboptimal outputs. To bridge this gap, we propose integrating Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization (NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for NL2VIS and develop an automatic pipeline to equip existing datasets with structured reasoning steps. Second, we introduce nvBench-CoT, a specialized dataset capturing detailed step-by-step reasoning from ambiguous natural language descriptions to finalized visualizations, which enables state-of-the-art performance when used for model fine-tuning. Third, we develop DeepVIS, an interactive visual interface that tightly integrates with the CoT reasoning process, allowing users to inspect reasoning steps, identify errors, and make targeted adjustments to improve visualization outcomes. Quantitative benchmark evaluations, two use cases, and a user study collectively demonstrate that our CoT framework effectively enhances NL2VIS quality while providing insightful reasoning steps to users.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"zhihaoshuai@hkust-gz.edu.cn","name":"Zhihao Shuai"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"bli303@connect.hkust-gz.edu.cn","name":"Boyan LI"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"syan195@connect.hkust-gz.edu.cn","name":"siyu yan"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yuyuluo@hkust-gz.edu.cn","name":"Yuyu Luo"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"weikaiyang@hkust-gz.edu.cn","name":"Weikai Yang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f4073f80-c0bd-4d83-9c72-3dcfd060ef7f","image_caption":"Data scientists can use the transparent reasoning as interactive tutorials, showing junior analysts how ambiguous questions are broken down into formal specifications.","keywords":["Data visualization","automatic visualization","large language models"],"open_access_supplemental_link":"https://github.com/Bvivib-shuai/DeepVIS","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1818-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTgxOC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.FXCLwdZIclGXTMVWCwenvCot0Ed0G7ZmeUnu46zcPAo","preprint_link":"https://arxiv.org/pdf/2508.01700","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1818","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f4073f80-c0bd-4d83-9c72","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"184d62a7-fb82-4d6f-9819-6f552e1c6c28","abstract":"Visualizing data often entails data transformations that can reveal and hide information, operations we dub disclosure tactics. Whether designers hide information intentionally or as an implicit consequence of other design choices, tools and frameworks for visualization offer little explicit guidance on disclosure. To systematically characterize how visualizations can limit access to an underlying dataset, we contribute a content analysis of 425 examples of visualization techniques sampled from academic papers in the visualization literature, resulting in a taxonomy of disclosure tactics. Our taxonomy organizes disclosure tactics based on how they change the data representation underlying a chart, providing a systematic way to reason about design trade-offs in terms of what information is revealed, distorted, or hidden. We demonstrate the benefits of using our taxonomy by showing how it can guide reasoning in design scenarios where disclosure is a first-order consideration. Adopting disclosure as a framework for visualization research offers new perspective on authoring tools, literacy, uncertainty communication, personalization, and ethical design.","accessible_pdf":null,"authors":[{"affiliation":"University of Chicago","email":"krisha@uchicago.edu","name":"Krisha Mehta"},{"affiliation":"University of Chicago","email":"glk@uchicago.edu","name":"Gordon Kindlmann"},{"affiliation":"University of Chicago","email":"kalea@uchicago.edu","name":"Alex Kale"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"184d62a7-fb82-4d6f-9819-6f552e1c6c28","image_caption":"Data visualization practitioners in general can be interested in reading this paper. The taxonomy provides a shared vocabulary for describing how data transformation choices can distort signals of interest, enabling practitioners to better recognize and discuss design trade-offs, avoid unintended misrepresentations, and select visualization techniques that align with their audience\u2019s decision-making needs.","keywords":["Information disclosure"],"open_access_supplemental_link":"https://github.com/krisha-mehta/DisclosureInDataVis","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1255-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI1NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.zuktZvkP7IWxkaCKKK-eSej1LWIY2ncDWk0iV9mMD7U","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1255","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"184d62a7-fb82-4d6f-9819","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Designing for Disclosure in Data Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"d117a849-8b10-47ad-babd-f387afab0563","abstract":"We present a systematic review on tasks, interactions, and visualization widgets (refer to tangible entities that are used to accomplish data exploration tasks through specific interactions) in the context of tangible data exploration. Tangible widgets have been shown to reduce cognitive load, enable more natural interactions, and support the completion of complex data exploration tasks. Yet, the field lacks a structured understanding of how task types, interaction methods, and widget designs are coordinated, limiting the ability to identify recurring design patterns and opportunities for innovation. To address this gap, we conduct a systematic review to analyze existing work and characterize the current design of data exploration tasks, interactions, and tangible visualization widgets. We next reflect based on our findings and propose a research agenda to inform the development of a future widget design toolkit for tangible data exploration. Our systematic review and supplemental materials are available at physicalviswidget.github.io and osf.io/vjw5e/.","accessible_pdf":"Accessible","authors":[{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"lyeveldie721@gmail.com","name":"Haonan Yao"},{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"lingyun.yu@xjtlu.edu.cn","name":"Lingyun Yu"},{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"yaolijie0219@gmail.com","name":"Lijie Yao"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"d117a849-8b10-47ad-babd-f387afab0563","image_caption":"Interaction designers","keywords":["Visualization widget","Tangible interaction","Data exploration"],"open_access_supplemental_link":"https://physicalviswidget.github.io/, https://osf.io/vjw5e/","open_access_supplemental_question":"Original dataset of our systematic review","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1348-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNDgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.D-QpXspqx1JZ0XEOo1km1Xftc3iFmvqdT-xrRtMLMY4","preprint_link":"https://doi.org/10.48550/arXiv.2507.00775","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1348","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"d117a849-8b10-47ad-babd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1af413bb-b7e1-4e7a-bbba-ec896ec760e7","abstract":"Various computational approaches predict chromatin structure, yielding concrete models that position genomic loci in physical space and help reveal genome organization and function. While prior visualization research has explored data and task abstractions for genomics, the design space for depicting these three-dimensional (3D) genome models---and associated genome-mapped data---remains unclear. In this paper, we investigate the visualization of genomic data with a spatial component. First, we systematically survey how 3D genome models are used and depicted in computational biology. We analyze over 300 papers with figures that visualize 3D genomic data and categorize the methods for visual representation. From this survey, we derive a design space for visualizing 3D genome data, identifying common patterns and key properties such as representation, visual channels, and composition. We position these findings within an existing genomics visualization taxonomy, refining and extending existing classifications. Second, we augment Gosling, a declarative visualization grammar for genomics, to support 3D genomic data. Our integration enables expressive authoring of visualizations that connect traditional genome-mapped information with 3D genome models, emphasizing their spatial characteristics. To demonstrate its utility, we employ our extended grammar to recreate interactive examples, showcasing its ability to represent complex visual designs. Comprehensive examples and an interactive editor are available at https://3d.gosling-lang.org.","accessible_pdf":"Accessible","authors":[{"affiliation":"Harvard Medical School","email":"david_kouril@hms.harvard.edu","name":"David Kou\u0159il"},{"affiliation":"Harvard Medical School","email":"trevor_manz@g.harvard.edu","name":"Trevor Manz"},{"affiliation":"Harvard Medical School","email":"sehi_lyi@hms.harvard.edu","name":"Sehi L'Yi"},{"affiliation":"Harvard Medical School","email":"nils@hms.harvard.edu","name":"Nils Gehlenborg"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"1af413bb-b7e1-4e7a-bbba-ec896ec760e7","image_caption":"Biologists working in genomics, and especially the ones studying chromatin organization, might be interested in the system's ability to link simulated 3D structures with other types of genomics data.","keywords":["genomic data","3D visualization","declarative grammars"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1080-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA4MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjcyLCJleHAiOjE3OTI0Nzk2NzJ9.E7Ot6F8BGFOPW8Yw4wstq5Sqo6hx34uucgYCMoP1t44","preprint_link":"https://doi.org/10.31219/osf.io/dtr6u_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1080","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1af413bb-b7e1-4e7a-bbba","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Design Space and Declarative Grammar for 3D Genomic Data Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1a6e079e-6775-4742-80e6-1bfa9f98605d","abstract":"Adjusting transparency is a common method of mitigating occlusion but is often detrimental for understanding the relative depth relationships between objects as well as removes potentially important information from the occluding object. We propose using dichoptic opacity, a novel method for occlusion management that contrasts the transparency of occluders presented to each eye. This allows for better simultaneous understanding of both occluder and occluded. A user study highlights the technique's potential, showing strong user engagement and a clear preference for dichoptic opacity over traditional presentations. While it does not determine optimal transparency values, it reveals promising trends in both percentage and range that merit further investigation.","accessible_pdf":"Accessible","authors":[{"affiliation":"Newcastle University","email":"g.bell1@newcastle.ac.uk","name":"George Bell"},{"affiliation":"Newcastle University","email":"alma.cantu@ncl.ac.uk","name":"Alma Cantu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1a6e079e-6775-4742-80e6-1bfa9f98605d","image_caption":"This paper will be of interest to data scientists, specifically those already making use of stereoscopic or immersive technology. It proposes a novel method of handling occlusion of 3D data that has potential to improve both depth perception, and define details of both occluder and occludee. With modern data acquisition leading to more complex, or dense, 3D visualisations, finding additional ways to solve the resulting occlusion is increasingly important. There is already a breadth of research exploring transparency for occlusion management, but this paper presents not only a foundational way of potentially improving these, but introduces a novel use of stereoscopic technology.","keywords":["Occlusion","dichoptic presentation","transparency","3D surface","stereoscopy"],"open_access_supplemental_link":"https://github.com/georgebellbell/Dichoptic-Opacity-Experiment-Data.git","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1262-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNjItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.I1FOLuu6a0VbBhOxDZDiyq1Slc8CFpZKp1zADhl5jw8","preprint_link":"https://doi.org/10.48550/arXiv.2506.22841","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1262","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1a6e079e-6775-4742-80e6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3d649242-7596-4300-84fe-793b6a69b69c","abstract":"Examining vision-language alignment in multimodal embeddings is crucial for various tasks, such as evaluating generative models and filtering pretraining data. The intricate nature of high-dimensional features necessitates dimensionality reduction (DR) methods to explore alignment of multimodal embeddings. However, existing DR methods fail to account for cross-modal alignment metrics, resulting in severe occlusion of points with divergent metrics clustered together, inaccurate contour maps from over-aggregation, and insufficient support for multi-scale exploration. To address these problems, this paper introduces DKMap, a novel DR visualization technique for interactive exploration of multimodal embeddings through Dynamic Kernel enhanced projection. First, rather than performing dimensionality reduction and contour estimation sequentially, we introduce a kernel regression supervised t-SNE that directly integrates post-projection contour mapping into the projection learning process, ensuring cross-modal alignment mapping accuracy. Second, to enable multi-scale exploration with dynamic zooming and progressively enhanced local detail, we integrate validation-constrained \u03b1 refinement of a generalized t-kernel with quad-tree-based multi-resolution technique, ensuring reliable kernel parameter tuning without overfitting. DKMap is implemented as a multi-platform visualization tool, featuring a web-based system for interactive exploration and a Python package for computational notebook analysis. Quantitative comparisons with baseline DR techniques demonstrate DKMap\u2019s superiority in accurately mapping cross-modal alignment metrics. We further demonstrate generalizability and scalability of DKMap with three usage scenarios, including visualizing million-scale text-to-image corpus, comparatively evaluating generative models, and exploring a billion-scale pretraining dataset.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yyebd@connect.ust.hk","name":"Yilin Ye"},{"affiliation":"South China University of Technology","email":"chenxiruan64@gmail.com","name":"Chenxi Ruan"},{"affiliation":"University of Oxford","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"South China University of Technology","email":"zkdeng@scut.edu.cn","name":"Zikun Deng"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"zengwei81@gmail.com","name":"Wei Zeng"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3d649242-7596-4300-84fe-793b6a69b69c","image_caption":"data scientists","keywords":["Kernel Regression","Vision-language Alignment","Multimodal Embeddings","Interactive Exploration"],"open_access_supplemental_link":"https://github.com/HKUST-CIVAL/DKMap","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1781-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc4MS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.mIEkuipp1uuDChYcICrqdgZPNtoFBcSMQvvq4GqkYzw","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1781","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3d649242-7596-4300-84fe","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"DKMap: Interactive Exploration of Vision-Language Alignment in Multimodal Embeddings via Dynamic Kernel Enhanced Projection","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"dc011ac4-7257-4423-9aa4-a46452c10067","abstract":"Charts and graphs help people analyze data, but can they also be useful to AI systems? To investigate this question, we perform a series of experiments with two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three representative analysis tasks, the two systems describe synthetic datasets more precisely and accurately when raw data is accompanied by a scatterplot, especially as datasets grow in complexity. Comparison with two baselines---providing a blank chart and a chart with mismatched data---shows that the improved performance is due to the content of the charts. Our results are initial evidence that AI systems, like humans, can benefit from visualization.","accessible_pdf":null,"authors":[{"affiliation":"Harvard","email":"jlsun@college.harvard.edu","name":"Johnathan Sun"},{"affiliation":"Harvard","email":"vrli@college.harvard.edu","name":"Victoria Li"},{"affiliation":"Harvard","email":"wattenberg@gmail.com","name":"Martin Wattenberg"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"dc011ac4-7257-4423-9aa4-a46452c10067","image_caption":"Anyone who works with datasets, ranging from scientists to data journalists and students, could be interested in reading this paper. Our results suggest that visualization may have a significant role to play in AI-enabled data analysis workflows. As AI tools become increasingly ubiquitous and sometimes autonomous, anyone interested in getting the most out of them might be curious about the importance of visual input in facilitating model data understanding. \n\nIndeed, we hope the visualization community, including scientists, designers, and developers, take interest in visualization for AI as a new and impactful design space. Our research, while preliminary, may lead to practical changes in the ways we present quantitative information to AI models across knowledge domains.\n\nPractitioners, especially when conducting exploratory data analysis, could take away trying both scenarios: (1) inputting full datasets into a model (even a couple hundred points) with a simple visualization, and (2) just the visualization itself as a starting point to understand salient dataset characteristics. Both scenarios may result in helpful analysis, but our results suggest the first may include convenient statistical data analysis, and the second a concise summary of standout features. This quick, accessible workflow could help any data analyst collaborate more effectively with AI systems.\n\nPractitioners would also learn how easily AIs may be silently tripped up by inconsistent data and visualization inputs. They might begin exercising more caution when conducting exploratory data and visual analysis with an AI, which may not realize or mention potential discrepancies in its inputs.\n\nFinally, practitioners may also be excited about how visualization design may evolve in the future. If the VIS community builds on our work to design charts or graphs that more effectively elicit insights from models, AI-enabled workflows could become more powerful and efficient, allowing humans and AI to jointly tackle more complex empirical questions. Overall, our results lay groundwork for both immediate and future implications for practitioners in a world with increasingly powerful AI systems.","keywords":["AI","Workflow Design","Human-Machine Analysis."],"open_access_supplemental_link":"https://github.com/johnathansun/lvlm-vis-data-understanding","open_access_supplemental_question":"Our source code is thoroughly documented, and we provide original, full datasets on Github for end-to-end verification and reproducibility. Our supplemental material contains additional analytics and graphics to document our experimental setup and results.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1345-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNDUtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.3mCT3BuY_btTQB2EATVJmcFcf3gHalsFuH1orZDDw2o","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1345","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"dc011ac4-7257-4423-9aa4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Does visualization help AI understand data?","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"266aaadd-b874-4660-9061-3746f4ce4747","abstract":"Railroad networks are complex systems where a single disruption can have cumulative effects, impacting services' planned schedules and degrading the network's performance. To support railroad planners in identifying these disruptions, we propose a Visual Analytics approach with multiple coordinated views: a map, calendar heatmap, Marey graph, and a multi-line chart. Our proposed approach enables the exploration and analysis of the disruptions' spatial and temporal patterns and delay accumulation to identify vulnerable segments in the railroad network. We assess the effectiveness of our approach through an expert interview highlighting how the accumulation of delays and disruptions is lucidly communicated and provides valuable insights into their spread across the network. We discuss the outcomes of the expert interview alongside the limitations we identified and how we resolve these. Finally, we illustrate directions for future work, including online data to assist railroad planners with real-time monitoring for proactive decision-making and improved operations.","accessible_pdf":"Accessible","authors":[{"affiliation":"Institute of Visual Computing and Human-Centered Technology","email":"sandhya.rajendran@tuwien.ac.at","name":"Sandhya Rajendran"},{"affiliation":"Eindhoven University of Technology","email":"a.arleo@tue.nl","name":"Alessio Arleo"},{"affiliation":"University of Cologne","email":"landesberger@cs.uni-koeln.de","name":"Tatiana von Landesberger"},{"affiliation":"Institute of Visual Computing and Human-Centered Technology","email":"miksch@ifs.tuwien.ac.at","name":"Silvia Miksch"},{"affiliation":"University of Cologne","email":"max.sondag93@gmail.com","name":"Max Sondag"},{"affiliation":"TU Wien","email":"michaela.tuscher@tuwien.ac.at","name":"Michaela Tuscher"},{"affiliation":"Institute of Visual Computing and Human-Centered Technology","email":"velitchko.filipov@tuwien.ac.at","name":"Velitchko Filipov"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"266aaadd-b874-4660-9061-3746f4ce4747","image_caption":"Transportation planners and analysts, Urban informatics or smart city researchers","keywords":["Spatio-temporal Data","Networks"],"open_access_supplemental_link":"https://osf.io/d2fzh/?view_only=2715e41e0e6c4e23812ec2465f4da48e","open_access_supplemental_question":"thoroughly documented source code, particularly substantial supplemental material","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1171-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExNzEtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.L367LDHBv2ARczZp_rIt4s8cy0koKlV36RAYE21RSdA","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1171","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"266aaadd-b874-4660-9061","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Don't Stop Me Now: Visualizing Disruptions in Railroad Networks","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"7dce17af-ccb4-4f41-ab8a-f66a16030913","abstract":"We present \u201cDrawing in the Flow\u201d, a mixed-reality 3D user interface for authoring illustrative, multivariate 3D flow visualizations by sketching on, or better stated, in, 3D data. The approach interprets hand-drawn 3D strokes relative to an underlying data \u201ccanvas\u201d and applies animated \u201cink-data settling\u201d to ensure the strokes accurately reflect vector field data (i.e., 3D streamline paths). Color and other visual properties are interpreted relative to scalar data variables with \u201clazy data binding\u201d to help users prioritize creative visual design tasks. Results include example 3D illustrations of multiple flow fields. The work is significant because of the ability to make authoring accurate 3D mixed reality data visualizations accessible to stakeholders without programming or scripting experience and because it demonstrates a novel approach to balancing the tradeoff between accuracy and expression in 3D data visualization.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"sands224@umn.edu","name":"Walter Sands"},{"affiliation":"University of Minnesota","email":"dorr0024@umn.edu","name":"Sean Dorr"},{"affiliation":"University of Minnesota","email":"tran0563@umn.edu","name":"Kiet Tran"},{"affiliation":"University of Minnesota","email":"dfk@umn.edu","name":"Daniel Keefe"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"7dce17af-ccb4-4f41-ab8a-f66a16030913","image_caption":"Data scientists could benefit from an alternative style of handling dense 3D fluid fields. Teachers that handle 3D fluid flows might like to use this system for highlighting specific parts of a fluid flow simulation in a class or other demonstrative environment.","keywords":["Extended Reality","Sketching","Flow Visualization."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1202-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMDItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.fr79GyGhH_0G4s8pOXjR0M27BSFtHYjv_G1jjx3y_hU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1202","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"7dce17af-ccb4-4f41-ab8a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Drawing in the Flow: A Data-Aware Mixed-Reality Sketching Interface for Illustrative 3D Flow Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"8b2cd3c8-c4e3-483a-a519-bbb3683dc274","abstract":"Embedding projections are popular for visualizing large datasets and models. However, people often encounter friction when using embedding visualization tools: (1) barriers to adoption, e.g., tedious data wrangling and loading, scalability limits, no integration of results into existing workflows, and (2) limitations in possible analyses, without integration with external tools to additionally show coordinated views of metadata. In this paper, we present Embedding Atlas, a scalable, interactive visualization tool designed to make interacting with large embeddings as easy as possible. Embedding Atlas uses modern web technologies and advanced algorithms\u2014including density-based clustering, and automated labeling\u2014to provide a fast and rich data analysis experience at scale. We evaluate Embedding Atlas with a competitive analysis against other popular embedding tools, showing that Embedding Atlas's feature set specifically helps reduce friction, and report a benchmark on its real-time rendering performance with millions of points. Embedding Atlas is available as open source to support future work in embedding-based analysis.","accessible_pdf":"Accessible","authors":[{"affiliation":"Apple","email":"donghao@apple.com","name":"Donghao Ren"},{"affiliation":"Apple","email":"fred.hohman@gmail.com","name":"Fred Hohman"},{"affiliation":"Apple Inc.","email":"halden.lin@gmail.com","name":"Halden Lin"},{"affiliation":"Apple","email":"domoritz@cmu.edu","name":"Dominik Moritz"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"8b2cd3c8-c4e3-483a-a519-bbb3683dc274","image_caption":"The open-source system in this paper is robust and ready to be used by practitioners (and has already been used by developers in our organization). Specific users could include machine learning engineers, data scientists, and people with large amounts of unstructured data such as text or images.","keywords":["Embedding visualization","visual analytics"],"open_access_supplemental_link":"https://apple.github.io/embedding-atlas/","open_access_supplemental_question":"Our paper is a visualization system with multiple implemented algorithms for transforming data, all of which are open-sourced and able to be demoed in a web-browser without any installation.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1147-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExNDctZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.RJXkhsblZw1jy1J2DRve786eLuJbRTxn-ZkDz8mdwB8","preprint_link":"https://www.arxiv.org/abs/2505.06386","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1147","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"8b2cd3c8-c4e3-483a-a519","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Embedding Atlas: Low-Friction, Interactive Embedding Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"33454738-a9a9-4944-a27f-262d1da9f35d","abstract":"Embodiment shapes how users verbally express intent when interacting with data through speech interfaces in immersive analytics. Despite growing interest in Natural Language Interactions (NLIs) for visual analytics in immersive environments, users\u2019 speech patterns and their use of embodiment cues in speech remain underexplored. Understanding their interplay is crucial to bridging the gap between users\u2019 intent and an immersive analytic system. To address this, we report the results from 15 participants in a user study conducted using the Wizard of Oz method. We performed axial coding on 1,280 speech acts derived from 734 utterances, examining how analysis tasks are carried out with embodiment and linguistic features. Next, we measured Speech Input Uncertainty for each analysis task using the semantic entropy of utterances, estimating how uncertain users\u2019 speech inputs appear to an analytic system. Through these analyses, we identified five speech input patterns, showing that users dynamically blend embodied and non-embodied speech acts depending on data analysis tasks, phases, and Embodiment Reliance driven by the counts and types of embodiment cues in each utterance. We then examined how these patterns align with user reflections on factors that challenge speech interaction during the study. Finally, we propose design implications aligned with the five patterns.","accessible_pdf":null,"authors":[{"affiliation":"University of Maryland","email":"hsong02@cs.umd.edu","name":"Hyemi Song"},{"affiliation":"University of Maryland","email":"mjohns28@terpmail.umd.edu","name":"Matthew Johnson"},{"affiliation":"Department of Defense","email":"visual.tycho@gmail.com","name":"Kirsten Whitley"},{"affiliation":"Department of Defense","email":"ericpkrokos@gmail.com","name":"Eric Krokos"},{"affiliation":"University of Maryland","email":"varshney@cs.umd.edu","name":"Amitabh Varshney"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"33454738-a9a9-4944-a27f-262d1da9f35d","image_caption":"Researchers, Developers, Designers","keywords":["Embodiment","Natural Language Interaction (NLI)","Immersive Analytics","Speech Patterns","Semantic Entropy","User Intent","Speech Acts"],"open_access_supplemental_link":"https://osf.io/sv4fn/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1747-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc0Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.s6fGM-5lFn9V8DO92XOoqD2bkIUDHuwi4Kye9nwCHXg","preprint_link":"Link (arXiv) will be updated upon release before the conference.","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1747","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"33454738-a9a9-4944-a27f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Embodied Natural Language Interaction (NLI): Speech Input Patterns in Immersive Analytics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"28a70097-2b4f-4ac5-9bb0-7b452093c16b","abstract":"In-vitro fertilization (IVF) has become standard practice to address infertility, which affects more than one in ten couples in the US. However, current protocols yield relatively low success rates of about 20% per treatment cycle. A critical but complex and time-consuming step is the grading and selection of embryos for implantation. Although incubators with time-lapse microscopy have enabled computational analysis of embryo development, existing automated approaches either require extensive manual annotations or use opaque deep learning models that are hard for clinicians to validate and trust. We present EmbryoProfiler, a visual analytics system collaboratively developed with embryologists, biologists, and machine learning researchers to support clinicians in visually assessing embryo viability from time-lapse microscopy imagery. Our system incorporates a deep learning pipeline that automatically annotates microscopy images and extracts clinically interpretable features relevant for embryo grading. Our contributions include: (1) a semi-automatic, visualization-based workflow that guides clinicians through fertilization assessment, developmental timing evaluation, morphological inspection, and comparative analysis of embryos; (2) innovative interactive visualizations, such as cell-shape plots, designed to facilitate efficient analysis of morphological and developmental characteristics; and (3) an integrated, explainable machine learning classifier offering transparent, clinically-informed embryo viability scoring to predict live birth outcomes. Quantitative evaluation of our classifier and qualitative case studies conducted with practitioners demonstrate that EmbryoProfiler enables clinicians to make better-informed embryo selection decisions, potentially leading to improved clinical outcomes in IVF treatments.","accessible_pdf":null,"authors":[{"affiliation":"Harvard University","email":"jknittel@seas.harvard.edu","name":"Johannes Knittel"},{"affiliation":"Harvard University","email":"simonwarchol@g.harvard.edu","name":"Simon Warchol"},{"affiliation":"Harvard University","email":"jakob.troidl@googlemail.com","name":"Jakob Troidl"},{"affiliation":"Tufts University","email":"camelia_daniela.brumar@tufts.edu","name":"Camelia D. Brumar"},{"affiliation":"Harvard University","email":"yuy068@g.harvard.edu","name":"Helen Yang"},{"affiliation":"Harvard Medical School","email":"eric.moerth@gmx.at","name":"Eric M\u00f6rth"},{"affiliation":"New York University","email":"rk4815@nyu.edu","name":"Robert Kr\u00fcger"},{"affiliation":"Harvard University","email":"dan.needleman@gmail.com","name":"Daniel Needleman"},{"affiliation":"Tel Aviv University","email":"dalitb@tasmc.health.gov.il","name":"Dalit Ben-Yosef"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"28a70097-2b4f-4ac5-9bb0-7b452093c16b","image_caption":"Our work supports computational biologists in analyzing time-lapse images and patient characteristics from in-vitro fertilization (IVF) treatments. Clinicians, including doctors and embryologists, can use our methods to streamline embryo grading workflows, potentially improving clinical outcomes in IVF procedures.","keywords":["in-vitro fertilization","embryo selection","visual analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1893-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTg5My1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.fzZP8iMbXjjcmrd87Id7gEizRlUMKZ0ewhvrdkXEOUc","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1893","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"28a70097-2b4f-4ac5-9bb0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"EmbryoProfiler: A Visual Clinical Decision Support System for IVF","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9942014b-e0f8-4cae-9b2a-81167d226c7b","abstract":"Multimodal vision-language models (VLMs) continue to achieve ever-improving scores on chart understanding benchmarks. Yet, we find that this progress does not fully capture the breadth of visual reasoning capabilities essential for interpreting charts. We introduce ENCQA, a novel benchmark informed by the visualization literature, designed to provide systematic coverage of visual encodings and analytic tasks that are crucial for chart understanding. ENCQA provides 2,076 synthetic question-answer pairs, enabling balanced coverage of six visual encoding channels (position, length, area, color quantitative, color nominal, and shape) and eight tasks (find extrema, retrieve value, find anomaly, filter values, compute derived value exact, compute derived value relative, correlate values, and correlate values relative). Our evaluation of 9 state-of-the-art VLMs reveals that performance varies significantly across encodings within the same task, as well as across tasks. Contrary to expectations, we observe that performance does not improve with model size for many task-encoding pairs. Our results suggest that advancing chart understanding requires targeted strategies addressing specific visual reasoning gaps, rather than solely scaling up model or dataset size.","accessible_pdf":null,"authors":[{"affiliation":"Stanford University","email":"kushinm11@gmail.com","name":"Kushin Mukherjee"},{"affiliation":"Apple","email":"donghao@apple.com","name":"Donghao Ren"},{"affiliation":"Apple","email":"domoritz@cmu.edu","name":"Dominik Moritz"},{"affiliation":"Apple","email":"yassogba@gmail.com","name":"Yannick Assogba"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9942014b-e0f8-4cae-9b2a-81167d226c7b","image_caption":"Machine learning researchers interested in improving chart-understanding in vision-language models and data visualization practitioners interested in leveraging vision-language models for automatically deriving insights from charts and graphs in documents and the web.","keywords":["Visual encodings","visualization understanding tasks","machine chart understanding","vision-language models","model benchmarking"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1590-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU5MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.B2TL8t_siKSuYoA7ZPAehmS5ckwI245UooQ5GUllfEg","preprint_link":"https://arxiv.org/pdf/2508.04650v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1590","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9942014b-e0f8-4cae-9b2a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"851f1b42-91da-4dcc-a08e-c6d8e564d251","abstract":"Interpreting data visualizations is an essential skill in today\u2019s education, yet students often struggle with understanding unfamiliar formats. This study investigates how four learning materials \u2013 textbook, comic, video, and game \u2013 affect middle- and high school students\u2019 ability to interpret line charts, area charts, stacked area charts, and stream graphs. We conducted a comparative classroom study with 68 students, using pre- and post-tests, worksheet activities, and group discussions to assess learning outcomes and understanding. Our results show statistically significant improvement in students\u2019 understanding of stacked area charts and streamgraphs, while no significant differences between the learning materials were found. This suggests that more factors than initially anticipated \u2013 such as engagement, motivation and active learning strategies \u2013 influence the learning outcome. The analysis of the worksheets revealed that while students could infer surface-level insights from charts, over 70% struggled to identify underlying patterns or relationships. Additionally, a common challenge across all learning materials was reading fatigue, which often led students to skim content, disengage, or misinterpret key information. These findings highlight the need for educational tools and approaches that foster deeper understanding of unfamiliar visualizations, reduce cognitive load, and encourage active engagement.","accessible_pdf":"Accessible","authors":[{"affiliation":"St. P\u00f6lten University of Applied Sciences","email":"magdalena.boucher@fhstp.ac.at","name":"Magdalena Boucher"},{"affiliation":"Masaryk University","email":"makej@mail.muni.cz","name":"Magdalena Kejstova"},{"affiliation":"St. Poelten University of Applied Sciences","email":"christina.stoiber@fhstp.ac.at","name":"Christina Stoiber"},{"affiliation":"Austrian Computer Society","email":"martin.kandlhofer@ocg.at","name":"Martin Kandlhofer"},{"affiliation":"Austrian Computer Society","email":"leniproduction@gmail.com","name":"Alena Boucher"},{"affiliation":"Masaryk University","email":"simone.kriglstein@univie.ac.at","name":"Simone Kriglstein"},{"affiliation":"Erich Fried Realgymnasium","email":"shelley.buchinger@brg9.at","name":"Shelley Buchinger"},{"affiliation":"St. Poelten University of Applied Sciences","email":"wolfgang.aigner@fhstp.ac.at","name":"Wolfgang Aigner"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"851f1b42-91da-4dcc-a08e-c6d8e564d251","image_caption":"Our findings are relevant for visualization educators and designers of learning materials  as they shed light on visualization literacy and student preferences in middle- and high schools.","keywords":["visualization education","schools","learning materials","visualization literacy"],"open_access_supplemental_link":"https://phaidra.fhstp.ac.at/detail/o:7304","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1863-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTg2My1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.LE5fAigXM1klY3wa3np7PG18Pbf1sPWARgNIUeVhadQ","preprint_link":"https://phaidra.fhstp.ac.at/detail/o:7302","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1863","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"851f1b42-91da-4dcc-a08e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Enhancing Data Visualization Literacy: A Comparative Study of Learning Materials in Schools","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"494d7752-3810-46fa-906c-610bb1c013ef","abstract":"As AI systems become increasingly integrated into high-stakes domains, enabling users to accurately interpret model behavior is critical. While AI explanations are readily available, users often struggle to reason effectively with these explanations, limiting their ability to validate or learn from AI decisions. To address this gap, we introduce Reverse Mapping, a novel approach that enhances visual explanations by incorporating user-derived insights back into the explanation workflow. Our system extracts structured insights from free-form user interpretations using a large language model and maps them back onto visual explanations through interactive annotations and coordinated multi-view visualizations. Inspired by the verification loop in the visualization knowledge generation model, this design aims to foster more deliberate, reflective interaction with AI explanations. We demonstrate our approach in a prototype system with two use cases and qualitative user feedback.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"nutha010@umn.edu","name":"Aniket Nuthalapati"},{"affiliation":"University of Minnesota Twin Cities","email":"hinds084@umn.edu","name":"Nicholas Hinds"},{"affiliation":"National University of Singapore","email":"brianlim@comp.nus.edu.sg","name":"Brian Lim"},{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"494d7752-3810-46fa-906c-610bb1c013ef","image_caption":"AI/ML practitioners, visualization researchers, and HCI experts would be most interested in this paper, as it addresses the critical challenge of making AI explanations more interpretable and actionable. \n\nData scientists and domain experts in high-stakes fields (healthcare, finance, policy) could apply the reverse mapping approach to create more engaging explanation interfaces and build appropriate trust in AI-assisted decision-making systems.","keywords":["XAI Visualization","reverse mapping","insight verification","explainable AI"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1124-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMjQtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.m_v-vlnxzrcL62b_ivHd7_4ul07KRjGVDJzObRC9m9Q","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1124","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"494d7752-3810-46fa-906c","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ae49090b-23a7-453b-8d5d-eb0ad8893a08","abstract":"Graph querying is the process of retrieving information from graph data using specialized languages (e.g., Cypher), often requiring programming expertise. Visual Graph Querying (VGQ) streamlines this process by enabling users to construct and execute queries via an interactive interface without resorting to complex coding. However, current VGQ tools only allow users to construct simple and specific query graphs, limiting users' ability to interactively express their query intent, especially for underspecified query intent. To address these limitations, we propose Envisage, an interactive visual graph querying system to enhance the expressiveness of VGQ in complex query scenarios by supporting intuitive graph structure construction and flexible parameterized rule specification. Specifically, Envisage comprises four stages: Query Expression allows users to interactively construct graph queries through intuitive operations; Query Verification enables the validation of constructed queries via rule verification and query instantiation; Progressive Query Execution can progressively execute queries to ensure meaningful querying results; and Result Analysis facilitates result exploration and interpretation. To evaluate Envisage, we conducted two case studies and in-depth user interviews with 14 graph analysts. The results demonstrate its effectiveness and usability in constructing, verifying, and executing complex graph queries.","accessible_pdf":"Accessible","authors":[{"affiliation":"Nanyang Technological University","email":"xiaolin004@e.ntu.edu.sg","name":"Xiaolin Wen"},{"affiliation":"Monash University","email":"qishuang.fu@monash.edu","name":"Qishuang Fu"},{"affiliation":"Nanyang Technological University","email":"shuangyu001@e.ntu.edu.sg","name":"Shuangyue Han"},{"affiliation":"Nanyang Technological University","email":"yguo017@e.ntu.edu.sg","name":"Yichen Guo"},{"affiliation":"Monash University","email":"joseph.liu@monash.edu","name":"Joseph Liu"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"ae49090b-23a7-453b-8d5d-eb0ad8893a08","image_caption":"Graph analyst, visualization researcher\n\nThey can use our tool to conduct the graph querying and learn the interaction design of our tools.","keywords":["Visual graph querying","interactive query construction","graph data"],"open_access_supplemental_link":"https://github.com/Selvalim/VGQ-front","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1567-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU2Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.rzuAUMg7XFNjDDwlsThoiJeFQZcTeIM3-7COVL_v98I","preprint_link":"https://arxiv.org/abs/2507.11999","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1567","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ae49090b-23a7-453b-8d5d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Envisage: Towards Expressive Visual Graph Querying","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"14e8969b-dbaf-41b3-8330-a0decc6b53b0","abstract":"In this work we study the identification of spatial correlation in distributions of 2D scalar fields, presented across different forms of visual displays. We study simple visual displays that directly show color-mapped scalar fields, namely those drawn from a distribution, and whether humans can identify strongly correlated spatial regions in these displays. In this setting, the recognition of correlation requires making judgments on a set of fields, rather than just one field. Thus, in our experimental design we compare two basic visualization designs: animation-based displays against juxtaposed views of scalar fields, along different choices of color scales. Moreover, we investigate the impacts of the distribution itself, controlling for the level of spatial correlation and discriminability in spatial\nscales. Our study\u2019s results illustrate the impacts of these distribution characteristics, while also highlighting how different visual displays impact the types of judgments made in assessing spatial correlation. Supplemental material is available at https://osf.io/zn4qy/","accessible_pdf":null,"authors":[{"affiliation":"Vanderbilt University","email":"yayan.zhao@vanderbilt.edu","name":"Yayan Zhao"},{"affiliation":"Vanderbilt University","email":"matthew.berger@vanderbilt.edu","name":"Matthew Berger"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"14e8969b-dbaf-41b3-8330-a0decc6b53b0","image_caption":"Simulation scientists might be interested in reading this paper. The results of our study suggest that simple, direct visualizations of scalar field distributions - a common output of numerical simulations - could support users in a variety of tasks. The paper is focused on spatial correlation, but potentially other tasks could be supported.","keywords":["Evaluation","spatial correlation","color maps"],"open_access_supplemental_link":"https://osf.io/zn4qy","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1495-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ5NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQzLCJleHAiOjE3OTI0ODAzNDN9.3VeKrlHCHLySUoMZkFAK7HeY8kSWhuIPHmnUjiEo-k0","preprint_link":"https://arxiv.org/abs/2507.17997","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1495","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"14e8969b-dbaf-41b3-8330","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Evaluating judgements of spatial correlation in visual displays of scalar field distributions","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"29b8a92c-ef97-44df-b315-a903fe54fa53","abstract":"The rapid growth and availability of event sequence data across domains requires effective analysis and exploration methods to facilitate decision-making. Visual analytics combines computational techniques with interactive visualizations, enabling the identification of patterns, anomalies, and attribute interactions. However, existing approaches frequently overlook the interplay between temporal and multivariate attributes. We introduce EventBox, a novel data representation and visual encoding approach for analyzing groups of events and their multivariate attributes. We have integrated EventBox into Sequen-C, a visual analytics system for the analysis of event sequences. To enable the agile creation of EventBoxes in Sequen-C, we have added user-driven transformations, including alignment, sorting, substitution and aggregation. To enhance analytical depth, we incorporate automatically generated statistical analyses, providing additional insight into the significance of attribute interactions. We evaluated our approach involving 21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T framework to assess visualization value, user performance metrics completing a series of tasks, and interactive sessions with domain experts. We also present three case studies with real-world healthcare data demonstrating how EventBox and its integration into Sequen-C reveal meaningful patterns, anomalies, and insights. These results demonstrate that our work advances visual analytics by providing a flexible solution for exploring temporal and multivariate attributes in event sequences.","accessible_pdf":null,"authors":[{"affiliation":"University of Sheffield","email":"l.montanagonzalez@sheffield.ac.uk","name":"Luis Montana Gonzalez"},{"affiliation":"University of Sheffield","email":"jgmagallanescastaneda1@sheffield.ac.uk","name":"Jessica Magallanes"},{"affiliation":"University of Sheffield","email":"m.juarez@sheffield.ac.uk","name":"Miguel A Juarez"},{"affiliation":"University of Sheffield","email":"s.mason@sheffield.ac.uk","name":"Suzanne Mason"},{"affiliation":"University of Sheffield","email":"a.j.narracott@sheffield.ac.uk","name":"Andrew Narracott"},{"affiliation":"Sheffield Teaching Hospitals Foundation Trust","email":"lindsey.vangemeren@nhs.net","name":"Lindsey van Gemeren"},{"affiliation":"Sheffield Teaching Hospitals Foundation Trust","email":"steven.wood8@nhs.net","name":"Steven Wood"},{"affiliation":"University of Sheffield","email":"m.villa-uriol@sheffield.ac.uk","name":"Maria-Cruz Villa-Uriol"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"29b8a92c-ef97-44df-b315-a903fe54fa53","image_caption":"Data scientists and visual analytics experts interested in studying or developing new techniques to analyze multivariate temporal event sequences that can be extracted from event logs. Event logs can be found in healthcare, finance and social science applications.\nWhen the event logs capture processes, process mining experts would also find the proposed techniques and visualizations valuable.\n\nDomain experts and data owners with questions about their event data logs can also benefit from using EventBox and its implementation in Sequen-C.","keywords":["Temporal event sequences","multivariate attribute analysis","temporal analysis","visual analytics","interactive visualization."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1663-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY2My1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.Mxt9MV74kdFF0ELThB0pZWfrI5jxk2MaGgvIBafBvp8","preprint_link":"https://arxiv.org/abs/2507.14685","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1663","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"29b8a92c-ef97-44df-b315","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"adaa08c8-ae8e-42b3-aeb5-506a2afe1dd1","abstract":"Accessibility is a foundational principle of effective visualization design, yet achieving perceptual accessibility remains a challenge for both experts and non-experts. Standards like the WCAG2 contrast ratio are intended to guide designers toward accessible color use but are increasingly misapplied as oversimplified and misunderstood shortcuts for achieving perceptual accessibility. This misuse highlights the need for more nuanced, perceptually grounded evaluation tools. In response, we present our Modular Accessible Perceptibility (MAP) contrast model and tool, which integrates advanced color appearance models to assess perceptibility and flag risks associated with simultaneous contrast. By augmenting the CAM16-UCS model with a CAM16 CCz simultaneous chromatic contrast calculation, our model can isolate and measure the predicted shift resulting from simultaneous contrast along the chroma axis. This enables a more refined evaluation of visual contrast that better reflects the complexities of human color perception. Additionally, our tool performs modularly segmented automated checks to assign MAP contrast index scores, offering a holistic assessment of a map\u2019s visual accessibility. This paper details the development and validation of the MAP model and tool, underscoring accessibility and inclusive design as fundamental rights, and demonstrating their utility in advancing perceptual accessibility in geovisualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Wisconsin-Madison","email":"limpisathian@wisc.edu","name":"P. William Limpisathian"},{"affiliation":"University of Wisconsin - Madison","email":"sccox@wisc.edu","name":"Susannah Cox"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"adaa08c8-ae8e-42b3-aeb5-506a2afe1dd1","image_caption":"I believe this research would be of interest to cartographers, geospatial data scientists, GIScientists, cartographic developer, geovisualization professionals, designers, web developers, color scientists, and digital accessibility advocates. The insights provided in the paper can help practitioners improve map design, enhance digital content, advance research in human color perception, and promote accessibility standards. By addressing the limitations of existing contrast ratio standards and introducing a more nuanced evaluation tool, this paper offers valuable guidance for creating more accessible and inclusive visualizations.","keywords":["accessibility","perception","cartography","design","WCAG","CIE","color appearance model"],"open_access_supplemental_link":"https://neurocarto.geography.wisc.edu/2025/05/30/maptoolkit/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1314-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMTQtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.23qWs4ASxEFC-5jwz78k8IIkIKzYzX3kzvtFzpnqX8o","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1314","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"adaa08c8-ae8e-42b3-aeb5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Evolving CAM16-UCS for Modular Accessible Perceptibility and Simultaneous Contrast Detection Tool for Geovisualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"fb22d5c6-8a2f-4d98-80ec-1fbbfed18538","abstract":"Visualizing and analyzing 3D unsteady flow fields is a very challenging task.\nWe approach this problem by leveraging the mathematical foundations of 3D observer fields to explore and analyze 3D flows in reference frames that are more suitable to visual analysis than the input reference frame.\nWe design novel interactive tools for determining, filtering, and combining reference frames for observer-aware 3D unsteady flow visualization. We represent the space of reference frame motions in a 3D spatial domain via a 6D parameter space, in which every observer is a time-dependent curve. Our framework supports operations in this 6D observer space by separately focusing on two 3D subspaces, for 3D translations, and 3D rotations, respectively. We show that this approach facilitates a variety of interactions with 3D flow fields.\nBuilding on the interactive selection of observers, we furthermore introduce novel techniques such as observer-aware streamline- and pathline-filtering as well as observer-aware isosurface animations of scalar fluid properties for the enhanced visualization and analysis of 3D unsteady flows.\nWe discuss the theoretical underpinnings as well as practical implementation considerations of our approach, and demonstrate the benefits of its 6+1D observer-based methodology on several 3D unsteady flow datasets.","accessible_pdf":"Accessible","authors":[{"affiliation":"King Abdullah University of Science and Technology (KAUST)","email":"xingdi.zhang@kaust.edu.sa","name":"Xingdi Zhang"},{"affiliation":"King Abdullah University of Science and Technology","email":"amani.ageeli@kaust.edu.sa","name":"Amani Ageeli"},{"affiliation":"Thomas Theussl","email":"tom.theussl@gmail.com","name":"Thomas Theu\u00dfl"},{"affiliation":"KAUST","email":"markus.hadwiger@kaust.edu.sa","name":"Markus Hadwiger"},{"affiliation":"King Abdullah University of Science and Technology","email":"peter.rautek@gmail.com","name":"Peter Rautek"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"fb22d5c6-8a2f-4d98-80ec-1fbbfed18538","image_caption":"simulation scientists\nCFD scientists\nphysicist","keywords":["Flow visualization","unsteady flow","reference frame optimization","interactive visualization","coherent structures"],"open_access_supplemental_link":"https://github.com/Cindy-xdZhang/PyflowVis","open_access_supplemental_question":"This paper is accompanied by comprehensive supplemental materials and a well-documented open-source implementation.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1817-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTgxNy1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.6O_7_NFXSzj7Yqg93evKZXnSu7xrijuKZM73SoKt53k","preprint_link":"https://vccvisualization.org/research/observerspaces/","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1817","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"fb22d5c6-8a2f-4d98-80ec","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Exploring 3D Unsteady Flow using 6D Observer Space Interactions","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"5a4eadeb-75eb-48ac-8a15-c21a5b376113","abstract":"We investigate methods for placing labels in AR environments that have visually cluttered scenes. As the number of items increases in a scene within the user' FOV, it is challenging to effectively place labels based on existing label placement guidelines. To address this issue, we implemented three label placement techniques for in-view objects for AR applications. We specifically target a scenario, where various items of different types are scattered within the user's field of view, and multiple items of the same type are situated close together. We evaluate three placement techniques for three target tasks. Our study shows that using a label to spatially group the same types of items is beneficial for identifying, comparing, and summarizing data.","accessible_pdf":null,"authors":[{"affiliation":"Rochester Institute of Technology","email":"jhpigm@rit.edu","name":"Ji Hwan Park"},{"affiliation":"The University of Oklahoma","email":"bradenroper@ou.edu","name":"Braden Roper"},{"affiliation":"University of Oklahoma","email":"amirhossein.arezoumand@ou.edu","name":"Amirhossein Arezoumand"},{"affiliation":"University of Oklahoma","email":"tien.g.tran@ou.edu","name":"Tien Tran"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5a4eadeb-75eb-48ac-8a15-c21a5b376113","image_caption":"The paper will be of interest to developers/researchers who need to label scenes in AR. In the scenes, various items of different types are scattered within the user\u2019s\nfield of view, and multiple items of the same type are situated close\ntogether, they will use a grouping method from the paper.","keywords":["Label placement","view management systems","augmented reality","situated visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1125-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMjUtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.lPVNKWeiP_nzxpLwPfKhckrxhzrciSn-MqN255KoGGs","preprint_link":"https://arxiv.org/abs/2507.00198","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1125","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"5a4eadeb-75eb-48ac-8a15","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Exploring AR Label Placements in Visually Cluttered Scenarios","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"a610d354-3cae-4df3-ac69-db5a5a7572a6","abstract":"Constructing expressive and legible visualizations is a key activity for visualization designers.While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/.","accessible_pdf":"Accessible","authors":[{"affiliation":"Aarhus University","email":"johannes@ellemose.eu","name":"Johannes Ellemose"},{"affiliation":"Aarhus University","email":"elm@cs.au.dk","name":"Niklas Elmqvist"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a610d354-3cae-4df3-ac69-db5a5a7572a6","image_caption":"We believe data journalists could benefit from understanding which type of visualizations tend to be perceived as more complex, which can inform them of their choice in visualizations for a given data-driven story. In addition, the findings are of potential interest to educators, as it may provide some insights into which elements of visualizations that are perceived as complex, and thus perhaps confusing to people.","keywords":["Visualization complexity","visualization literacy","perception","crowdsourcing","LLMs."],"open_access_supplemental_link":"https://osf.io/w85a4/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1260-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI2MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.AM2twNe6nzDphK9Iy2xjzSCJvEpY72xAOpuRWhvPf0g","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1260","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"a610d354-3cae-4df3-ac69","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Eye of the Beholder: Towards Measuring Visualization Complexity","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"bbfc6b4e-96d6-4981-87da-8bb285d256dc","abstract":"Storyline visualizations represent character interactions over time. When these characters belong to different groups, a new research question emerges: how can we balance optimization of readability across the groups while preserving the overall narrative structure of the story? Traditional algorithms that optimize global readability metrics (like minimizing crossings) can introduce quality biases between the different groups based on their cardinality and other aspects of the data. Visual consequences of these biases are: making characters of minority groups disproportionately harder to follow, and visually deprioritizing important characters when their curves become entangled with numerous secondary characters. We present F2Stories, a modular framework that addresses these challenges in storylines by offering three complementary optimization modes: (1) fairnessMode ensures that no group bears a disproportionate burden of visualization complexity regardless of their representation in the story; (2) focusMode allows prioritizing a group of characters while maintaining good readability for secondary characters; and (3) standardMode globally optimizes classical aesthetic metrics. Our approach is based on Mixed Integer Linear Programming (MILP), offering optimality guarantees, precise balancing of competing metrics through weighted objectives, and the flexibility to incorporate complex fairness concepts as additional constraints without the need to redesign the entire algorithm. We conducted an extensive experimental analysis to demonstrate how F2Stories enables more fair or focus group-prioritized storyline visualizations while maintaining adherence to established layout constraints. Our evaluation includes comprehensive results from a detailed case study that shows the effectiveness of our approach in real-world narrative contexts. An open access copy of this paper and all supplemental materials are available at https://osf.io/e2qvy/.","accessible_pdf":null,"authors":[{"affiliation":"Universit\u00e0 degli Studi di Perugia","email":"tommaso.piselli@gmail.com","name":"Tommaso Piselli"},{"affiliation":"University of Perugia","email":"giuseppe.liotta@unipg.it","name":"Giuseppe Liotta"},{"affiliation":"University of Perugia","email":"fabrizio.montecchiani@unipg.it","name":"Fabrizio Montecchiani"},{"affiliation":"TU Wien","email":"noellenburg@ac.tuwien.ac.at","name":"Martin N\u00f6llenburg"},{"affiliation":"TU Wien","email":"dibartolomeo.sara@gmail.com","name":"Sara Di Bartolomeo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"bbfc6b4e-96d6-4981-87da-8bb285d256dc","image_caption":"Data Journalists, Data Analysts, Screen writers","keywords":["Storyline layouts","optimization","fairness"],"open_access_supplemental_link":"https://osf.io/e2qvy/","open_access_supplemental_question":"We did everything according to the Open Practices Committee page (i.e. pre-print @https://www.ac.tuwien.ac.at/files/tr/ac-tr-25-001.pdf and open source code in osf@https://osf.io/e2qvy/) . We plan to apply to the Graphics Replicability Stamp Initiative and made sure that our experiments are replicable.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1587-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU4Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.NqrOP32YrXac0NczlplZ_7bWTGlRqRhK2z3-kNqvjhk","preprint_link":"https://www.ac.tuwien.ac.at/files/tr/ac-tr-25-001.pdf","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1587","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"bbfc6b4e-96d6-4981-87da","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"F^2Stories: A Modular Framework for Multi-Objective Optimization of Storylines with a Focus on Fairness","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"333107f7-56b6-4215-823a-2550e2a1c1e1","abstract":"Edge bundling reduces the visual complexity of drawings of dense graphs by clustering compatible edges. However, existing edge bundling methods often have high computational complexity, leading to scalability issues. This paper presents a new framework for fast edge bundling and faithfulness metrics for large and complex graphs using spectral sparsification, which sparsifies a graph G into a subgraph G' with O(nlogn) edges, preserving the spectrum of G. We first present a general framework, FEB (Fast Edge Bundling), utilizing spectral sparsification to improve the efficiency of existing bundling methods while maintaining a similar quality of bundling. We then present the FBQ (Fast Bundling Quality) framework for proxy bundle faithfulness metrics, to measure how FEB faithfully preserves the ground truth structure in the original edge bundling, with two variants, FBQ_JS (utilizing Jaccard Similarity) and  FBQ_SQ (utilizing sampling quality metrics). Extensive experiments using various real-world networks demonstrate the efficiency of the FEB framework, with 61% runtime improvement over the original edge bundling methods without sparsification, while maintaining a similar quality by FBQ quality metrics and visual comparison.","accessible_pdf":null,"authors":[{"affiliation":"University of Sydney","email":"xjia9238@uni.sydney.edu.au","name":"Xingjue Jiang"},{"affiliation":"University of Sydney","email":"seokhee.hong@sydney.edu.au","name":"Seok-Hee Hong"},{"affiliation":"University of Sydney","email":"amei2916@uni.sydney.edu.au","name":"Amyra Meidiana"},{"affiliation":"University of Sydney","email":"xzen6984@uni.sydney.edu.au","name":"Xianyuan Zeng"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"333107f7-56b6-4215-823a-2550e2a1c1e1","image_caption":"Data scientists, System biologists, and Sociologists using large and complex networks.\n\nThey can use the new framework for fast edge bundling to produce a faithful visualization of large and complex graphs, with fast runtime.","keywords":["Edge bundling","Spectral sparsification","Faithfulness metrics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1132-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMzItZG9jLnBkZiIsImlhdCI6MTc2MDk0NTk0NCwiZXhwIjoxNzkyNDgxOTQ0fQ.iMwWAsU1tVLAgTSM-ZtG889XudxsHjD5YGSB9HwwqeU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1132","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"333107f7-56b6-4215-823a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Fast and Faithful Edge Bundling for Large and Complex Networks","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"72737da9-0932-41f6-99a2-803817cf8c15","abstract":"Interactive time-varying volume visualization is challenging due to its complex spatiotemporal features and sheer size of the dataset. Recent works transform the original discrete time-varying volumetric data into continuous Implicit Neural Representations (INR) to address the issues of compression, rendering, and super-resolution in both spatial and temporal domains. However, training the INR takes a long time to converge, especially when handling large-scale time-varying volumetric datasets. In this work, we proposed F-Hash, a novel feature-based multi-resolution Tesseract encoding architecture to greatly enhance the convergence speed compared with existing input encoding methods for modeling time-varying volumetric data. The proposed design incorporates multi-level collision-free hash functions that map dynamic 4D multi-resolution embedding grids without bucket waste, achieving high encoding capacity with compact encoding parameters. Our encoding method is agnostic to time-varying feature detection methods, making it a unified encoding solution for feature tracking and evolution visualization. Experiments show the F-Hash achieves state-of-the-art convergence speed in training various time-varying volumetric datasets for diverse features. We also proposed an adaptive ray marching algorithm to optimize the sample streaming for faster rendering of the time-varying neural representation.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Nebraska-Lincoln","email":"sunjianxin66@gmail.com","name":"Jianxin Sun"},{"affiliation":"Argonne National Laboratory","email":"dlenz@anl.gov","name":"David Lenz"},{"affiliation":"University of Nebraska-Lincoln","email":"hfyu@unl.edu","name":"Hongfeng Yu"},{"affiliation":"Argonne National Laboratory","email":"tpeterka@mcs.anl.gov","name":"Tom Peterka"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"72737da9-0932-41f6-99a2-803817cf8c15","image_caption":"Scientists in scientific simulation, data scientists, practitioners in data visualization.\n\nThe practitioners can apply the proposed method to reduce the training time of modeling large-scale, complex, and high-dimensional data into an implicit neural representation.","keywords":["Time-varying volume","volume visualization","input encoding","deep learning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1455-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ1NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzUwLCJleHAiOjE3OTI0ODAzNTB9.rqoQ6nd-0Bc9lkPXVIekY_JsKNxXzKyT0k9nuz70yYM","preprint_link":"https://arxiv.org/abs/2507.03836","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1455","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"72737da9-0932-41f6-99a2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b2ea7546-bba9-4e9e-9d93-b7e43b147009","abstract":"We introduce FlexPhys, a cookbook that researchers can use to operationalize data physicalization research questions through workshop design. While guidelines exist for running workshops in educational contexts, designing a data physicalization workshop when the goal is to answer research questions is an ad-hoc process for which little guidance exists, but for which many choices must be made (e.g., in terms of materials, tools, and data). We draw from our experience designing data physicalization workshops and from reviewing three existing workshops, to distill the cookbook's core ingredient (context and goal) and eight additional ingredients related to making (material, tool, technique), data encoding (data type, variable, mark/unit), and interactivity (interaction, sensory modality). We then show how FlexPhys can be used to describe and compare physicalization workshops, and to generate workshops that address specific data physicalization research questions.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Victoria","email":"bahare.bakhtiari97@gmail.com","name":"Bahare Bakhtiari"},{"affiliation":"Universit\u00e9 Claude Bernard Lyon 1","email":"aurelien@tabard.fr","name":"Aur\u00e9lien Tabard"},{"affiliation":"University of Victoria","email":"sowmya.somanath@gmail.com","name":"Sowmya Somanath"},{"affiliation":"University of Victoria","email":"cperin@uvic.ca","name":"Charles Perin"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b2ea7546-bba9-4e9e-9d93-b7e43b147009","image_caption":"Researchers in the field of data visualization and data physicalization would be interested in reading this paper. They can use FlexPhys, the framework introduced in this paper, as a guideline to design data physicalization workshops to answer research questions.","keywords":["data physicalization","methods","workshops"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1057-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwNTctZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.fmROOAa_5jpSG8B1S0xXrjcWNfLIf8pX8pxG_Bs0ggA","preprint_link":"https://hal.science/hal-05151375","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1057","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b2ea7546-bba9-4e9e-9d93","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"FlexPhys: A Workshop Cookbook for Operationalizing Data Physicalization Research Questions","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ed3195e2-8726-4d85-acb7-c5ed2dc361bb","abstract":"Multi-agent workflows have become a powerful approach to solve complicated tasks by decomposing them into multiple sub-tasks and assigning the sub-tasks to specialized agents. However, designing optimal workflows remains challenging due to the expansive design space. Current practices rely heavily on practitioner intuition and expertise, often resulting in design fixation or an unstructured trial-and-error exploration. To address these challenges, this work introduces FlowForge, an interactive visualization tool to facilitate multi-agent workflow creation through i) a structured visual explore of the design space and ii) in-situ guidance based on design patterns. Based on formative studies and literature review, FlowForge organizes the workflow design process into three levels (i.e., task planning, agent assignment, and agent optimization), ranging from abstract to concrete. A structured visual exploration of the design space enable users to transition from high-level concepts to detailed implementations, and to compare alternative solutions across multiple performance metrics. Additionally, drawing from established workflow design patterns, FlowForge provides contextually relevant in-situ suggestions at each level as users navigate the design space. Use cases and expert interviews demonstrate the usability and effectiveness of FlowForge, while also yielding valuable observations into how practitioners explore the design space and leverage guidance in workflow creation.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"panzhan1hao@gmail.com","name":"Pan Hao"},{"affiliation":"University of Minnesota","email":"dongyeop@umn.edu","name":"Dongyeop Kang"},{"affiliation":"University of Minnesota Twin Cities","email":"hinds084@umn.edu","name":"Nicholas Hinds"},{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ed3195e2-8726-4d85-acb7-c5ed2dc361bb","image_caption":"AI practitioners interested in designing multi-agent workflows can use this paper to learn how to systematically explore the workflow design space and apply structured guidance to improve the effectiveness and efficiency of their workflow creation process.","keywords":["LLM workflows","Multi-agent Workflows","design space","hierarchical visualization"],"open_access_supplemental_link":"https://github.com/Visual-Intelligence-UMN/FlowForge","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1729-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTcyOS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.y3ksqVhoQI55jk6aNOhgKdYcw58xju9yyeQod7Vpc_Q","preprint_link":"https://arxiv.org/abs/2507.15559","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1729","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ed3195e2-8726-4d85-acb7","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"FlowForge: Guiding the Creation of Multi-agent Workflows with Interactive Visualizations as a Thinking Scaffold","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"787f3aab-3371-4bf1-94c6-9141cd0db272","abstract":"Visualization design influences how people perceive data patterns, yet most research focuses on low-level analytic tasks, such as finding correlations. The extent to which these perceptual affordances translate to high-level decision-making in the real world remains underexplored. Through a case study of academic mentorship selection using bar charts and pie charts, we investigated whether chart types differentially influence how students evaluate faculty research profiles. Our crowdsourced experiment revealed only minimal differences in decision outcomes between chart types, suggesting that perceptual affordances established in controlled analytical tasks may not directly translate to high-level decision scenarios. These findings emphasize the importance of evaluating visualizations within real-world contexts and highlight the need to distinguish between perceptual and decision affordances when developing visualization guidelines.","accessible_pdf":"Accessible","authors":[{"affiliation":"Georgia Institute of Technology","email":"yixuanli@gatech.edu","name":"Yixuan Li"},{"affiliation":"University of Massachusetts Amherst","email":"emery@cs.umass.edu","name":"Emery Berger"},{"affiliation":"Yonsei University","email":"minsuk.kahng@gmail.com","name":"Minsuk Kahng"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"787f3aab-3371-4bf1-94c6-9141cd0db272","image_caption":"Practitioners such as data visualization designers and user experience designers would be most interested in this paper, as it provides insights into how different chart types influence high-level decision-making, which they can apply to design visualizations that are not only perceptually effective but also decision-aware in real-world contexts.","keywords":["Visual Affordances","Decision-Making","Perception"],"open_access_supplemental_link":"https://osf.io/gfuwp/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1118-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMTgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.J2EW6PIihG1MQc5Ir16l3AmxEhnc5Ou94K0oHKf7E-8","preprint_link":"https://arxiv.org/abs/2410.04686","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1118","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"787f3aab-3371-4bf1-94c6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"From Perception to Decision: Assessing the Role of Chart Type Affordances in High-Level Decision Tasks","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"544233de-86be-4bc4-ab7e-2e5de7d366b2","abstract":"Tactile graphics are widely used to present maps and statistical diagrams to blind and low vision (BLV) people, with accessibility guidelines recommending their use for graphics where spatial relationships are important. Their use is expected to grow with the advent of commodity refreshable tactile displays. However, in stark contrast to visual information graphics, we lack a clear understanding of the benefits that well-designed tactile information graphics offer over text descriptions for BLV people. To address this gap, we introduce a framework considering the three components of encoding, perception and cognition to examine the known benefits for visual information graphics and explore their applicability to tactile information graphics. This work establishes a preliminary theoretical foundation for the tactile-first design of information graphics and identifies future research avenues.","accessible_pdf":"Accessible","authors":[{"affiliation":"Monash University","email":"kim.marriott@monash.edu","name":"Kim Marriott"},{"affiliation":"Monash University","email":"matthew.butler@monash.edu","name":"Matthew Butler"},{"affiliation":"Monash University","email":"leona.holloway@monash.edu","name":"Leona Holloway"},{"affiliation":"None","email":"wjolley@bigpond.com","name":"William Jolley"},{"affiliation":"Yonsei University","email":"b.lee@yonsei.ac.kr","name":"Bongshin Lee"},{"affiliation":"Vision Australia","email":"bruce.maguire@visionaustralia.org","name":"Bruce Maguire"},{"affiliation":"University of North Carolina-Chapel Hill","email":"danielle.szafir@gmail.com","name":"Danielle Szafir"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"544233de-86be-4bc4-ab7e-2e5de7d366b2","image_caption":"Accessible tactile graphic transcribers","keywords":["Tactile graphic","visual perception","haptic perception","accessible data representation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1468-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ2OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.OTCG3VAnOerDDMRybJNGMLL00pXfVGMWvly3Y37Tv7o","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1468","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"544233de-86be-4bc4-ab7e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"5cec6278-7356-4658-8125-0dc6b65f8b08","abstract":"Unstructured meshes present challenges in scientific data analysis due to irregular distribution and complex connectivity. Computing and storing connectivity information is a major bottleneck for visualization algorithms, affecting both time and memory performance. Recent task-parallel data structures address this by precomputing connectivity information at runtime while the analysis algorithm executes, effectively hiding computation costs and improving performance. However, existing approaches are CPU-bound, forcing the data structure and analysis algorithm to compete for the same computational resources, limiting potential speedups. To overcome this limitation, we introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU systems. Specifically, we offload the computation of mesh connectivity information to GPU threads, enabling CPU threads to focus on executing the visualization algorithm. Following this paradigm, we propose GPU-Aided Localized data structurE (GALE), the first open-source CUDA-based data structure designed for heterogeneous task parallelism. Experiments on two 20-core CPUs and an NVIDIA V100 GPU show that GALE achieves up to 2.7\u00d7 speedup over state-of-the-art localized data structures while maintaining memory efficiency.","accessible_pdf":null,"authors":[{"affiliation":"The Ohio State University","email":"liu.12722@osu.edu","name":"Guoxi Liu"},{"affiliation":"Clemson University","email":"tlranda@clemson.edu","name":"Thomas Randall"},{"affiliation":"Clemson University","email":"rge@clemson.edu","name":"Rong Ge"},{"affiliation":"Clemson University","email":"fiurici@clemson.edu","name":"Federico Iuricich"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"5cec6278-7356-4658-8125-0dc6b65f8b08","image_caption":"Practitioners aiming to accelerate unstructured mesh analysis\u2014whether for real-time visualization, scientific simulation, or large-scale data processing\u2014can leverage GALE\u2019s heterogeneous task-parallel design to overcome traditional CPU-bound bottlenecks. By effectively offloading computations of topological relations to the GPU, GALE not only speeds up processing but also sustains memory efficiency, making it a compelling choice for modern accelerated computing workflows.","keywords":["Data structure","unstructured mesh","topological data analysis","parallel computation","GPU algorithm"],"open_access_supplemental_link":"https://osf.io/zxm4w","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1512-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTUxMi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.lNBQmlBKrzZ5taYKrqsLiE38nd5uncN6AxmMMZkOvWQ","preprint_link":"https://arxiv.org/abs/2507.15230","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1512","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"5cec6278-7356-4658-8125","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"28a61ed9-3f15-4835-a688-e72b1fd6fa0c","abstract":"Despite the widespread use of Uniform Manifold Approximation and Projection (UMAP), the impact of its stochastic optimization process on the results remains underexplored. We observed that it often produces unstable results where the projections of data points are determined mostly by chance rather than reflecting neighboring structures. To address this limitation, we introduce (r,d)-stability to UMAP: a framework that analyzes the stochastic positioning of data points in the projection space. To assess how stochastic elements\u2014specifically, initial projection positions and negative sampling\u2014impact UMAP results, we introduce \u201cghosts\u201d, or duplicates of data points representing potential positional variations due to stochasticity. We define a data point\u2019s projection as (r,d)-stable if its ghosts perturbed within a circle of radius r in the initial projection remain confined within a circle of radius d for their final positions. To efficiently compute the ghost projections, we develop an adaptive dropping scheme that reduces a runtime up to 60% compared to an unoptimized baseline while maintaining approximately 90% of unstable points. We also present a visualization tool that supports the interactive exploration of the (r,d)-stability of data points. Finally, we demonstrate the effectiveness of our framework by examining the stability of projections of real-world datasets and present usage guidelines for the effective use of our framework.","accessible_pdf":"Accessible","authors":[{"affiliation":"Sungkyunkwan University","email":"mw.jung@skku.edu","name":"Myeongwon Jung"},{"affiliation":"Link\u00f6ping University","email":"tfujiwara@ucdavis.edu","name":"Takanori Fujiwara"},{"affiliation":"Sungkyunkwan University","email":"jmjo@skku.edu","name":"Jaemin Jo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"28a61ed9-3f15-4835-a688-e72b1fd6fa0c","image_caption":"Practitioners who work with high-dimensional data and use dimensionality reduction (DR) techniques, such as data scientists, machine learning engineers, and biologists, would be particularly interested in this paper. Our method provides a way to assess the stability of low-dimensional projections against the stochasticity of UMAP. By applying our technique, practitioners can gain valuable insights into their data, such as identifying which instances are unstable in their projection space or which data points might shift between clusters across different DR runs. This can lead to more informed interpretations and increased confidence in downstream analyses or visualizations.","keywords":["Dimensionality reduction","manifold learning","stochastic optimization","reliability","visualization","WebGPU"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1641-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY0MS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.BlrOI22Js6-7fJ3fLuY5KGKE7oa2uVEG0GcSva08R4Q","preprint_link":"https://arxiv.org/abs/2507.17174","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1641","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"28a61ed9-3f15-4835-a688","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"aa5d2960-377f-4f57-a6a1-0cb8c110af24","abstract":"Experiments in visualization perception demonstrate that people can perceive positions highly accurately. However, position encodings can be susceptible to systematic biases depending on the intrinsic properties of the visualized data, such as its shape and cognitive processes of memory and perception. Using line charts as a case study, we investigate how the shape of data, such as local and global extrema, can bias the perception and recall of average data values. In two studies, participants estimated the average data values in a line chart by adjusting a slider with their mouse. We found that participants\u2019 estimates were systematically biased toward the direction of the global extremum. When multiple salient extrema were present, estimates appeared influenced by several extrema simultaneously but ultimately leaned toward the global extremum. \nNotably, the strength of this bias varied depending on whether participants were perceiving or recalling the mean. This work advances our understanding of how extrema influence perception and memory, potentially exaggerating or underestimating critical trends and contributing to a skewed interpretation of data. These findings offer valuable guidance for the design of narrative visualization tools and data storytelling strategies.","accessible_pdf":null,"authors":[{"affiliation":"Emory University","email":"tsavalia@umass.edu","name":"Tejas Savalia"},{"affiliation":"US Naval Research Lab","email":"andrew.lovett@nrl.navy.mil","name":"Andrew Lovett"},{"affiliation":"Northwestern University","email":"crceja@u.northwestern.edu","name":"Cristina Ceja"},{"affiliation":"University of Colorado Boulder","email":"rosie.cowell@colorado.edu","name":"Rosemary Cowell"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"aa5d2960-377f-4f57-a6a1-0cb8c110af24","image_caption":"Our article provides experimental considerations for scientists who study the effects of visual representation of data. Specifically, the shape of data has varying effects on the perception and memory of its summary, and experimental designs should carefully consider interpretations of their findings.\n\nOur article is also geared towards visualization designers involved in creating visualizations for viewers looking to obtain quick takeaways from the data. Designers may be able to strategically use our work to mitigate bias (e.g., by adding more information on the graphs).","keywords":["Data visualization","Memory","Perception","Shape","Global Maxima and Minima"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1332-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMzItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.rb_CLYRG9tq2Mmn5NcAwdwiQUPa6L8nHavE0UgJ1lj0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1332","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"aa5d2960-377f-4f57-a6a1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Global Extrema Bias Perception and Recall of Average Data Values in Line Charts","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"12e7281f-5e70-4d16-a9a0-a87a0d46df5c","abstract":"Visualization grammars from ggplot2 to Vega-Lite are based on the Grammar of Graphics (GoG), our most comprehensive formal theory of visualization. The GoG helped expand the expressive gamut of visualization by moving beyond fixed chart types and towards a design space of composable operators. Yet, the resultant design space has surprising limitations, inconsistencies, and cliffs---even seemingly simple charts like mosaics, waffles, and ribbons fall out of scope of most GoG implementations. To author such charts, visualization designers must either rely on overburdened grammar developers to implement purpose-built mark types (thus reintroducing the issues of typologies) or drop to lower-level frameworks. In response, we present GoFish: a declarative visualization grammar that formalizes Gestalt principles (e.g., uniform spacing, containment, and connection) that have heretofore been complected in GoG constructs. These graphical operators achieve greater expressive power than their predecessors by enabling recursive composition: they can be nested and overlapped arbitrarily. Through a diverse example gallery, we demonstrate how graphical operators free users to arrange shapes in many different ways while retaining the benefits of high-level grammars like scale resolution and coordinate transform management. Recursive composition naturally yields an infinite design space that blurs the boundary between an expressive, low-level grammar and a concise, high-level one. In doing so, we point towards an updated theory of visualization, one that is open to an innumerable space of graphic representations instead of limited to a fixed set of good designs.","accessible_pdf":"Accessible","authors":[{"affiliation":"Massachusetts Institute of Technology","email":"jopo@mit.edu","name":"Josh Pollock"},{"affiliation":"MIT","email":"arvindsatya@mit.edu","name":"Arvind Satyanarayan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"12e7281f-5e70-4d16-a9a0-a87a0d46df5c","image_caption":"GoFish is a visualization grammar like ggplot2, Vega-Lite, and Observable Plot that lets people make more complex charts that could previously only be made in systems like D3. GoFish is currently embedded in JavaScript and would be interesting to anyone that wants to make charts in JavaScript, especially bespoke designs.\n\nGoFish may also be interesting to visualization library developers, as we propose several techniques for simplifying existing grammar of graphics APIs and increasing the expressive power of visualization libraries.","keywords":["Grammar of Graphics","Graphical operators","Gestalt principles","Relational paradigm"],"open_access_supplemental_link":"https://github.com/starfish-graphics/gofish-graphics","open_access_supplemental_question":"Our work includes a live website (https://gofish.graphics/) with installation instructions, a tutorial, a live code editor, and 30 examples.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1078-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA3OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYxLCJleHAiOjE3OTI0Nzk2NjF9.t9lsju3xgLwmVzyId_2-JLYOILUG4v47ZHxRnk2xttw","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1078","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"12e7281f-5e70-4d16-a9a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"GoFish: A Grammar of More Graphics!","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1e9e6140-6939-402f-8781-8420814824be","abstract":"Visualizations support critical decision making in domains like health risk communication. This is particularly important for those at higher health risks and their care providers, allowing for better risk interpretation which may lead to more informed decisions. However, the kinds of visualizations used to represent data may impart biases that influence data interpretation and decision making. Both continuous representations using bar charts and discrete representations using icon arrays are pervasive in health risk communication, but express the same quantities using fundamentally different visual paradigms. We conducted a series of studies to investigate how bar charts, icon arrays, and their layout (juxtaposed, explicit encoding, explicit encoding plus juxtaposition) affect the perception of value comparison and subsequent decision-making in health risk communication. Our results suggest that icon arrays and explicit encoding combined with juxtaposition can optimize for both accurate difference estimation and perceptual biases in decision making. We also found misalignment between estimation accuracy and decision making, as well as between low and high literacy groups, emphasizing the importance of tailoring visualization approaches to specific audiences and evaluating visualizations beyond perceptual accuracy alone. This research contributes empirically-grounded design recommendations to improve comparison in health risk communication and support more informed decision-making across domains.","accessible_pdf":null,"authors":[{"affiliation":"University of North Carolina at Chapel Hill","email":"jadekandel@gmail.com","name":"Jade Kandel"},{"affiliation":"The University of North Carolina at Chapel Hill","email":"liujiayi@unc.edu","name":"Jiayi Liu"},{"affiliation":"University of North Carolina-Chapel Hill","email":"zeyuwang@cs.unc.edu","name":"Arran Zeyu Wang"},{"affiliation":"University of North Carolina-Chapel Hill","email":"chint@cs.unc.edu","name":"Chin Tseng"},{"affiliation":"University of North Carolina-Chapel Hill","email":"danielle.szafir@cs.unc.edu","name":"Danielle Szafir"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"1e9e6140-6939-402f-8781-8420814824be","image_caption":"People interested in designing accessible visualizations and people in healthcare would be interested in reading this paper. Our paper provides recommendations for designing data visualizations for communicating health risks and eliciting comparisons between values.","keywords":["Information Visualization","Graphical Perception","Health Risk Communication","Bar Chart","Icon Array","Composition"],"open_access_supplemental_link":"https://osf.io/4nhtf/?view_only=b5168623250a44aa826ab027df42d87b","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1933-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkzMy1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.U7MBKOErdelB-IslBYRIm859yKuXOLh4e8pCNdjjcKY","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1933","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1e9e6140-6939-402f-8781","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Graphical Perception of Icon Arrays versus Bar Charts for Value Comparisons in Health Risk Communication","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"2a0c8e77-b8b1-4b7d-ab16-0034e10e1dee","abstract":"Grounded image generation enables precise spatial control over pre-trained diffusion models, making it possible to use chart images as visual guides during the image generation process. This paper presents a novel approach that generates cohesive and natural illustrations of vertical bar charts by integrating real-world object images as visual embellishments. The proposed pipeline takes an object image and a reference bar chart as input and produces an embellished bar chart that follows the structure of the input chart. To preserve chart integrity by maintaining the count, position, size, and order of data values, we introduce a strategy that anchors the top and bottom parts of the object image to the top and bottom of each bar while allowing the middle section to be filled by the generation model. We demonstrate the efficacy of the pipeline through the generation of 4,725 chart images followed by evaluation based on three integrity metrics. The results show that generation success rate is affected by various factors. Finally, we discuss future directions for generalization and better usability of our pipeline, and limitations of evaluation used in our approach.","accessible_pdf":null,"authors":[{"affiliation":"KAIST","email":"ksg_0320@kaist.ac.kr","name":"Seon Gyeom Kim"},{"affiliation":"KAIST","email":"jaeyoungchoi@kaist.ac.kr","name":"Jae Young Choi"},{"affiliation":"KAIST","email":"phillip0701@kaist.ac.kr","name":"Yuseung Lee"},{"affiliation":"KAIST","email":"jhyun513@kaist.ac.kr","name":"Jaeryung Chung"},{"affiliation":"Adobe Research","email":"ryrossi@adobe.com","name":"Ryan Rossi"},{"affiliation":"Adobe Research","email":"jkil@adobe.com","name":"Jihyung Kil"},{"affiliation":"Adobe Research","email":"eunyee@adobe.com","name":"Eunyee Koh"},{"affiliation":"KAIST","email":"takyeonlee@kaist.ac.kr","name":"Tak Yeon Lee"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2a0c8e77-b8b1-4b7d-ab16-0034e10e1dee","image_caption":"Data journalists\nData Visualization and Graphic Designers\nData Vis Researchers\nComputer Vision(GenAI, T2I Model) Researchers and Practitioners","keywords":["Chart Embellishment","Grounded Image Generation"],"open_access_supplemental_link":"https://groundedchartgeneration.github.io/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1326-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMjYtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU3MSwiZXhwIjoxNzkyNDgyNTcxfQ.dKkqADEL3Rf1IAiPaCctsSmGmzYEOx1YlnCV2IWpBEs","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1326","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"2a0c8e77-b8b1-4b7d-ab16","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Grounded Generation of Embellished Bar Chart Ensuring Chart Integrity","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"99d47ba0-22e8-4d68-9484-346e6fc722b8","abstract":"Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.","accessible_pdf":null,"authors":[{"affiliation":"UC Davis","email":"david.bauer009@gmail.com","name":"David Bauer"},{"affiliation":"NVIDIA","email":"qiwu@nvidia.com","name":"Qi Wu"},{"affiliation":"University of Groningen","email":"h.gadirov@rug.nl","name":"Hamid Gadirov"},{"affiliation":"University of California at Davis","email":"ma@cs.ucdavis.edu","name":"Kwan-Liu Ma"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"99d47ba0-22e8-4d68-9484-346e6fc722b8","image_caption":"This work would be interesting to simulation scientists, scientific rendering specialists, and computer graphics researchers.\n\nThe technique could support practitioners in creating more performant, higher-quality rendering systems using the presented radiance cache.","keywords":["Radiance caching","path tracing","volume rendering","gaussian splatting"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1100-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTEwMC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.2y2bNyzkMX52ICLA9Nccye_l0c9TRfdzSwdhlNMu2zg","preprint_link":"https://arxiv.org/abs/2507.19718","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1100","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"99d47ba0-22e8-4d68-9484","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"849ca631-398a-4087-9f66-a0837fc3af86","abstract":"Data-driven decision making has become a popular practice in science, industry, and public policy. Yet data alone, as an imperfect and partial representation of reality, is often insufficient to make good analysis decisions. Knowledge about the context of a dataset, its strengths and weaknesses, and its applicability for certain tasks is essential. Analysts are often not only familiar with the data itself, but also have data hunches about their analysis subject. In this work, we present an interview study with analysts from a wide range of domains and with varied expertise and experience, inquiring about the role of contextual knowledge. We provide insights into how data is insufficient in analysts' workflows and how they incorporate other sources of knowledge into their analysis. We analyzed how knowledge of data shaped their analysis outcome. Based on the results, we suggest design opportunities to better and more robustly consider both knowledge and data in analysis processes.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Utah","email":"haihan.lin@utah.edu","name":"Haihan Lin"},{"affiliation":"University of Utah","email":"mlisnic@wpi.edu","name":"Maxim Lisnic"},{"affiliation":"Link\u00f6ping University","email":"derya.akbaba@liu.se","name":"Derya Akbaba"},{"affiliation":"Link\u00f6ping University","email":"miriah.meyer@liu.se","name":"Miriah Meyer"},{"affiliation":"University of Utah","email":"alexander.lex@gmail.com","name":"Alexander Lex"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"849ca631-398a-4087-9f66-a0837fc3af86","image_caption":"visualziation designers and software engineers\nBased on our interview, practioners can learn how data analysts produce their analysis and where the current tooling fall short. How to provide better tooling to faciliate communication and reproducibility.","keywords":["Human-Subjects Qualitative Studies"],"open_access_supplemental_link":"https://osf.io/f89jp/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1785-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc4NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.R_fTNoNIZTQHretSlem6DPyUDrVgkg3yeujefEPkoNU","preprint_link":"https://osf.io/preprints/osf/dn32z_v2","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1785","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"849ca631-398a-4087-9f66","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Here's what you need to know about my data: Exploring Expert Knowledge's Role in Data Analysis","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"6c986f26-eabe-4109-b6ab-7ba4c489451d","abstract":"Maps are essential to news media as they provide a familiar way to convey spatial context and present engaging narratives. However, the design of journalistic maps may be challenging, as editorial teams need to balance multiple aspects, such as aesthetics, the audience\u2019s expected data literacy, tight publication deadlines, and the team\u2019s technical skills. Data journalists often come from multiple areas and lack a cartography, data visualization, and data science background, limiting their competence in creating maps. While previous studies have examined spatial visualizations in data stories, this research seeks to gain a deeper understanding of the map design process employed by news outlets. To achieve this, we strive to answer two specific research questions: what is the design space of journalistic maps? and how do editorial teams produce journalistic map articles? To answer the first one, we collected and analyzed a large corpus of 462 journalistic maps used in news articles from five major news outlets published over three months. As a result, we created a design space comprised of eight dimensions that involved both properties describing the articles\u2019 aspects and the visual/interactive features of maps. We approach the second research question via semi-structured interviews with four data journalists who create data-driven articles daily. Through these interviews, we identified the most common design rationales made by editorial teams and potential gaps in current practices. We also collected the practitioners\u2019 feedback on our design space to externally validate it. With these results, we aim to provide researchers and journalists with empirical data to design and study journalistic maps.","accessible_pdf":null,"authors":[{"affiliation":"Universidade Federal de Pernambuco","email":"agsn@cin.ufpe.br","name":"Arlindo Gomes"},{"affiliation":"Universidade Federal de Pernambuco","email":"emilly.brito@ufpe.br","name":"Emilly Brito"},{"affiliation":"Universidade Federal de Pernambuco","email":"gusto@cin.ufpe.br","name":"Luiz Morais"},{"affiliation":"Universidade Federal de Pernambuco","email":"nivan@cin.ufpe.br","name":"Nivan Ferreira"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"6c986f26-eabe-4109-b6ab-7ba4c489451d","image_caption":"Maps are crucial tools that data journalists use to tell stories with spatial data; the design space can help with the conceptualization of new maps and in the classification of map production within the newsroom. Additionally, our design space can also serve as an educational tool for novice journalists in the field for classes, workshops, or group meetings. This article also serves as a bridge for multidisciplinary collaboration between journalists, designers, and cartographers in the newsroom, promoting a common ground that can assist the collaborative and iterative nature of the journalistic practice. This paper can also provide insights for innovative and well-informed narrative development in news reporting.","keywords":["design space","maps","news","visualization design","data visualization","data journalism"],"open_access_supplemental_link":"https://osf.io/bhn6e/?view_only=6a130b6323364a1db514a3df45b94647","open_access_supplemental_question":"We provided our data collection in PDF format, a dataset that other authors can use to conduct new experiments and apply our data to different objectives, such as exploring data literacy or trust in data visualization. Additionally, we provided our classification of those maps in spreadsheet format, allowing other researchers to perform new analyses and explore factors we did not discuss in our data. Finally, we provide a Codebook that introduces newcomers to our design space and can also be used as supporting material for classes and workshops. We believe that these practices offer many possibilities for researchers to explore ways we did not and expand this work with new ideas and findings.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1345-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTM0NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQxLCJleHAiOjE3OTI0ODAzNDF9.jRC2gkOe88tIVWtMdsrtYoTjvTz4s_VxEoJqJtpU_ZE","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1345","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"6c986f26-eabe-4109-b6ab","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"How do Data Journalists Design Maps to Tell Stories?","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"a4b21717-3ef3-4fdf-b25a-30b41b9f0ef9","abstract":"Many toolkit developers seek to streamline the visualization programming process through structured support such as prescribed templates and example galleries. However, few projects examine how users organize their own visualization programs and how their coding choices may deviate from the intents of toolkit developers, impacting visualization prototyping and design. Further, is it possible to infer users\u2019 reasoning indirectly through their code, even when users copy code from other sources? We explore this question through a qualitative analysis of 715 D3 programs on Observable. We identify three levels of program organization based on how users decompose their code into smaller blocks: Program-, Chart-, and Component-Level code decomposition, with a strong preference for Component-Level reasoning. In a series of interviews, we corroborate that these levels reflect how Observable users reason about visualization programs. We compare common user-made components with those theorized in the Grammar of Graphics to assess overlap in user and toolkit developer reasoning. We find that, while the Grammar of Graphics covers basic visualizations well, it falls short in describing complex visualization types, especially those with animation, interaction, and parameterization components. Our findings highlight how user practices differ from formal grammars and reinforce ongoing efforts to rethink visualization toolkit support, including augmenting learning tools and AI assistants to better reflect real-world coding strategies.","accessible_pdf":"Accessible","authors":[{"affiliation":"Carnegie Mellon University","email":"lin.family.folders@gmail.com","name":"Melissa Lin"},{"affiliation":"University of Washington","email":"heerpate@cs.washington.edu","name":"Heer Patel"},{"affiliation":"University of Washington","email":"mlamkin@cs.washington.edu","name":"Medina Lamkin"},{"affiliation":"University of Maryland","email":"hbako@virginia.edu","name":"Hannah Bako"},{"affiliation":"University of Washington","email":"leibatt@cs.washington.edu","name":"Leilani Battle"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"a4b21717-3ef3-4fdf-b25a-30b41b9f0ef9","image_caption":"Practitioners who use D3 or plan to use D3 might benefit from this work. They would learn about how common visualization practices for D3.","keywords":["Visualization toolkits","Code reuse."],"open_access_supplemental_link":"https://osf.io/sudb8/?view_only=cc72bdc685804e478852a96297328eb8","open_access_supplemental_question":"We provided substantial supplemental of our transparent analysis practices and data. This allows for future reproducibility of this work.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1052-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwNTItZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.mStmggs7TlT_mVgBB98SclmrFB_dnLrh6XitJjsfTAA","preprint_link":"https://arxiv.org/abs/2405.14341","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1052","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"a4b21717-3ef3-4fdf-b25a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"How Do Observable Users Decompose D3 Code? A Qualitative Study","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"c06df49a-9aa2-40f9-94b9-23789ec137b8","abstract":"Modern scientific discovery encounters significant challenges in integrating the rapidly expanding and heterogeneous body of knowledge required for driving breakthroughs in biomedicine and drug development. While traditional hypothesis-driven research has proven effective, it is constrained by human cognitive limitations, the complexity of biological systems, and the high costs associated with trial-and-error experimentation. Deep learning models, particularly graph neural networks (GNNs), have accelerated scientific progress. However, the sheer volume of predictions they generate makes manual selection for experimental validation impractical. Attempts to leverage large language models (LLMs) for filtering predictions and generating novel hypotheses have been impeded by issues such as hallucinations and the lack of structured knowledge grounding, which undermine their reliability. To address these challenges, we propose HypoChainer, a collaborative visualization framework that integrates human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance scientific discovery and validation visually. HypoChainer operates through three key stages: (1) Exploration and Contextualization: Domain experts employ retrieval-augmented LLMs (RAGs) and dimensionality reduction techniques to extract insights and research entry points from vast GNN predictions, supplemented by interactive explanations for in-depth understanding; (2) Hypothesis Chain Formation: Experts iteratively explore the relationships between KG information relevant to the predictions and semantically linked nodes consistent with the hypothesis, gaining knowledge and insights while refining the hypothesis through suggestions from LLMs and KGs; and (3) Validation Prioritization: Predictions are filtered and prioritized based on the refined hypothesis chains and KG-supported evidence, identifying high-priority candidates for experimental validation. Weak points in the hypothesis chain are further optimized through visual analytics of the retrieval results. We evaluated the effectiveness of HypoChainer in hypothesis construction and scientific discovery through case studies in two distinct domains and expert interviews.","accessible_pdf":null,"authors":[{"affiliation":"ShanghaiTech University","email":"shishh2023@shanghaitech.edu.cn","name":"Shaohan Shi"},{"affiliation":"ShanghaiTech University","email":"jianghr2023@shanghaitech.edu.cn","name":"Haoran Jiang"},{"affiliation":"ShanghaiTech University","email":"yaoyj2024@shanghaitech.edu.cn","name":"Yunjie Yao"},{"affiliation":"Shanghai Clinical Research and Trial Center","email":"cjiang_fdu@yeah.net","name":"Chang Jiang"},{"affiliation":"ShanghaiTech University","email":"liquan@shanghaitech.edu.cn","name":"Quan Li"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"c06df49a-9aa2-40f9-94b9-23789ec137b8","image_caption":"Researchers engaged in scientific discovery, professionals in biology or related fields, and scholars in the field of visualization.","keywords":["Large Language Model","Visual Analytics","Iterative Human-AI Collaboration","Knowledge Graph","Hypothesis Construction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1399-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTM5OS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.uqob9O2eV8mKwyMoelby34k0iEE42K_bsg1dL0VrQws","preprint_link":"https://arxiv.org/abs/2507.17209","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1399","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"c06df49a-9aa2-40f9-94b9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ce206043-1fa9-45b7-801b-2910c1dec585","abstract":"In sports analytics, tactical visualization is widely used to convey valuable insights. However, due to the complex domain knowledge and contextual information involved in tactical visualizations, it is challenging for users to connect high-level tactical insights to corresponding visual patterns. This requires users to engage in a reasoning process to interpret insights within game contexts, which remains insufficiently supported in existing visual-text linking studies. In this work, we propose InsightChaser, a novel approach to bridge tactical insights and soccer visualizations through visual-text linking and visual reasoning enhancement. InsightChaser constructs knowledge graphs to represent both visual elements and contextual game information. Integrating large language models (LLMs), our approach retrieves relevant visual elements and establishes explicit links with insights. Moreover, InsightChaser utilizes LLMs to enhance these visual-text links by providing reasoning explanations and visual effects. We further develop an interactive visualization system that supports navigation and explanation of enhanced visual-text links. Users can explore linked tactical insights interactively and reason through enhanced visual explanations. We conduct two case studies using real-world soccer data and a user study to demonstrate the effectiveness of our approach.","accessible_pdf":null,"authors":[{"affiliation":"Zhejiang University","email":"ziao_liu@outlook.com","name":"Ziao Liu"},{"affiliation":"Zhejiang University","email":"zhao_ws@zju.edu.cn","name":"Wenshuo Zhao"},{"affiliation":"Zhejiang University","email":"xxie@zju.edu.cn","name":"Xiao Xie"},{"affiliation":"Zhejiang University","email":"caoanqi28@163.com","name":"Anqi Cao"},{"affiliation":"Zhejiang University","email":"wuyihong0606@gmail.com","name":"Yihong Wu"},{"affiliation":"Zhejiang University","email":"zhang_hui@zju.edu.cn","name":"Hui Zhang"},{"affiliation":"Zhejiang University","email":"ycwu@zju.edu.cn","name":"Yingcai Wu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"ce206043-1fa9-45b7-801b-2910c1dec585","image_caption":"This work will interest sports analysts, data journalists, and visualization designers who aim to communicate sports tactical insights more effectively. Practitioners can apply our approach to enhance visual reasoning in sports articles, reports, and analysis tools, enabling clearer connections between complex data and narrative explanations.","keywords":["Sports visualization","tactical analysis","visual-text linking"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1066-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA2Ni1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYxLCJleHAiOjE3OTI0Nzk2NjF9.0DH9dJHNH-4FKYxPsM-bS-ToyWIANVy0b6NYAiGBO3g","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1066","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ce206043-1fa9-45b7-801b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"InsightChaser: Enhancing Visual Reasoning of Sports Tactical Visualization with Visual-Text Linking","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b43c7d8b-fdda-4a75-818d-e1fb4e4dfe31","abstract":"Linear embeddings support interactive visual exploration by mapping high-dimensional (nD) data into a two-dimensional\nspace. Despite their popularity, selecting meaningful projection parameters remains a key challenge due to the infi nite 2n-dimensional\nparameter space. Once an informative projection is found, users often seek similar ones that emphasize specifi c items differently\nwhile preserving global structure. For instance: Do clusters become outliers under slight changes? Can grouped items separate\u2014or\nmerge\u2014through parameter adjustments? Which changes to the embedding parameters lead to such projections \u2014 and do they exist\nat all? Answering these questions effi ciently is critical for effective visual search. Yet, current methods\u2014such as projection tours or\nmanual parameter tuning\u2014are time-consuming and risk overlooking important views, including those of specifi c interest. We propose\nComposition Operators, a mathematical foundation for a novel set-of-point manipulation concept for linear embeddings\u2014such as Star\nCoordinates\u2014as an alternative approach to selecting informative embedding parameters in a more controllable manner with respect to\nthe desired outcome. Users specify item-based constraints on the projection result; the corresponding 2n parameters are then derived\nautomatically, eliminating the need to exhaustively search the entire parameter space to get a similar outcome. Neither the embedding\nspace nor the set of parameters is altered \u2013 only the mechanism for navigating and selecting parameters is redefi ned. We provide\nclosed-form solutions for this and demonstrate our interactive prototype on nD datasets from the UCI repository.","accessible_pdf":"Accessible","authors":[{"affiliation":"Ostfalia University of Applied Sciences","email":"di.lehmann@ostfalia.de","name":"Dirk Lehmann"},{"affiliation":"Institute for Information Engineering","email":"kaiblum95@web.de","name":"Kai Blum"},{"affiliation":"Universidad Rey Juan Carlos","email":"manuel.rubio@urjc.es","name":"Manuel Rubio-S\u00e1nchez"},{"affiliation":"Robert Bosch GmbH, Stuttgart, GERMANY","email":"konrad.simon2@de.bosch.com","name":"Konrad Simon"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b43c7d8b-fdda-4a75-818d-e1fb4e4dfe31","image_caption":"Each practitioner with data science applications would be interested","keywords":["Star Coordinates","Multivariate Projections","Composition Operators","Multidimensional Data"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/2004-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMjAwNC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQwLCJleHAiOjE3OTI0ODA5NDB9.cl6G4OgjoN2Jc6YgSbyhphBgI1h8xAcxaZVeSY1iWBA","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2004","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b43c7d8b-fdda-4a75-818d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Interactive Composition Operators: An Alternative Approach for Selecting Linear Embedding Parameters","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b8d95afa-0b83-46b6-a0b5-a48369a85ff7","abstract":"Process Discovery, a key area of Process Mining, constructs models from event logs containing sequences of activity executions. Current automatic approaches suffer from opaque algorithms, limited incorporation of domain knowledge, and static, hard-to-explore visualizations. To address these limitations, we propose an interactive discovery process involving users in constructing process trees. This allows real-time interaction, dynamic domain expertise integration, and better understanding of the relationship between logs and models. We demonstrate this with a prototype featuring four linked views, and introduce improved process tree visualizations.","accessible_pdf":null,"authors":[{"affiliation":"Eindhoven University of Technology","email":"w.b.v.d.heijden@student.tue.nl","name":"Wouter van der Heijden"},{"affiliation":"Eindhoven University of Technology","email":"s.j.v.d.elzen@tue.nl","name":"Stef van den Elzen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b8d95afa-0b83-46b6-a0b5-a48369a85ff7","image_caption":"Our approach targets three main user groups: (1) developers of process tree models seeking greater explainability and incorporation of domain knowledge; (2) educators who benefit from visual, step-by-step construction to teach process mining concepts; and (3) researchers and PM developers aiming to explore and improve the discovery process itself.","keywords":["Visual Analytics","Process Mining","Process Trees."],"open_access_supplemental_link":"https://github.com/stefvandenelzen/process-tree-builder","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1190-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExOTAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTk0MCwiZXhwIjoxNzkyNDgxOTQwfQ.BpF2tj_eFveB6J5f7SsN1BYy0oGHr7AmvLTUy_kd9sQ","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1190","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b8d95afa-0b83-46b6-a0b5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Interactive Discovery and Visualization of Process Trees","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"4009fec8-7d38-4e00-9e7d-f9493fb32941","abstract":"Hybrid rice breeding crossbreeds different rice lines and cultivates the resulting hybrids in fields to select those with desirable agronomic traits, such as higher yields. Recently, genomic selection has emerged as an efficient way for hybrid rice breeding. It predicts the traits of hybrids based on their genes, which helps exclude many undesired hybrids, largely reducing the workload of field cultivation. However, due to the limited accuracy of genomic prediction models, breeders still need to combine their experience with the models to identify regulatory genes that control traits and select hybrids, which remains a time-consuming process. To ease this process, in this paper, we proposed a visual analysis method to facilitate interactive hybrid rice breeding. Regulatory gene identification and hybrid selection naturally ensemble a dual-analysis task. Therefore, we developed a parametric dual projection method with theoretical guarantees to facilitate interactive dual analysis. Based on this dual projection method, we further developed a gene visualization and a hybrid visualization to verify the identified regulatory genes and hybrids. The effectiveness of our method is demonstrated through the quantitative evaluation of the parametric dual projection method, identified regulatory genes and desired hybrids in the case study, and positive feedback from breeders.","accessible_pdf":null,"authors":[{"affiliation":"Hunan University","email":"changjianchen@hnu.edu.cn","name":"Changjian Chen"},{"affiliation":"Hunan University","email":"wangpengcheng@hnu.edu.cn","name":"Pengcheng Wang"},{"affiliation":"Hunan University","email":"feilv@hnu.edu.cn","name":"Fei Lyu"},{"affiliation":"Hunan University","email":"ztang@hnu.edu.cn","name":"Zhuo Tang"},{"affiliation":"Hunan University","email":"yanglixt@hnu.edu.cn","name":"Li Yang"},{"affiliation":"Hunan University","email":"wanglong8591@hnu.edu.cn","name":"Long Wang"},{"affiliation":"Hunan University","email":"caiyong911@hnu.edu.cn","name":"Yong Cai"},{"affiliation":"Hunan University","email":"feng_yu@hnu.edu.cn","name":"Feng Yu"},{"affiliation":"Hunan University","email":"lkl@hnu.edu.cn","name":"Kenli Li"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"4009fec8-7d38-4e00-9e7d-f9493fb32941","image_caption":"This paper on hybrid rice breeding, is relevant to bioinformaticians and data scientists. Practitioners can apply the proposed visual analysis method to streamline regulatory gene identification and hybrid selection, enhancing breeding efficiency.","keywords":["Hybrid rice breeding","dual projection","genomic prediction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1645-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY0NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.uaKvjzjNijMP5kNOEVRWy-mIk2sUvwJj34bXlLCdZjQ","preprint_link":"https://arxiv.org/abs/2507.11848","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1645","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"4009fec8-7d38-4e00-9e7d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Interactive Hybrid Rice Breeding with Parametric Dual Projection","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9259ca04-ae61-4513-b066-711f3a75cbc1","abstract":"Tracking the flow of carbon (C) through the Earth\u2019s terrestrial biosphere remains a major challenge to understanding how ecosystems respond to environmental change [12]. To build an understanding of this flow, researchers have recently developed models that include thousands of variables whose intricate inter-dependencies can make them difficult to interpret. To open this scientific black box, we partnered with C cycle scientists at the NASA Jet Propulsion Lab who developed the Carbon Data-Model (CARDAMOM) framework [5]. This paper presents a design study of CLOVE, a C science visual analytics application that encodes model dimensions in an intuitive way by using visual metaphors that correspond to the natural world. We describe how CLOVE encapsulates C states and fluxes in complex models, and explains how C cycle scientists can use it to diagnose their model outputs.","accessible_pdf":"Accessible","authors":[{"affiliation":"Havard University","email":"linhphaaam@gmail.com","name":"Linh Pham"},{"affiliation":"Harvard University","email":"inbox.kevin.hu@gmail.com","name":"Kevin Hu"},{"affiliation":"ArtCenter College of Design","email":"junaline4@gmail.com","name":"Minyoung Joo"},{"affiliation":"University of Wisconsin - Madison","email":"ntwhite@wisc.edu","name":"Nathan White"},{"affiliation":"Jet Propulsion Lab","email":"alexis.a.bloom@jpl.nasa.gov","name":"Anthony Bloom"},{"affiliation":"Jet Propulsion Lab","email":"krys.t.blackwood@jpl.nasa.gov","name":"Krys Blackwood"},{"affiliation":"California Institute of Technology","email":"santiago@caltech.edu","name":"Santiago Lombeyda"},{"affiliation":"California Institute of Technology (Caltech)","email":"hmushkin@caltech.edu","name":"Hillary Mushkin"},{"affiliation":"California Institute of Technology","email":"sd@scottdavidoff.com","name":"Scott Davidoff"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9259ca04-ae61-4513-b066-711f3a75cbc1","image_caption":"This paper will interest practitioners in climate and Earth system science, ecological modeling, and environmental data visualization, including carbon cycle modelers, ecological data scientists, and environmental informatics researchers. Practitioners could apply the visual metaphors and interactive design strategies presented in this paper to improve the interpretability of other complex, high-dimensional environmental or ecological datasets. Specifically, CLOVE\u2019s approaches can inform the design of tools for exploring multi-scale, multi-variable model outputs to support hypothesis generation, validation, and science communication in their work.","keywords":["earth system","bayesian carbon cycle","design study"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1112-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMTItZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.VCPjSFPiCWPrzlodx9YhlJnBcO4ULZG-pXu87b07cbg","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1112","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9259ca04-ae61-4513-b066","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Interactive Visual Analytics of Carbon Cycle Science","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"86dbd3cf-0467-4e0e-8612-b70910cdefbc","abstract":"Augmented reality (AR) has increasingly been used to communicate environmental impacts, offering greater engagement than conventional displays. However, its effect on message credibility\u2014how much people believe in the content of the communication\u2014remains unclear. In a preregistered study, we compared the perceived credibility of environmental information presented via visualizations on an AR headset or a desktop display. We created display-specific visual encodings (3D concrete for AR, 2D bar charts for desktop) and added two control conditions to cross display and encoding. We found no difference in message credibility between AR and desktop, though concrete AR was rated most engaging. Supplementary material is available at https://osf.io/n4p5c/.","accessible_pdf":null,"authors":[{"affiliation":"Universit\u00e9 de Bordeaux","email":"aymeric.ferron@inria.fr","name":"Aymeric Ferron"},{"affiliation":"Inria, CNRS, LISN","email":"ambre.assor@inria.fr","name":"Ambre Assor"},{"affiliation":"Inria, CNRS, Univ. Bordeaux","email":"pierre.dragice@gmail.com","name":"Pierre Dragicevic"},{"affiliation":"CNRS, Inria, Univ. Bordeaux, LaBRI","email":"yvonne.jansen@cnrs.fr","name":"Yvonne Jansen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"86dbd3cf-0467-4e0e-8612-b70910cdefbc","image_caption":"People working in XR, communication, and data visualization for non-experts could be interested in this paper. \nPractitioners like researchers could be interested in reproducing or extending the results of the paper. Designers could use the takeaways to back-up their designs.","keywords":["Augmented Reality","Data Visualization","Credibility","Comparative Study","Sustainable HCI"],"open_access_supplemental_link":"https://osf.io/n4p5c/files/osfstorage","open_access_supplemental_question":"We conducted a pre-registered study: https://osf.io/3djhs\nAll data and analyses scripts are available on OSF.\nThe code is publicly available on our Gitlab: https://gitlab.inria.fr/aferron/ar-credibility\nWe provide extra documentation on our OSF repository, including videos, questionnaires and the applications we tested: https://osf.io/n4p5c/","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1257-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI1Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzUxLCJleHAiOjE3OTI0Nzk3NTF9.cT4sr3PtO4DSBhHPxUGVjNQ9QmXTdiy33Ks5rSiHGfU","preprint_link":"https://hal.science/hal-05200516/document","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1257","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"86dbd3cf-0467-4e0e-8612","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Investigating the Effects of Augmented Reality on Message Credibility When Visualizing Environmental Impacts","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"619269e9-f6be-4882-aec8-0ffe4cee1a96","abstract":"We contribute an in-depth analysis of the workflows and tensions arising from generative AI (genAI) use in biomedical visualization (BioMedVis). Although genAI affords facile production of aesthetic visuals for biological and medical content, the architecture of these tools fundamentally limits the accuracy and trustworthiness of the depicted information, from imaginary (or fanciful) molecules to alien anatomy. Through 17 interviews with a diverse group of practitioners and researchers, we qualitatively analyze the concerns and values driving genAI (dis)use for the visual representation of spatially oriented biomedical data. We find that BioMedVis experts, both in roles as developers and designers, use genAI tools at different stages of their daily workflows and hold attitudes ranging from enthusiastic adopters to skeptical avoiders of genAI. In contrasting the current use and perspectives on genAI observed in our study with predictions towards genAI in the visualization pipeline from prior work, we refocus the discussion of genAI\u2019s effects on projects in visualization in the here and now with its respective opportunities and pitfalls for future visualization research. At a time when public trust in science is in jeopardy, we are reminded to first do no harm, not just in biomedical visualization but in science communication more broadly. Our observations reaffirm the necessity of human intervention for empathetic design and assessment of accurate scientific visuals. Supplemental study materials are available at https://osf.io/genaixbiomedvis/.","accessible_pdf":null,"authors":[{"affiliation":"University of Bergen","email":"roxanne.ziman@uib.no","name":"Roxanne Ziman"},{"affiliation":"University of Toronto","email":"s.saharan@utoronto.ca","name":"Shehryar Saharan"},{"affiliation":"Harvard Medical School","email":"mcgill@digizyme.com","name":"Ga\u00ebl McGill"},{"affiliation":"University of Bergen","email":"laura.garrison@uib.no","name":"Laura Garrison"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"619269e9-f6be-4882-aec8-0ffe4cee1a96","image_caption":"We invite practitioners in biomedical visualization and science communication to read this paper, as well as practitioners in related fields: data journalism, general/information visualization, AI developers, clinicians/healthcare/allied health practitioners, public health policy, education, and others. We hope it will encourage reflection about work practices that integrate generative AI tools in the creation and dissemination of scientific and medical visualizations, and the consequences for collaboration with various stakeholders -- including patients and the general public. This work aims to stimulate open discussion about the potentials and pitfalls of using genAI in BioMedVis and to explore further opportunities to best support ethical and transparent uses of genAI in science communication more broadly.","keywords":["biomedical visualization","science communication","generative AI","human-AI collaboration","creativity","qualitative methods"],"open_access_supplemental_link":"https://osf.io/mbw86/?view_only=e087ab5b90a6474abec7bfc42cd2b105","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1548-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU0OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM0LCJleHAiOjE3OTI0ODA1MzR9.xleQaSVl3d0y_fM8-VMpirV6wi93kT8EPPwcq8vfD2A","preprint_link":"https://arxiv.org/abs/2507.14494","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1548","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"619269e9-f6be-4882-aec8","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"It looks sexy but it's wrong.'' Tensions in creativity and accuracy using genAI for biomedical visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"164ea18d-29f7-49a8-87e4-4d95da79af04","abstract":"Professional visualization design has become an increasingly important area of inquiry, yet much of the field\u2019s discourse remains anchored in researcher-centered contexts. Studies of design practice often focus on individual designers\u2019 decisions and reflections, offering limited insight into the collaborative and systemic dimensions of professional work. In this paper, we propose a systems-level reframing of design judgment grounded in the coordination and adaptation that sustain progress amid uncertainty, constraint, and misalignment. Drawing on sustained engagement across multiple empirical studies\u2014including ethnographic observation of design teams and qualitative studies of individual practitioners\u2014we identify recurring episodes in which coherence was preserved not by selecting an optimal option, but by repairing alignment, adjusting plans, and reframing goals. We interpret these dynamics through the lens of Joint Cognitive Systems, which provide tools for analyzing how judgment emerges as a distributed capacity within sociotechnical activity. This perspective surfaces often-invisible work in visualization design and offers researchers a new conceptual vocabulary for studying how design activity is sustained in practice.","accessible_pdf":null,"authors":[{"affiliation":"Purdue University","email":"parsonsp@purdue.edu","name":"Paul Parsons"},{"affiliation":"Monash University","email":"arranridley@gmail.com","name":"Arran Ridley"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"164ea18d-29f7-49a8-87e4-4d95da79af04","image_caption":"This paper may interest practitioners involved in collaborative or client-facing visualization work, including data journalists, data visualization designers, data scientists, and project leads. These professionals often navigate shifting goals, constraints, and roles\u2014conditions where this paper\u2019s systems-level framing of design judgment is especially relevant. The paper offers a vocabulary for understanding judgment as adaptive coordination rather than isolated choice. Practitioners can apply these insights by recognizing coordination breakdowns, documenting evolving rationales, and reframing trade-offs as intentional judgments that sustain momentum under constraint.","keywords":["Human-centered Computing [Visualization]"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1237-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMzctZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.opvoFOPUO-WOoYV2CUmqTY0KVrT_zC6Aa7dX7RhfXZM","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1237","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"164ea18d-29f7-49a8-87e4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Judgment as Coordination: A Joint Systems View of Visualization Design Practice","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1ee915a9-ec02-4dd8-85c3-f03ce8b9bb54","abstract":"Kaleidoscope of Thoughts is a 360\u00b0 immersive visualization project that explores the multilayered dynamics of human cognition and emotion. Using participatory data collection and experimental visualization strategies, the project captures and translates raw thoughts in multiple languages into an immersive, multisensory environment. Visual motifs, such as cyanotype textures, Perlin noise patterns, and algorithmically generated butterfly effects based on the Lorenz attractor, function as representations of emotional turbulence and transformation. The installation foregrounds how minor shifts in cognitive perspective can significantly alter emotional well-being, fostering a shared experience that encourages reflection and connection among diverse audiences (see Figure 1).","accessible_pdf":null,"authors":[{"affiliation":"Arizona State University","email":"meghasachdeva135@gmail.com","name":"Megha Sachdeva"},{"affiliation":"Media and Immersive Experience (MIX) Center","email":"cami.gregory02@gmail.com","name":"Cambelle Gregory"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1ee915a9-ec02-4dd8-85c3-f03ce8b9bb54","image_caption":null,"keywords":["Experimental visualization; Participatory data collection; Immersive storytelling; Emotional data representation; Multilingual art installation; Interactive media; Cognitive visualization; Collective experience"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1053-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzEwNTMtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1NywiZXhwIjoxNzkyNDc1ODU3fQ.kvJqoRQZXOS0_M52ofRjdpVvSjUj4HrnQza_oEQLh6U","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1053","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1ee915a9-ec02-4dd8-85c3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Kaleidoscope of Thoughts: Experimental Visualization of Cognitive Turbulence","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"2dfe369c-3743-4695-aeba-b0bc4de4429d","abstract":"As biodiversity loss and climate change accelerate, botanical gar-\ndens serve as vital infrastructures for research, education, and con-\nservation. This project focuses on the Arnold Arboretum of Har-\nvard University, a 281-acre living museum founded in 1872 in\nBoston, understood as a hybrid site where scientific inquiry, envi-\nronmental stewardship, and interspecies encounters meet. Drawing\non more than a century of curatorial data, the research combines\nhistorical analysis with computational methods to visualize the in-\ntertwined biographies of plants and people. The resulting digital\nplatform reveals patterns of care and scientific observation, along\nwith the ethical, infrastructural, and collective dimensions embed-\nded in botanical data. Using techniques from artificial intelligence,\ngeospatial mapping, and information design, the project frames the\narboretum as a system of shared agency\u2014an active archive of more-\nthan-human affinities that records the layered memory of curatorial\nlabor, the situated nature of knowledge production, and the poten-\ntial of design to bridge archival record and future care.","accessible_pdf":null,"authors":[{"affiliation":"GRIDH","email":"johan.malmstedt@gmail.com","name":"Johan Malmstedt"},{"affiliation":"University of Groningen","email":"d.rodighiero@rug.nl","name":"Dario Rodighiero"},{"affiliation":"metaLAB (at) Berlin","email":"gn.nann@gmail.com","name":"Giacomo Nanni"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2dfe369c-3743-4695-aeba-b0bc4de4429d","image_caption":null,"keywords":["Index Terms: botanical data","deep learning","digital archives","ethnobotany","historical visualization","interspecies relations","science\nand technology studies (STS)"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1057-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzEwNTctZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1MiwiZXhwIjoxNzkyNDc1ODUyfQ.4i4_QGpe3pPqeg6xRKsFobGiYyvvfgr-8bFMweEA3hY","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1057","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"2dfe369c-3743-4695-aeba","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Living Library of Trees: Mapping Knowledge Ecology in Arnold Arboretum","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b71e9ec2-8579-410a-94c0-a9e2812a2712","abstract":"The detection and analysis of features in fluid flow are important tasks in fluid mechanics and flow visualization. One recent class of methods to approach this problem is to first compute objective optimal reference frames, relative to which the input vector field becomes as steady as possible. However, existing methods either optimize locally over a fixed neighborhood, which might not match the extent of interesting features well, or perform global optimization, which is costly. We propose a novel objective method for the computation of optimal reference frames that automatically adapts to the flow field locally, without having to choose neighborhoods a priori. We enable adaptivity by formulating this problem as a moving least squares approximation, through which we determine a continuous field of reference frames. To incorporate fluid features into the computation of the reference frame field, we introduce the use of a scalar guidance field into the moving least squares approximation. The guidance field determines a curved manifold on which a regularly sampled input vector field becomes a set of irregularly spaced samples, which then forms the input to the moving least squares approximation. Although the guidance field can be any scalar field, by using a field that corresponds to flow features the resulting reference frame field will adapt accordingly. We show that using an FTLE field as the guidance field results in a reference frame field that adapts better to local features in the flow than prior work. However, our moving least squares framework is formulated in a very general way, and therefore other types of guidance fields could be used in the future to adapt to local fluid features.","accessible_pdf":null,"authors":[{"affiliation":"King Abdullah University of Science and Technology - KAUST","email":"julio.reyramirez@kaust.edu.sa","name":"Julio Rey Ramirez"},{"affiliation":"KAUST","email":"peter.rautek@kaust.edu.sa","name":"Peter Rautek"},{"affiliation":"Friedrich-Alexander-University Erlangen-N\u00fcrnberg","email":"tobias.guenther@fau.de","name":"Tobias G\u00fcnther"},{"affiliation":"KAUST","email":"markus.hadwiger@kaust.edu.sa","name":"Markus Hadwiger"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b71e9ec2-8579-410a-94c0-a9e2812a2712","image_caption":"Flow visualization scientists can apply this work to extend existing local reference frame optimization to turbulent flow, enabling the application of reference frame optimization in fields ranging from engineering to biomedical applications, where extracting vortex coreline geometry enables a quantitative study of the fluid dynamics","keywords":["Scientific visualization","unsteady flow","reference frame optimization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1172-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTE3Mi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MjYzLCJleHAiOjE3OTI0ODAyNjN9.78GTaGloI7TAs3J-9fmByg1QPKPJIID7DkG6hQPYm1g","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1172","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b71e9ec2-8579-410a-94c0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Locally Adapted Reference Frame Fields using Moving Least Squares","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b9fa4e27-b3e0-425b-b3c6-af1ebecd478a","abstract":"We propose manvr3d, a VR platform for immersive, AI-assisted human-in-the-loop cell tracking. Life scientists reconstruct the developmental history of organisms at the cellular level by analyzing 3D time-lapse microscopy images acquired at high spatio-temporal resolution. However, reconstruction of cell trajectories and lineage trees is a highly time consuming and error prone task. Common tools are often limited to 2D image display, which greatly limits spatial understanding and navigation. Deep Learning-based algorithms accelerate this process, yet depend heavily on manually-annotated, high-quality ground truth data and curation. In this work, we bridge the gap between Deep Learning-based cell tracking software and 3D/VR visualization to create a hybrid AI-human-in-the-loop cell tracking system. We lift the incremental annotation, training and proofreading loop of the deep learning model into the third dimension and apply natural user interfaces like hand gestures and eye tracking to accelerate the cell tracking workflow for life scientists. We present here the technical architecture of our platform and first analysis of performance. Our code is released open source.","accessible_pdf":null,"authors":[{"affiliation":"Center for  Advanced Systems Understanding (CASUS)","email":"s.pantze@hzdr.de","name":"Samuel Pantze"},{"affiliation":"Institut Pasteur","email":"tinevez@pasteur.fr","name":"Jean-Yves Tinevez"},{"affiliation":"Technische Universit\u00e4t Dresden","email":"matthew.mcginity@tu-dresden.de","name":"Matthew McGinity"},{"affiliation":"Helmholtz-Zentrum Dresden-Rossendorf e.V","email":"ulrik.guenther@hzdr.de","name":"Ulrik G\u00fcnther"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b9fa4e27-b3e0-425b-b3c6-af1ebecd478a","image_caption":"biologists","keywords":["Systems Biology","Virtual Reality","Microscopy","Cell Tracking","Volume Rendering","Eye Tracking"],"open_access_supplemental_link":"https://osf.io/rn9h7/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1134-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMzQtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.2J_YonQGGMP0qdo3Y6K-Z6LadrCyHhiejHt_vKj6DfY","preprint_link":"https://arxiv.org/abs/2505.03440","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1134","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b9fa4e27-b3e0-425b-b3c6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"33761c1b-3649-4d86-8e2f-41cdb361e6a6","abstract":"Affective visualization design is an emerging research direction focused on communicating and influencing emotion through visualization. However, as revealed by previous research, this area is highly interdisciplinary and involves theories and practices from diverse fields and disciplines, thus awaiting analysis from more fine-grained angles. To address this need, this work focuses on a pioneering and relatively mature sub-area, affective geovisualization design, to further the research in this direction and provide more domain-specific insights. Through an analysis of a curated corpus of affective geovisualization designs using the Person-Process-Place (PPP) model from geographic theory, we derived a design taxonomy that characterizes a variety of methods for eliciting and enhancing emotions through geographic visualization. We also identified four underlying high-level design paradigms of affective geovisualization design (e.g., computational, anthropomorphic) that guide distinct approaches to linking geographic information with human experience. By extending existing affective visualization design frameworks with geographic specificity, we provide additional design examples, domain-specific analyses, and insights to guide future research and practices in this underexplored yet highly innovative domain.","accessible_pdf":null,"authors":[{"affiliation":"Fudan University","email":"xingyulan96@gmail.com","name":"Xingyu Lan"},{"affiliation":"Shanghai Jiao Tong University","email":"flora20@sjtu.edu.cn","name":"Yutong Yang"},{"affiliation":"Fudan University","email":"wangyifanlea@gmail.com","name":"Yifan Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"33761c1b-3649-4d86-8e2f-41cdb361e6a6","image_caption":"mapper, cartographer, geographer, data artists","keywords":["Affective Visualization Design","Geographic Visualization","User Experience"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1175-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTE3NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.8qDZyGHuLJ00-059mZBr1Vd6y-h0Jh4wn5s1rSod5Ig","preprint_link":"https://arxiv.org/abs/2507.11841","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1175","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"33761c1b-3649-4d86-8e2f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Mapping What I Feel: Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"894782c9-d5b8-4e59-959b-5f71ea2ff688","abstract":"Implicit Neural Representations (INRs) are widely used to encode data as continuous functions, enabling the visualization of large scale multivariate scientific simulation data with reduced memory usage. However, existing INR-based methods face three main limitations: (1) inflexible representation of complex structures, (2) primarily focusing on single-variable data, and (3) dependence on structured grids. Thus, their performance degrades when applied to complex real-world datasets. To address these limitations, we propose a novel neural network-based framework, MC-INR, which handles multivariate data on unstructured grids. It combines meta-learning and clustering to enable flexible encoding of complex structures. To further improve performance, we introduce a residual-based dynamic re-clustering mechanism that adaptively partitions clusters based on local error. We also propose a branched layer to leverage multivariate data through independent branches simultaneously. Experimental results demonstrate that MC-INR outperforms existing methods on scientific data encoding tasks.","accessible_pdf":"Accessible","authors":[{"affiliation":"Korea University","email":"ehsson@korea.ac.kr","name":"Hyunsoo Son"},{"affiliation":"Korea University","email":"wjdgus0967@korea.ac.kr","name":"Jeonghyun Noh"},{"affiliation":"Korea University","email":"orangeblush@korea.ac.kr","name":"Suemin Jeon"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"},{"affiliation":"Korea University","email":"wkjeong@korea.ac.kr","name":"Won-Ki Jeong"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"894782c9-d5b8-4e59-959b-5f71ea2ff688","image_caption":"simulation scientists","keywords":["Implicit neural representation","meta-learning","clustering","multivariate data encoding"],"open_access_supplemental_link":null,"open_access_supplemental_question":"superior performance on the unstructured grids datasets","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1115-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMTUtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.GdVrvXJ7Eqdqg1_e4iP_N9s6mx2_h7Ok6fKlmslI9AU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1115","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"894782c9-d5b8-4e59-959b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data  using Meta-Learning and Clustered Implicit Neural Representations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3dd6a593-74c1-4ea0-b7bc-249654187e88","abstract":"Evaluating the accuracy of dimensionality reduction (DR) projections in preserving the structure of high-dimensional data is crucial for reliable visual analytics. Diverse evaluation metrics targeting different structural characteristics have thus been developed. However, evaluations of DR projections can become biased if highly correlated metrics\u2014those measuring similar structural characteristics\u2014are inadvertently selected, favoring DR techniques that emphasize those characteristics. To address this issue, we propose a novel workflow that reduces bias in the selection of evaluation metrics by clustering metrics based on their empirical correlations rather than on their intended design characteristics alone. Our workflow works by computing metric similarity using pairwise correlations, clustering metrics to minimize overlap, and selecting a representative metric from each cluster. Quantitative experiments demonstrate that our approach improves the stability of DR evaluation, which indicates that our workflow contributes to mitigating evaluation bias.","accessible_pdf":"Accessible","authors":[{"affiliation":"Seoul National University","email":"bjy7266@gmail.com","name":"Jiyeon Bae"},{"affiliation":"Seoul National University","email":"hj@hcil.snu.ac.kr","name":"Hyeon Jeon"},{"affiliation":"Seoul National University","email":"jseo@snu.ac.kr","name":"Jinwook Seo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3dd6a593-74c1-4ea0-b7bc-249654187e88","image_caption":"The paper will interest data scientists and machine-learning engineers who inspect low-dimensional embeddings. It will also benefit visualization practitioners and tool developers responsible for reporting the quality of dimensionality-reduction (DR) projections. In addition, domain researchers in fields where DR is common\u2014such as bioinformatics, HCI, and signal processing\u2014will find it relevant.\nReaders can apply the workflow immediately by (1) computing empirical correlations among metrics across diverse DR projections; (2) clustering the metrics by their correlation-based similarity; and (3) selecting one representative metric from each cluster to minimize redundancy and bias, thereby making DR evaluations fairer and more trustworthy.","keywords":["Dimensionality reduction","Evaluation metrics","Correlation analysis","Benchmarking","Visual analytics"],"open_access_supplemental_link":"https://github.com/JiyeonBae/dr-metric-selection.git","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1049-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwNDktZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzNywiZXhwIjoxNzkyNDgxOTM3fQ.ipfgBI37PKbMTvppStedqS_rVFceAvVuAmff1jsrvj4","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1049","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3dd6a593-74c1-4ea0-b7bc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e7cc7c10-4310-4753-b586-467106cdc883","abstract":"Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models (LLMs) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming LLM-based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data communication.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"amitkumar.das@stonybrook.edu","name":"Amit Kumar Das"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"e7cc7c10-4310-4753-b586-467106cdc883","image_caption":"Type of Practitioners:\nData journalists, fact-checkers, data scientists, visualization designers, educators, business intelligence professionals, and social media content moderators.\n\nHow they could apply this work:\nData journalists and fact-checkers could integrate MisVisFix into their verification workflows to automatically detect misleading elements in charts they encounter during research, enabling faster and more thorough analysis of visual misinformation. Data scientists and visualization designers could use the system as a quality control tool before publishing dashboards or reports, ensuring their visualizations adhere to best practices and don't inadvertently mislead audiences. Educators could incorporate MisVisFix into data literacy curricula to teach students how to critically evaluate visualizations and understand common deception techniques. Business intelligence professionals could deploy the system to audit existing corporate dashboards and improve data communication standards. Social media platforms and content moderation teams could adapt the detection framework to automatically flag potentially misleading charts in user-generated content, helping combat the spread of visual misinformation at scale.","keywords":["Misinformation","Large Language Models","Data Extraction"],"open_access_supplemental_link":"https://github.com/vhcailab/MisVisFix","open_access_supplemental_question":"We provide comprehensive supplementary materials including all source code, evaluation datasets, prompts, and detailed experimental results to ensure full reproducibility. All materials are publicly accessible through both the submission system and a dedicated GitHub repository (https://github.com/vhcailab/MisVisFix), enabling researchers to replicate our findings and build upon our interactive dashboard system.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1999-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTk5OS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.6g-j6jXDx9HSO5fEn5OfI2rCMDn4azTpZGQ_QNjwgpA","preprint_link":"https://arxiv.org/abs/2508.04679","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1999","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e7cc7c10-4310-4753-b586","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"4becbe1b-2bde-4b12-adfd-e45a36678884","abstract":"Text labels are widely used to convey auxiliary information in visualization and graphic design. The substantial variability in the categories and structures of labeled objects leads to diverse label layouts. Recent single-model learning-based solutions in label placement struggle to capture fine-grained differences between these layouts, which in turn limits their performance. In addition, although human designers often consult previous works to gain design insights, existing label layouts typically serve merely as training data, limiting the extent to which embedded design knowledge can be exploited. To address these challenges, we propose a mixture of cluster-guided experts (MoCE) solution for label placement. In this design, multiple experts jointly refine layout features, with each expert responsible for a specific cluster of layouts. A cluster-based gating function assigns input samples to experts based on representation clustering. We implement this idea through the Label Placement Cluster-guided Experts (LPCE) model, in which a MoCE layer integrates multiple feed-forward networks (FFNs), with each expert composed of a pair of FFNs. Furthermore, we introduce a retrieval augmentation strategy into LPCE, which retrieves and encodes reference layouts for each input sample to enrich its representations. Extensive experiments demonstrate that LPCE achieves superior performance in label placement, both quantitatively and qualitatively, surpassing a range of state-of-the-art baselines. Our algorithm is available at https://github.com/PingshunZhang/LPCE.","accessible_pdf":"Accessible","authors":[{"affiliation":"Southwest University","email":"z2211973606@email.swu.edu.cn","name":"Pingshun Zhang"},{"affiliation":"Southwest University","email":"enyuche@gmail.com","name":"Enyu Che"},{"affiliation":"COLLEGE OF COMPUTER AND INFORMATION SCIENCE, SOUTHWEST UNIVERSITY SCHOOL OF SOFTWAREC","email":"out1147205215@outlook.com","name":"Yinan Chen"},{"affiliation":"Southwest University","email":"bihuang@cs.stonybrook.edu","name":"Bingyao Huang"},{"affiliation":"Stony Brook University","email":"hling@cs.stonybrook.edu","name":"Haibin Ling"},{"affiliation":"Southwest University","email":"qujingwei@swu.edu.cn","name":"Jingwei Qu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"4becbe1b-2bde-4b12-adfd-e45a36678884","image_caption":"Data scientists, visualization designers, computer vision researchers","keywords":["Label placement","Mixture of experts","Retrieval augmentation"],"open_access_supplemental_link":"https://jingweiqu.github.io/project/LPCE","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1431-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQzMS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.4HMVSWfHKHoHFwZUSsFZ9G2V0lSKwHT37KDaCs9bgOA","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1431","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"4becbe1b-2bde-4b12-adfd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Mixture of Cluster-guided Experts for Retrieval-Augmented Label Placement","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9af6703f-fc27-4be6-a995-f84e8091b517","abstract":"Implicit neural representations (INRs) have emerged as a transformative paradigm for time-varying volumetric data compression and representation, owing to their ability to model high-dimensional signals effectively. INRs represent scalar fields based on sampled coordinates, typically using either a single network for the entire field or multiple networks across different spatial domains. However, these approaches often face challenges in modeling complex patterns and introducing boundary artifacts. To address these limitations, we propose MoE-INR, an INR architecture based on a mixture-of-experts (MoE) framework. MoE-INR automates irregular subdivisions of spatiotemporal fields and dynamically assigns them to different expert networks. The architecture comprises three key components: a policy network, a shared encoder, and multiple expert decoders. The policy network subdivides the field and determines which expert decoder is responsible for a given input coordinate. The shared encoder extracts hidden representations from the input coordinates, and the expert decoders transform these high-dimensional features into scalar values. This design results in a unified framework accommodating diverse INR types, including conventional, grid-based, and ensemble. We evaluate the effectiveness of MoE-INR on multiple time-varying datasets with varying characteristics. Experimental results demonstrate that MoE-INR significantly outperforms existing non-MoE and MoE-based INRs and traditional lossy compression methods across quantitative and qualitative metrics under various compression ratios.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology","email":"junhanvis@outlook.com","name":"Jun Han"},{"affiliation":"University of Notre Dame","email":"ktang2@nd.edu","name":"Kaiyuan Tang"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9af6703f-fc27-4be6-a995-f84e8091b517","image_caption":"simulation scientists and data scientists","keywords":["Time-varying data compression","implicit neural representation","volume visualization","mixture-of-experts"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1023-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTAyMy1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYyLCJleHAiOjE3OTI0Nzk2NjJ9.n8moh3uK60FRB9qvPPqWOSFYUnckICZpHU_ESpQZi6c","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1023","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9af6703f-fc27-4be6-a995","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"MoE-INR: Implicit Neural Representation with Mixture-of-Experts for Time-Varying Volumetric Data Compression","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"bba75087-a544-4e3c-85f9-947b8b3537dc","abstract":"Connectomics, a subfield of neuroscience, reconstructs structural and functional brain maps at synapse-level resolution. These complex spatial maps consist of tree-like neurons interconnected by synapses. Motif analysis is a widely used method for identifying recurring subgraph patterns in connectomes. These motifs, thus, potentially represent fundamental units of information processing. However, existing computational tools often oversimplify neurons as mere nodes in a graph, disregarding their intricate morphologies. In this paper, we introduce MoMo, a novel interactive visualization framework for analyzing neuron morphology-aware motifs in large connectome graphs. First, we propose an advanced graph data structure that integrates both neuronal morphology and synaptic connectivity. This enables highly efficient, parallel subgraph isomorphism searches, allowing for interactive morphological motif queries. Second, we develop a sketch-based interface that facilitates the intuitive exploration of morphology-based motifs within our new data structure. Users can conduct interactive motif searches on state-of-the-art connectomes and visualize results as interactive 3D renderings. We present a detailed goal and task analysis for motif exploration in connectomes, incorporating neuron morphology. Finally, we evaluate MoMo through case studies with four domain experts, who asses the tool\u2019s usefulness and effectiveness in motif exploration, and relevance to real-world neuroscience research. The source code for MoMo is available here: https://github.com/VCG/momo","accessible_pdf":null,"authors":[{"affiliation":"Harvard University","email":"michael.shewarega@rwth-aachen.de","name":"Michael Shewarega"},{"affiliation":"Harvard University","email":"jakob.troidl@googlemail.com","name":"Jakob Troidl"},{"affiliation":"New Jersey Institute of Technology","email":"oaa9@njit.edu","name":"Oliver Alvarado Rodriguez"},{"affiliation":"New Jersey Institute of Technology","email":"md724@njit.edu","name":"Mohammad Dindoost"},{"affiliation":"Zuse Institue","email":"harth@zib.de","name":"Philipp Harth"},{"affiliation":"University of W\u00fcrzburg","email":"hannah.haberkern@uni-wuerzburg.de","name":"Hannah Haberkern"},{"affiliation":"RWTH Aachen","email":"johannes.stegmaier@lfb.rwth-aachen.de","name":"Johannes Stegmaier"},{"affiliation":"New Jersey Institute of Technology","email":"bader@njit.edu","name":"David Bader"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"bba75087-a544-4e3c-85f9-947b8b3537dc","image_caption":"This paper will interest neuroscientists, computational neuroscientists, neuroinformaticians, and data visualization researchers. Practitioners can apply its methods to explore morphology-aware motifs in large connectomes, integrate spatial structure into subgraph analysis, and use the sketch-based interface for intuitive, interactive 3D motif exploration in neuroscience and other spatial network domains.","keywords":["Visual motif analysis","Scientific visualization","Neuroscience","Connectomics."],"open_access_supplemental_link":"https://github.com/VCG/momo","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1700-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTcwMC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.sK31ayPyqU7Pdpj3UryguK_150_a8wiI6vz_MCaGWYw","preprint_link":"https://www.biorxiv.org/content/10.1101/2025.07.02.662847v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1700","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"bba75087-a544-4e3c-85f9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"MoMo - Combining Neuron Morphology and Connectivity for Interactive Motif Analysis in Connectomes","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"603070d4-693a-4aec-8b08-632a59a24c59","abstract":"Though powerful tools for analysis and communication, interactive visualizations often fail to support real-time interaction with large datasets with millions or more records. To highlight and filter data, users indicate values or intervals of interest. Such selections may span multiple components, combine in complex ways, and require optimizations to ensure low-latency updates. We describe Mosaic Selections, a model for representing, managing, and optimizing user selections, in which one or more filter predicates are added to queries that request data for visualizations and input widgets. By analyzing both queries and selection predicates, Mosaic Selections enable automatic optimizations, including pre-aggregating data to rapidly compute selection updates. We contribute a formal description of our selection model and optimization methods, and their implementation in the open-source Mosaic architecture. Benchmark results demonstrate orders-of-magnitude latency improvements for selection-based optimizations over unoptimized queries and existing optimizers for the Vega language. The Mosaic Selection model provides infrastructure for flexible, interoperable filtering across multiple visualizations, alongside automatic optimizations to scale to millions and even billions of records.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Washington","email":"jheer@uw.edu","name":"Jeffrey Heer"},{"affiliation":"Carnegie Mellon University","email":"domoritz@cmu.edu","name":"Dominik Moritz"},{"affiliation":"University of Washington","email":"rpechuk@uw.edu","name":"Ron Pechuk"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"603070d4-693a-4aec-8b08-632a59a24c59","image_caption":"Visualization developers, data system developers, and data engineers could apply the techniques in this paper to provide scalable, interactive data interfaces.","keywords":["scalable visualization","interactive selection","multiple coordinated views","brushing and linking","user interfaces"],"open_access_supplemental_link":"https://github.com/uwdata/mosaic-selection-benchmarks","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1313-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTMxMy1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQxLCJleHAiOjE3OTI0ODAzNDF9.rsrZ1WZFt2qnjsrTcKKfaI-_ub8r4UNonguKKcb_6Pk","preprint_link":"https://arxiv.org/abs/2507.19690","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1313","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"603070d4-693a-4aec-8b08","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e14aa8da-3794-45ba-9dd7-5e0ced3449c7","abstract":"Detecting and interpreting common patterns in relational data is crucial for understanding complex topological structures across various domains. These patterns, or network motifs, can often be detected algorithmically. However, visual inspection remains vital for exploring and discovering patterns. This paper focuses on presenting motifs within BioFabric network visualizations---a unique technique that opens opportunities for research on scaling to larger networks, design variations, and layout algorithms to better expose motifs. Our goal is to show how highlighting motifs can assist users in identifying and interpreting patterns in BioFabric visualizations. To this end, we leverage existing motif simplification techniques. We replace edges with glyphs representing fundamental motifs such as staircases, cliques, paths, and connector nodes. The results of our controlled experiment and usage scenarios demonstrate that motif simplification for BioFabric is useful for detecting and interpreting network patterns. Our participants were faster and more confident using the simplified view without sacrificing accuracy. The efficacy of our current motif simplification approach depends on which extant layout algorithm is used. We hope our promising findings on user performance will motivate future research on layout algorithms tailored to maximizing motif presentation. Our supplemental material is available at https://osf.io/f8s3g/?view_only=7e2df9109dfd4e6c85b89ed828320843","accessible_pdf":null,"authors":[{"affiliation":"University of Konstanz","email":"fuchs@dbvis.inf.uni-konstanz.de","name":"Johannes Fuchs"},{"affiliation":"Northeastern University","email":"c.dunne@northeastern.edu","name":"Cody Dunne"},{"affiliation":"University of Konstanz","email":"maria-viktoria.heinle@uni-konstanz.de","name":"Maria-Viktoria Heinle"},{"affiliation":"University of Konstanz","email":"keim@uni-konstanz.de","name":"Daniel Keim"},{"affiliation":"TU Wien","email":"dibartolomeo.sara@gmail.com","name":"Sara Di Bartolomeo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"e14aa8da-3794-45ba-9dd7-5e0ced3449c7","image_caption":"This work offers practitioners a method for enhancing the detection and interpretation of complex patterns in relational data through motif simplification in BioFabric visualizations. By replacing edges with glyphs that represent common network motifs, such as staircases, cliques, paths, and connector nodes, the approach enables faster, more confident pattern recognition without sacrificing accuracy. As a result, practitioners across domains like bioinformatics, cybersecurity, and social network analysis can benefit from more efficient and scalable network exploration and analysis.","keywords":["BioFabric","Network Visualization","Motif Simplification","Glyph Design","Quantitative Experiment"],"open_access_supplemental_link":"https://osf.io/f8s3g/?view_only=7e2df9109dfd4e6c85b89ed828320843","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1698-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY5OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTQwLCJleHAiOjE3OTI0ODA1NDB9.bLPdCfqk02891Cb1WuQw0yeaA8qqrl_A4_6WrmcKosI","preprint_link":"https://osf.io/preprints/osf/5d9q6_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1698","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e14aa8da-3794-45ba-9dd7","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Motif Simplification for BioFabric Network Visualizations: Improving Pattern Recognition and Interpretation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f4284776-c09e-4528-8684-cb06c085d1b1","abstract":"Immersive visualization of network data enables users to physically navigate and interact with complex structures, but managing transitions between detailed local (egocentric) views and global (exocentric) overviews remains a major challenge. We present a multifocus probe technique for immersive environments that allows users to instantiate multiple egocentric subgraph views while maintaining persistent links to the global network context. Each probe acts as a portable local focus, enabling fine-grained inspection and editing of distant or occluded regions. Visual and haptic guidance mechanisms ensure context preservation during multi-scale interaction. We demonstrate and discuss the usability of our technique for the editing of network data.","accessible_pdf":null,"authors":[{"affiliation":"Institute for Visual and Analytic Computing","email":"e.zimmermann@uni-rostock.de","name":"Eric Zimmermann"},{"affiliation":"University of Rostock","email":"stefan.bruckner@gmail.com","name":"Stefan Bruckner"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f4284776-c09e-4528-8684-cb06c085d1b1","image_caption":"Our proposed technique extends the interaction vocabulary for network visualizations in virtual environments allowing network exploration, editing, and navigation via multiple focuses while preserving context and orientation. Therefore, this technique could be of potential interest for practitioners interacting and editing (annotating) complex networks in VR.","keywords":["Virtual Reality","Graph","Focus+Context","Interaction","Editing"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1273-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNzMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.qEpfd33rXGxAAMfjYJ-AMA8EhtdtZ1nJXGFROts_2dk","preprint_link":"http://arxiv.org/abs/2507.01140","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1273","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f4284776-c09e-4528-8684","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Multi-Focus Probes for Context-Preserving Network Exploration and Interaction in Immersive Analytics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3f463b80-5efc-4150-8b6f-3f5fbb1aed72","abstract":"Exploring volumetric data is crucial for interpreting scientific datasets. However, selecting optimal viewpoints for effective navigation can be challenging, particularly for users without extensive domain expertise or familiarity with 3D navigation. In this paper, we propose a novel framework that leverages natural language interaction to enhance volumetric data exploration. Our approach encodes volumetric blocks to capture and differentiate underlying structures. It further incorporates a CLIP Score mechanism, which provides semantic information to the blocks to guide navigation. The navigation is empowered by a reinforcement learning framework that leverage these semantic cues to efficiently search for and identify desired viewpoints that align with the user\u2019s intent. The selected viewpoints are evaluated using CLIP Score to ensure that they best reflect the user queries. By automating viewpoint selection, our method improves the efficiency of volumetric data navigation and enhances the interpretability of complex scientific phenomena.","accessible_pdf":null,"authors":[{"affiliation":"Sun Yat-sen University","email":"zhaox269@mail2.sysu.edu.cn","name":"Xuan Zhao"},{"affiliation":"Sun Yat-sen University","email":"taoj23@mail.sysu.edu.cn","name":"Jun Tao"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3f463b80-5efc-4150-8b6f-3f5fbb1aed72","image_caption":"Target practitioners include scientific visualization developers working with volumetric datasets such as CT scans, fluid simulations, or biological reconstructions.\nThey can benefit from the system's natural language-driven navigation and semantic block encoding to streamline 3D data exploration.","keywords":["Volume rendering","Viewpoint navigation","Natural language interaction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/2050-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMjA1MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQ5LCJleHAiOjE3OTI0ODA5NDl9.fqyrBAQ5IV5eD2gNdn8TT112QzkbJwZ21L-1X7ylOCs","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2050","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3f463b80-5efc-4150-8b6f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"093d9538-804e-4951-a5bf-dab72e99c35d","abstract":"Voronoi treemaps are used to depict nodes and their hierarchical relationships simultaneously. However, in addition to the hierarchical structure, data attributes, such as co-occurring features or similarities, frequently exist. Examples include geographical attributes like shared borders between countries or contextualized semantic information such as embedding vectors derived from large language models. In this work, we introduce a Voronoi treemap algorithm that leverages data similarity to generate neighborhood-preserving treemaps. First, we extend the treemap layout pipeline to consider similarity during data preprocessing. We then use a Kuhn-Munkres matching of similarities to centroidal Voronoi tessellation (CVT) cells to create initial Voronoi diagrams with equal cell sizes for each level. Greedy swapping is used to improve the neighborhoods of cells to match the data's similarity further. During optimization, cell areas are iteratively adjusted to their respective sizes while preserving the existing neighborhoods. We demonstrate the practicality of our approach through multiple real-world examples drawn from infographics and linguistics. To quantitatively assess the resulting treemaps, we employ treemap metrics and measure neighborhood preservation.","accessible_pdf":null,"authors":[{"affiliation":"University of Konstanz","email":"patrick.paetzold@uni-konstanz.de","name":"Patrick Paetzold"},{"affiliation":"University of Konstanz","email":"rebecca.kehlbeck@uni-konstanz.de","name":"Rebecca Kehlbeck"},{"affiliation":"University of Konstanz","email":"yumeng.xue@uni-konstanz.de","name":"Yumeng Xue"},{"affiliation":"University of Konstanz","email":"bin.chen@uni-konstanz.de","name":"Bin Chen"},{"affiliation":"Renmin University of China","email":"cloudseawang@gmail.com","name":"Yunhai Wang"},{"affiliation":"University of Konstanz","email":"oliver.deussen@uni-konstanz.de","name":"Oliver Deussen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"093d9538-804e-4951-a5bf-dab72e99c35d","image_caption":"Treemaps are a popular method for visualizing hierarchical data.\nAmong them, Voronoi treemaps are particularly appreciated for their organic appearance. In the past, they have been used by data journalists and scientists alike to make complex data more accessible, not just to researchers, but also to a broader audience. \nOur extension to Voronoi treemaps adds a valuable dimension by enabling the visualization of not only hierarchical structure but also neighborhood relationships between data elements.","keywords":["Hierarchical data","Treemap","Voronoi diagram","Voronoi treemap"],"open_access_supplemental_link":"https://github.com/cgmi/Neighborhood-Preserving-Voronoi-Treemaps","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1598-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU5OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.40Qunv_t5UC7bUSDSAl4REi4LbQq0j5kix8rfPEJ88o","preprint_link":"https://arxiv.org/abs/2508.03445","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1598","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"093d9538-804e-4951-a5bf","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Neighbourhood-Preserving Voronoi Treemaps","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"d22e24df-9562-498b-96d4-e9b19056c72d","abstract":"Traditional volume visualization (VolVis) methods, like direct volume rendering, suffer from rigid transfer function designs and high computational costs. Although novel view synthesis approaches enhance rendering efficiency, they require additional learning effort for non-experts and lack support for semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an interactive system that enables users to explore, query, and edit volumetric scenes using natural language. NLI4VolVis integrates multi-view semantic segmentation and vision-language models to extract and understand semantic components in a scene. We introduce a multi-agent large language model architecture equipped with extensive function-calling tools to interpret user intents and execute visualization tasks. The agents leverage external tools and declarative VolVis commands to interact with the VolVis engine powered by 3D editable Gaussians, enabling open-vocabulary object querying, real-time scene editing, best-view selection, and 2D stylization. We validate our system through case studies and a user study, highlighting its improved accessibility and usability in volumetric data exploration. We strongly recommend readers check out our case studies, demo video, and source code at https://nli4volvis.github.io/.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Notre Dame","email":"kai@nd.edu","name":"Kuangshi Ai"},{"affiliation":"University of Notre Dame","email":"ktang2@nd.edu","name":"Kaiyuan Tang"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"d22e24df-9562-498b-96d4-e9b19056c72d","image_caption":"This work will interest practitioners in scientific visualization, LLM multi-agent system, and human-computer interaction\u2014especially those working with complex 3D volumetric data.\n\nPractitioners can use NLI4VolVis to explore and edit volumetric datasets using natural language, simplifying tasks like object selection, view manipulation, lighting management, and stylization. The system lowers the barrier for non-experts, enhances accessibility, and provides a flexible framework for integrating LLM multi-agents into interactive visualization workflows.","keywords":["Volume visualization","novel view synthesis","natural language interaction","open-vocabulary querying","editable 3D Gaussian splatting","multi-agent collaboration"],"open_access_supplemental_link":"https://github.com/KuangshiAi/nli4volvis","open_access_supplemental_question":"We provide open-source code, a detailed demo video, and a project page showcasing diverse case studies across multiple datasets. These materials support transparency and reproducibility of our novel multi-agent natural language interaction system for volume visualization.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1444-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ0NC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzUwLCJleHAiOjE3OTI0ODAzNTB9.wh-upz64Kyn93D0pWQyecFBIFd8yMoHEw3TmgLM4EQs","preprint_link":"https://arxiv.org/abs/2507.12621","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1444","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"d22e24df-9562-498b-96d4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"84d64ebc-a5cc-4fd5-855f-ead6dc4ddfc0","abstract":"Communicating the complexity of oceanic phenomena\u2014such as hypoxia and acidification\u2014poses a persistent challenge for marine science. Despite advances in sensing technologies and computational models, conventional formats like static visualizations and text-based reports often fall short in conveying the dynamics of ocean changes. To address this gap, we present OceanVive, an immersive and interactive visualization system that transforms complex ocean datasets into navigable spatial narratives. OceanVive incorporates an exploratory panel on a table-sized tablet for managing immersive content on a large screen and integrates adaptive visual encodings, contextual storytelling, and intuitive navigation pathways to support effective communication. We validate the system through expert interviews, demonstrating its potential to enhance science communication and promote deeper public understanding.","accessible_pdf":"Accessible","authors":[{"affiliation":"ShanghaiTech University","email":"ouyy@shanghaitech.edu.cn","name":"Yang Ouyang"},{"affiliation":"ShanghaiTech University","email":"wuych3@shanghaitech.edu.cn","name":"Yuchen Wu"},{"affiliation":"ShanghaiTech University","email":"wangxy7@shanghaitech.edu.cn","name":"Xiyuan Wang"},{"affiliation":"ShanghaiTech University","email":"xielx@shanghaitech.edu.cn","name":"Laixin Xie"},{"affiliation":"The Hong Kong University of Science and Technology","email":"chengwc@ust.hk","name":"Weicong Cheng"},{"affiliation":"The Hong Kong University of Science and Technology","email":"magan@ust.hk","name":"Jianping Gan"},{"affiliation":"ShanghaiTech University","email":"liquan@shanghaitech.edu.cn","name":"Quan Li"},{"affiliation":"Hong Kong University of Science and Technology","email":"mxj@cse.ust.hk","name":"Xiaojuan Ma"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"84d64ebc-a5cc-4fd5-855f-ead6dc4ddfc0","image_caption":"Oceanographer, data scientists","keywords":["Immersive visualization","Communication"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1076-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwNzYtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.yw5Y5W6zZYSt77zWB4dofHcM137PpoZ-_AXKGL1eMsQ","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1076","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"84d64ebc-a5cc-4fd5-855f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e6aa88dc-1973-4b48-80b2-ac68e82b3d04","abstract":"Collaborative causal loop diagrams (C-CLDs) help decision makers to model complex systems and processes, but existing tools offer little support for documenting the model-building process or capturing the provenance of stakeholder contributions. In this paper, we map the C-CLD design space to derive concrete requirements for documentation and transparency. We introduce Perspectiva, an interactive prototype shaped by those requirements and refined through iterative feedback. Perspectiva enables side-by-side navigation and comparison of CLDs, codifies changes and conflicting relationships, and preserves term provenance and contributor attribution. Its core features include anchored nodes for topological consistency, node interaction, hover-activated provenance pop-ups, and colour-coded encodings. In user studies with domain and visualisation experts, participants reported that Perspectiva improved navigation, comparison, and provenance tracking relative to static diagrams as well as highlighting opportunities for enhancement and future research.","accessible_pdf":null,"authors":[{"affiliation":"Monash University","email":"jessica.bounassar@monash.edu","name":"Jessica Bou Nassar"},{"affiliation":"Monash University","email":"yyio0001@student.monash.edu","name":"Yu Xuan Yio"},{"affiliation":"Monash University","email":"nath0002@student.monash.edu","name":"Nethara Athukorala"},{"affiliation":"Monash University","email":"simrandhawan21s@gmail.com","name":"Simran -"},{"affiliation":"Monash University","email":"songhai.fan@monash.edu","name":"Songhai Fan"},{"affiliation":"Monash University","email":"cynthia.huang@monash.edu","name":"Cynthia Huang"},{"affiliation":"Simon Fraser University","email":"lyn@sfu.ca","name":"Lyn Bartram"},{"affiliation":"Monash University","email":"tgdwyer@gmail.com","name":"Tim Dwyer"},{"affiliation":"Monash University","email":"sarah.goodwin@monash.edu","name":"Sarah Goodwin"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"e6aa88dc-1973-4b48-80b2-ac68e82b3d04","image_caption":"This paper would be of interest to practitioners who engage in participatory methods and more specifically, those who apply collaborative systems thinking methods.","keywords":["Causal loop diagram","Collaborative","Conceptual Modelling","Systems thinking","Documentation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1130-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMzAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.egUxGGGyQJ7qirIx2Vva8u5YZSC9nLIUulOYWio07do","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1130","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e6aa88dc-1973-4b48-80b2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Out of the Loop: Enhancing Documentation and Transparency  in Causal Loop Diagrams to Capture Multiple Perspectives","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"a79b8861-5ff1-41ab-ab84-c564496de8de","abstract":"Open-world object detection (OWOD) extends traditional object detection to identifying both known and unknown object, necessitating continuous model adaptation as new annotations emerge. Current approaches face significant limitations: 1) data-hungry training due to reliance on a large number of crowdsourced annotations, 2) susceptibility to partial feature overfitting, and 3) limited flexibility due to required model architecture modifications. To tackle these issues, we present OW-CLIP, a visual analytics system that provides curated data and enables data-efficient OWOD model incremental training. OW-CLIP implements plug-and-play multimodal prompt tuning tailored for OWOD settings and introduces a novel Crop-Smoothing technique to mitigate partial feature overfitting. To meet the data requirements for the training methodology, we propose dual-modal data refinement methods that leverage large language models and cross-modal similarity for data generation and filtering. Simultaneously, we develope a visualization interface that enables users to explore and deliver high-quality annotations\u2014including class-specific visual feature phrases and fine-grained differentiated images. Quantitative evaluation demonstrates that OW-CLIP achieves competitive performance at 89% of state-of-the-art performance while requiring only 3.8% self-generated data, while outperforming SOTA approach when trained with equivalent data volumes.  A case study shows the effectiveness of the developed method and the improved annotation quality of our visualization system.","accessible_pdf":null,"authors":[{"affiliation":"Central South University","email":"jwduan@csu.edu.cn","name":"Junwen Duan"},{"affiliation":"Central South University","email":"234711076@csu.edu.cn","name":"Wei Xue"},{"affiliation":"Central South University","email":"scv.kangziyao@gmail.com","name":"Ziyao Kang"},{"affiliation":"Tsinghua University","email":"shixia@tsinghua.edu.cn","name":"Shixia Liu"},{"affiliation":"Central South University","email":"xiajiazhi@csu.edu.cn","name":"Jiazhi Xia"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"a79b8861-5ff1-41ab-ab84-c564496de8de","image_caption":"Visualization Practitioners, AI Researchers, Computer Vision Specialists","keywords":["Open-world object detection","data-efficient supervision","large language model","human-AI collaboration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1774-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc3NC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.BEGW-Le6DtEV9jjO9GwDXzuko0mWcfL3cxRGM2ja92U","preprint_link":"https://arxiv.org/abs/2507.19870","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1774","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"a79b8861-5ff1-41ab-ab84","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"4e91945c-334a-4953-87ed-1564920f05dc","abstract":"Ownership relationships of early printed books from the 15th century reveal complex patterns of distribution and possession, offering valuable insights for historical research. This paper presents OwnershipTracker, a visual analytics application developed to explore and trace these relationships using data from the Material Evidence in Incunabula (MEI) database. OwnershipTracker integrates bibliographic records, copy-specific data, and book provenance and ownership details, enabling users to uncover intricate ownership sequences over time. The application combines several visualization techniques, including network graphs to map connections between owners, timelines for temporal analysis, chord diagrams to quantify transfer patterns, and a distinctive, collaboratively designed spiderweb-like diagram highlighting converging and dispersing ownership transfers through specific owners. Developed iteratively with input from historical book researchers, the application underwent multiple refinements to align with domain research requirements. A summative evaluation with domain experts showcased the tool\u2019s ability to address the defined requirements and tasks. The final version of OwnershipTracker is deployed and accessible at: https://booktracker.nms.kcl.ac.uk/ownership.","accessible_pdf":null,"authors":[{"affiliation":"King's College London","email":"yiwen.xing@kcl.ac.uk","name":"Yiwen Xing"},{"affiliation":"King's College London","email":"jimeilai222@gmail.com","name":"Meilai Ji"},{"affiliation":"University of Oxford","email":"c.dondi@cerl.org","name":"Cristina Dondi"},{"affiliation":"King's College London","email":"alfie.abdulrahman@gmail.com","name":"Alfie Abdul-Rahman"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"4e91945c-334a-4953-87ed-1564920f05dc","image_caption":"This paper will be of interest to practitioners working on visualization applications in the digital humanities, particularly those involved in collaborative projects with historians or cultural heritage researchers. It presents a unique design study carried out in close collaboration with historical book scholars, resulting in a co-designed visual representation to support the exploration and explanation of book ownership changes and co-ownership patterns. The experiences and lessons learned from this study can serve as a reference for future research and system development in similar interdisciplinary contexts.","keywords":["Design study","visualization application","human-centered design"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1558-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU1OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.Tp1Knhmhup9dYrJ76LISW6P2viN6rD23g-fHi9aGsF8","preprint_link":"https://kclpure.kcl.ac.uk/portal/en/publications/ownershiptracker-a-visual-analytics-approach-to-uncovering-histor","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1558","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"4e91945c-334a-4953-87ed","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"OwnershipTracker: A Visual Analytics Approach to Uncovering Historical Book Ownership Patterns","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"881a464d-c01d-453c-8f14-8b59acdde104","abstract":"While powerful and well-established, tools like ParaView present a steep learning curve that can discourage many potential users. This work introduces ParaView-MCP, an autonomous agent that integrates modern multimodal large language models (MLLMs) with ParaView to not only lower the barrier to entry but also augment ParaView with intelligent decision support. By leveraging the state-of-the-art reasoning, command execution, and vision capabilities of MLLMs, ParaView-MCP enables users to interact with ParaView through natural language and visual inputs. Specifically, our system adopted the Model Context Protocol (MCP), a standardized interface for model-application communication, which facilitates direct interaction between MLLMs and ParaView's Python API, allowing seamless information exchange between the user, the language model, and the visualization tool itself. Furthermore, by implementing a visual feedback mechanism that allows the agent to observe the viewport, we unlock a range of new capabilities, including recreating visualizations from examples, closed-loop visualization parameter updates based on user-defined goals, and even cross-application collaboration involving multiple tools.","accessible_pdf":null,"authors":[{"affiliation":"Lawrence Livermore National Laboratory","email":"shusen.liu.hust@gmail.com","name":"Shusen Liu"},{"affiliation":"Lawrence Livermore National Laboratory","email":"miao1@llnl.gov","name":"Haichao Miao"},{"affiliation":"Lawrence Livermore National Laboratory","email":"bremer5@llnl.gov","name":"Peer-Timo Bremer"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"881a464d-c01d-453c-8f14-8b59acdde104","image_caption":"domain scientists / user who want to use paraview to generate visualization but never have time to learn it.","keywords":["Agent","Tool Use","Model Context Protocol"],"open_access_supplemental_link":"https://github.com/LLNL/paraview_mcp","open_access_supplemental_question":"Well documented source code","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1100-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMDAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.YibVKkYT_n4KY0LL-mvnB6_KSUZ0vnb290OeagWz5Kc","preprint_link":"https://arxiv.org/abs/2505.07064","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1100","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"881a464d-c01d-453c-8f14","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Paraview-MCP: An Autonomous Visualization Agent with Direct Tool Use","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9e3dfe2c-bbe4-4431-ae8b-1ee64a399c8b","abstract":"Sonification offers a non-visual way to understand data, with pitch-based encodings being the most common. Yet, how well people perceive slope and acceleration\u2014key features of data trends\u2014remains poorly understood. Drawing on people's natural abilities to perceive tempo, we introduce a novel sampling method for pitch-based sonification to enhance the perception of slope and acceleration in univariate functions. While traditional sonification methods often sample data at uniform x-spacing, yielding notes played at a fixed tempo with variable pitch intervals (Variable Pitch Interval), our approach samples at uniform y-spacing, producing notes with consistent pitch intervals but variable tempo (Variable Tempo). We conducted psychoacoustic experiments to understand slope and acceleration perception across three sampling methods: Variable Pitch Interval, Variable Tempo, and a Continuous (no sampling) baseline. In slope comparison tasks, Variable Tempo was more accurate than the other methods when modulated by the magnitude ratio between slopes. For acceleration perception, just-noticeable differences under Variable Tempo were over 13 times finer than with other methods. Participants also commonly reported higher confidence, lower mental effort, and a stronger preference for Variable Tempo compared to other methods. This work contributes models of slope and acceleration perception across pitch-based sonification techniques, introduces Variable Tempo as a novel and preferred sampling method, and provides promising initial evidence that leveraging timing can lead to more sensitive, accurate, and precise interpretation of derivative-based data features.","accessible_pdf":"Accessible","authors":[{"affiliation":"Stanford University","email":"danfan17@stanford.edu","name":"Danyang Fan"},{"affiliation":"Stanford University","email":"walksmit@stanford.edu","name":"Walker Smith"},{"affiliation":"Stanford University","email":"takako@ccrma.stanford.edu","name":"Takako Fujioka"},{"affiliation":"Stanford University","email":"cc@ccrma.stanford.edu","name":"Chris Chafe"},{"affiliation":"University of Michigan","email":"sileo@umich.edu","name":"Sile O'Modhrain"},{"affiliation":"Stanford University","email":"othello5@stanford.edu","name":"Diana Deutsch"},{"affiliation":"Stanford University","email":"sfollmer@stanford.edu","name":"Sean Follmer"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9e3dfe2c-bbe4-4431-ae8b-1ee64a399c8b","image_caption":"This research demonstrates that Variable Tempo sampling in pitch sonification\u2014sampling data at equal y-intervals to create tempo changes reflecting slope\u2014can enable more accurate and precise slope comparison and finer curvature discrimination compared to traditional pitch-based methods. Accessibility technology developers may implement this approach in screen readers and data exploration tools to help blind users more accurately perceive trends and rate changes in charts and graphs. Developers of real-time monitoring systems may leverage tempo-based feedback to provide operators with improved peripheral awareness of critical parameter changes without requiring visual attention.","keywords":["Visualization","Sonification","Empirical Studies","Auditory Perception"],"open_access_supplemental_link":"https://osf.io/a4cth/?view_only=784965f9e9f64a3ea720cf53ff241481","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1627-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTYyNy1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.zigl_2uz3hd0aRuEF-9W4juCczxyKJW9TRFGHaN0ZXI","preprint_link":"https://shape.stanford.edu/research/TempoSonification/Fan25PerceivingSlopeAndAccelerationVariableTempoSamplingSonification.pdf","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1627","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9e3dfe2c-bbe4-4431-ae8b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1513daf8-e0da-48f1-b800-f7b0975869ff","abstract":"We present PiCCL (Pictorial Chart Composition Language), a new language that enables users to easily create pictorial charts using a set of simple operators. To support systematic construction while addressing the main challenge of expressive pictorial chart authoring\u2013manual composition and fine-tuning of visual properties\u2013PiCCL introduces a parametric representation that integrates data-driven chart generation with graphical composition. It also employs a lazy data-binding mechanism that automatically synthesizes charts. PiCCL is grounded in a comprehensive analysis of real-world pictorial chart examples. We describe PiCCL\u2019s design and its implementation as piccl.js, a JavaScript-based library. To evaluate PiCCL, we showcase a gallery that demonstrates its expressiveness and report findings from a user study assessing the usability of piccl.js. We conclude with a discussion of PiCCL\u2019s limitations and potential, as well as future research directions.","accessible_pdf":null,"authors":[{"affiliation":"Shandong University","email":"202315173@mail.sdu.edu.cn","name":"Haoyan Shi"},{"affiliation":"Renmin University of China","email":"cloudseawang@gmail.com","name":"Yunhai Wang"},{"affiliation":"Renmin University of China","email":"chenjunhao11@ruc.edu.cn","name":"Junhao Chen"},{"affiliation":"Microsoft Research","email":"clwang15uw@gmail.com","name":"Chenglong Wang"},{"affiliation":"Yonsei University","email":"b.lee@yonsei.ac.kr","name":"Bongshin Lee"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"1513daf8-e0da-48f1-b800-f7b0975869ff","image_caption":"This paper will be valuable for data visualization designers, data journalists, and developers who work with expressive pictorial charts (e.g., infographics, isotype diagrams). Our proposed language, PiCCL, simplifies the creation of data-driven pictorial charts through a parametric representation, alleviating the burden of manual graphical composition and data binding. We also provide piccl.js, a JavaScript library, enabling practitioners to integrate PiCCL directly into their workflows for automated chart generation.","keywords":["pictorial charts","data-driven composition","chart composition","parametric representation"],"open_access_supplemental_link":"https://osf.io/5eqb7","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/2006-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMjAwNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQwLCJleHAiOjE3OTI0ODA5NDB9.TEO0TIozNs1cQVoZhxxVpGy6u5XH40MC8pSIBWZYY5w","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2006","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1513daf8-e0da-48f1-b800","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"PiCCL: Data-Driven Mark Composition of Bespoke Pictorial Chart","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"76560e50-f8e7-446f-943e-2ba7ff188c86","abstract":"Overdraw is inevitable in large-scale scatterplots. Current scatterplot abstraction methods lose features in medium-to-low density regions. We propose a visual abstraction method designed to provide better feature preservation across arbitrary abstraction levels for large-scale scatterplots, particularly in medium-to-low density regions. The method consists of three closely interconnected steps: first, we partition the scatterplot into iso-density regions and equalize visual density; then, we allocate pixels for different classes within each region; finally, we reconstruct the data distribution based on pixels. User studies, quantitative and qualitative evaluations demonstrate that, compared to previous methods, our approach better preserves features and exhibits a special advantage when handling ultra-high dynamic range data distributions.","accessible_pdf":null,"authors":[{"affiliation":"Visual Computing Lab","email":"sygzh6@tju.edu.cn","name":"Ziheng Guo"},{"affiliation":"Tianjin University","email":"tianxiangwei@tju.edu.cn","name":"Tianxiang Wei"},{"affiliation":"Communication University of China","email":"lzyfun@cuc.edu.cn","name":"Zeyu Li"},{"affiliation":"Tianjin University","email":"lianghaozhang@tju.edu.cn","name":"Lianghao Zhang"},{"affiliation":"Tianjin University","email":"sisilee144144@gmail.com","name":"Sisi Li"},{"affiliation":"Tianjin University","email":"jwzhang@tju.edu.cn","name":"Jiawan Zhang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"76560e50-f8e7-446f-943e-2ba7ff188c86","image_caption":"This paper is for practitioners like data scientists, ML engineers, and bioinformaticians who need to visualize massive datasets, such as those from dimensionality reduction. Our method transforms overdraw scatterplots into clear visualizations that reveal the shape, texture, and relative density. Outliers can remain visible without distorting the overall data patterns, aiding in more reliable anomaly detection. Additionally, its is adaptable to any screen size, making it equally useful for quick-glance thumbnails in a dashboard.","keywords":["Scatterplot Abstraction","Overlap-free","Overdraw","Arbitrary Abstraction Level"],"open_access_supplemental_link":"https://github.com/Guozihengwww/PixelatedScatter","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1593-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU5My1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.mn7FFeeoD-8R7pDOHcZ1IoVvdwEl2CUGQMjDdPAw0UA","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1593","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"76560e50-f8e7-446f-943e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"PixelatedScatter: Arbitrary-level Visual Abstraction for Large-scale Multiclass Scatterplots","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b12be8f4-754d-4d42-af8f-9c58c2cebf47","abstract":"Visual archives of political movements are rich cultural resources, yet often difficult to explore at scale due to complex visual semantics and limited interaction models. We present Posterity, an interactive visualization system for 784 digitized American labor posters (1900\u20132010), designed to support both historical contextualization and visual-semantic exploration. Posterity integrates curated metadata, CLIP-based multimodal embeddings, and unsupervised clustering to offer three coordinated views: a timeline aligned with key labor events, a 3D semantic cloud, and a similarity spiral responsive to image-, object-, or gesture-based input. Together, these views enable users to trace recurring visual motifs, discover rhetorical patterns, and explore labor movement narratives from multiple entry points. While developed for labor posters, the approach demonstrates potential for adaptation to other visual cultural heritage collections,\nparticularly those with rich metadata and symbolic content.","accessible_pdf":"Accessible","authors":[{"affiliation":"Havard University","email":"linhphaaam@gmail.com","name":"Linh Pham"},{"affiliation":"Harvard University","email":"daniel2rodrig@gmail.com","name":"Daniel Rodriguez-Rodriguez"},{"affiliation":"Harvard University","email":"jhuang10@pratt.edu","name":"Jingfei Huang"},{"affiliation":"Harvard University","email":"hysuk012@gmail.com","name":"Hui-Ying Suk"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b12be8f4-754d-4d42-af8f-9c58c2cebf47","image_caption":"Digital-humanities scholars, museum and archive curators, social historians, data-journalists, visualization designers, and general public who are interested in this topic would all find this paper useful because it shows how multimodal CLIP embeddings, unsupervised clustering, and coordinated views can turn a large, visually rich heritage collection into an explorable interface without exhaustive manual tagging. By adapting the described pipeline\u2014pairing existing metadata with CLIP vectors, reducing dimensionality, and linking timeline, thematic, and similarity views\u2014practitioners can build similarly interactive portals for other image-heavy cultural heritage datasets, accelerating discovery, public engagement, and insight generation with minimal annotation overhead.","keywords":["digital humanities","image visualization","image interpretation","historical records","similarity retrieval"],"open_access_supplemental_link":"(Source Code) https://github.com/linhphaaam/posterity","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1286-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyODYtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.v4NCUhklR38f7jjEqWAuUfPAnb_EdZp8XzLDnsBIDew","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1286","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b12be8f4-754d-4d42-af8f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Posterity: Balancing historical context and visual dynamism while visualizing a collection of American labor posters","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"47c46990-8edb-4014-be27-c1c07f128d60","abstract":"Visual analytics (VA) is typically applied to complex data, thus requiring complex tools.\nWhile visual analytics empowers analysts in data analysis, analysts may get lost in the complexity occasionally. This highlights the need for intelligent assistance mechanisms.\nHowever, even the latest LLM-assisted VA systems only provide help when explicitly requested by the user, making them insufficiently intelligent to offer suggestions when analysts need them the most. We propose a ProactiveVA framework in which LLM-powered UI agent monitors user interactions and delivers context-aware assistance proactively. To design effective proactive assistance, we first conducted a formative study analyzing help-seeking behaviors in user interaction logs, identifying when users need proactive help, what assistance they require, and how the agent should intervene. Based on this analysis, we distilled key design requirements in terms of intent recognition, solution generation, interpretability and controllability. Guided by these requirements, we develop a three-stage UI agent pipeline including perception, reasoning, and acting.  The agent autonomously perceives users' needs from VA interaction logs, providing tailored suggestions and intuitive guidance through interactive exploration of the system. We implemented the framework in two representative types of VA systems, demonstrating its generalizability, and evaluated the effectiveness through an algorithm evaluation, case and expert study and a user study. We also discuss current design trade-offs of proactive VA and areas for further exploration.","accessible_pdf":null,"authors":[{"affiliation":"Fudan Univerisity","email":"yuhengzhao_cn@163.com","name":"Yuheng Zhao"},{"affiliation":"Fudan University","email":"3504936154@qq.com","name":"Xueli Shu"},{"affiliation":"Beijing Institute of Technology","email":"flwfdd@gmail.com","name":"Liwen Fan"},{"affiliation":"Fudan University","email":"lgao.lynne@gmail.com","name":"Lin Gao"},{"affiliation":"University of Oxford","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"Fudan University","email":"simingchen3@gmail.com","name":"Siming Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"47c46990-8edb-4014-be27-c1c07f128d60","image_caption":"Algorithm developer,  LLM application developer, Visual analyst, Data scientist, Data Manager, HCI and Visual analyst researcher","keywords":["Visual analytics","mixed-initiative","large language model","interface agent","proactive agent","human-AI collaboration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1045-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA0NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYxLCJleHAiOjE3OTI0Nzk2NjF9.ioTHCDTYphNutdQJB7qROsJMPAXYI2P50u1SjY2GWcA","preprint_link":"https://www.arxiv.org/abs/2507.18165","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1045","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"47c46990-8edb-4014-be27","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"ProactiveVA: Proactive Visual Analytics with LLM-based UI Agent","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"6f935187-a32b-477d-90a0-1a7e959c6222","abstract":"Vision Language Models (VLMs) demonstrate promising chart comprehension capabilities. Yet, prior explorations of their visualization literacy have been limited to assessing their response correctness and fail to explore their internal reasoning. To address this gap, we adapted attention-guided class activation maps (AG-CAM) for VLMs to visualize the influence and importance of input features (image and text) on model responses.  Using this approach,  we conducted an examination of four open-source (ChartGemma, Janus 1B and 7B, and LLaVA) and two closed-source (GPT-4o, Gemini) models comparing their performance and, for the open-source models, their AG-CAM results. Overall, we found that ChartGemma, a 3B parameter VLM fine-tuned for chart question-answering (QA), outperformed other open-source models and exhibited performance on par with significantly larger closed-source VLMs. We also found that VLMs exhibit spatial reasoning by accurately localizing key chart features, and semantic reasoning by associating visual elements with corresponding data values and query tokens. Our approach is the first to demonstrate the use of AG-CAM on early fusion VLM architectures, which are widely used, and for chart QA. We also show preliminary evidence that these results can align with human reasoning. Our promising open-source VLMs results pave the way for transparent and reproducible research in AI visualization literacy.","accessible_pdf":null,"authors":[{"affiliation":"University of Waterloo","email":"a7dong@uwaterloo.ca","name":"Lianghan Dong"},{"affiliation":"University of Waterloo","email":"amcrisan@uwaterloo.ca","name":"Anamaria Crisan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"6f935187-a32b-477d-90a0-1a7e959c6222","image_caption":"Data Scientists, Machine Learning Engineers, AI Engineers","keywords":["Vision Language Models","Visualization Literacy","Explainability","Chart Question and Answering"],"open_access_supplemental_link":"https://osf.io/fp3rg/","open_access_supplemental_question":"Novel methods, open source code, live demo","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1740-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc0MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.cc7gYgPLj5k-xArGkxlKK5g4imTZxOi6lW4WL9V-B5s","preprint_link":"https://arxiv.org/abs/2504.05445","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1740","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"6f935187-a32b-477d-90a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"efe038da-ab5b-4531-9235-89a36836d893","abstract":"Psychomare is an artistic research project that explores the visualization of nightmares through a psychoanalytic and XR-based performance methodology. By treating dreams as symbolic data and nightmares as distortions of subjective recognition, the project translates psychological fear into tangible visual forms. Utilizing AI-driven imagery, embodied dance performance, and virtual production technologies, Psychomare creates an immersive dreamscape where the dancer confronts surreal nightmare entities derived from personal and collective dream memories. Drawing on Lacanian theory, the work proposes that nightmares emerge from a misrecognition of the self\u2014a mirrored distortion of unconscious desires and fears projected onto dream imagery. This symbolic misrecognition becomes the conceptual core of the project, guiding its aesthetic and choreographic strategies. Feedback from the dancer, audience, and a psychoanalyst reveals strong emotional resonance and aesthetic depth, suggesting that the visualization of nightmares can foster self-reflection, emotional confrontation, and collective empathy. This project offers a new model for integrating immersive art, psychoanalytic theory, and technological mediation as a path of collective care.","accessible_pdf":null,"authors":[{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"jarryyyhuang@gmail.com","name":"Jiayang Huang"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"anijiati587@connect.hkust-gz.edu.cn","name":"Joshua Nijiati Alimujiang"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"kzhangcma@hkust-gz.edu.cn","name":"Kang Zhang"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"daveyip@hkust-gz.edu.cn","name":"David Yip"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"efe038da-ab5b-4531-9235-89a36836d893","image_caption":null,"keywords":["Nightmares","XR Performance","Psychoanalytic Art Practice","Dream Visualization","Collective Emotion."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1036-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzEwMzYtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1MywiZXhwIjoxNzkyNDc1ODUzfQ.iEo87H9R-45Is0WTuFug0Kuw3aH6n9-7sI5FFE2ii3s","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1036","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"efe038da-ab5b-4531-9235","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Psychomare: A Psychoanalytic and XR-Based Artistic Exploration into Nightmare Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"32ebace7-cf30-462d-b6a4-40a3e39c6226","abstract":"Fast loading and responsive interaction lead to more effective web-based visualizations. While run-time optimizations such as caching and data tiling improve interaction latency, these approaches leave initial load performance unoptimized. In this work, we investigate _publish-time optimizations_ that shift computational work ahead of user sessions to accelerate both loading and interaction. We organize the space of publish-time optimizations into categories of data preparation, pre-computation of data assets for optimization, and pre-rendering; and then reason about tradeoffs in terms of time-to-render (TTR), time-to-activation (TTA), and storage cost (SC). To assess their effectiveness, we implement publish-time optimizations for the open-source Mosaic architecture and evaluate their impact across varied visualizations and dataset sizes. On average, publish-time strategies reduced rendering latency by 83.7% and activation latency by 33.3%, demonstrating their value for improving the performance of web-based visualizations.","accessible_pdf":null,"authors":[{"affiliation":"University of Washington","email":"rpechuk@uw.edu","name":"Ron Pechuk"},{"affiliation":"University of Washington","email":"jheer@uw.edu","name":"Jeffrey Heer"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"32ebace7-cf30-462d-b6a4-40a3e39c6226","image_caption":"Practitioners who author and/or deploy interactive web-based visualizations or visualization systems will find this paper the most relevant. This group includes visualization developers, web developers, and systems developers. Meanwhile, people in the larger data science and database communities may also find it interesting and applicable. These practitioners can directly apply the results of the paper to guide implementations of the three publish-time strategies we detail: data preparation, pre-computation of data assets for optimization, and pre-rendering. By adopting these techniques, practitioners can reduce initial latencies and achieve faster loading and interaction times of deployed visualizations. Beyond immediate application, this work aims to inspire system developers and the data science community to further explore publish-time optimization as an approach for improving responsiveness and scalability in widely-deployed visualization and more general interactive data applications.","keywords":["scalable visualization","web visualization","visualization optimization","visualization systems","user interfaces"],"open_access_supplemental_link":"https://github.com/uwdata/mosaic-publish","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1295-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyOTUtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.vrWqs4ltHOSjM1midRK83vyViKGuM3AR-Woq1G0kpeY","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1295","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"32ebace7-cf30-462d-b6a4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Publish-Time Optimizations for Web-Based Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"29656e00-00d7-4080-b589-2d7c838c72be","abstract":"Design studies aim to develop visualization solutions for real-world problems across various application domains. Recently, the emergence of large language models (LLMs) has introduced new opportunities to enhance the design study process, providing capabilities such as creative problem-solving, data handling, and insightful analysis. However, despite their growing popularity, there remains a lack of systematic understanding of how LLMs can effectively assist researchers in visualization-specific design studies. In this paper, we conducted a multi-stage qualitative study to fill this gap, which involved 30 design study researchers from diverse backgrounds and expertise levels. Through in-depth interviews and carefully-designed questionnaires, we investigated strategies for utilizing LLMs, the challenges encountered, and the practices used to overcome them. We further compiled the roles that LLMs can play across different stages of the design study process. Our findings highlight practical implications to inform visualization practitioners, and also provide a framework for leveraging LLMs to facilitate the design study process in visualization research.","accessible_pdf":"Accessible","authors":[{"affiliation":"Singapore Management University","email":"haywardryan@foxmail.com","name":"Shaolun Ruan"},{"affiliation":"The Hong Kong University of Science and Technology","email":"rshengac@connect.ust.hk","name":"Rui Sheng"},{"affiliation":"Nanyang Technological University","email":"xiaolin004@e.ntu.edu.sg","name":"Xiaolin Wen"},{"affiliation":"Zhejiang University","email":"wangjiachen@zju.edu.cn","name":"Jiachen Wang"},{"affiliation":"Singapore Management University","email":"tianyizhang.2023@phdcs.smu.edu.sg","name":"Tianyi Zhang"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"},{"affiliation":"Monash University","email":"tgdwyer@gmail.com","name":"Tim Dwyer"},{"affiliation":"Singapore Management University","email":"jiannanli@smu.edu.sg","name":"Jiannan Li"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"29656e00-00d7-4080-b589-2d7c838c72be","image_caption":"Visualization Researchers, Data Scientists, UX/UI Designers, HCI Researchers, LLM Practioners/Researchers","keywords":["Design Study","Large Language Models (LLMs)","Qualitative Study","Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1207-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTIwNy1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.Xm6xxvxQDptDJdkDmxBnwimXfCXJWcnFbdh0c-4JQJA","preprint_link":"https://arxiv.org/abs/2507.10024","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1207","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"29656e00-00d7-4080-b589","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"c35d8073-d3b1-4978-bdd4-0b428299f354","abstract":"What impressions might readers form with visualizations that go beyond the data they encode? In this paper, we build on recent work that demonstrates the socio-indexical function of visualization, showing that visualizations communicate more than the data they explicitly encode. Bridging this with prior work examining public discourse about visualizations, we contribute an analytic framework for describing inferences about an artifact\u2019s social provenance. Via a series of attribution-elicitation surveys, we offer descriptive evidence that these social inferences: (1) can be studied asynchronously, (2) are not unique to a particular sociocultural group or a function of limited data literacy, and (3) may influence assessments of trust. Further, we demonstrate (4) how design features act in concert with the topic and underlying messages of an artifact\u2019s data to give rise to such \u2018beyond-data\u2019 readings. We conclude by discussing the design and research implications of inferences about social provenance, and why we believe broadening the scope of research on human factors in visualization to include sociocultural phenomena can yield actionable design recommendations to address urgent challenges in public data communication.","accessible_pdf":"Accessible","authors":[{"affiliation":"Massachusetts Institute of Technology","email":"amyraefoxphd@gmail.com","name":"Amy Fox"},{"affiliation":"Massachusetts Institute of Technology","email":"mjmorgen@mit.edu","name":"Michelle Morgenstern"},{"affiliation":"Massachusetts Institute of Technology","email":"gmj@mit.edu","name":"Graham Jones"},{"affiliation":"MIT","email":"arvindsatya@mit.edu","name":"Arvind Satyanarayan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"c35d8073-d3b1-4978-bdd4-0b428299f354","image_caption":"This paper would be of interest to visualization designers and data journalists involved in public data communication. It offers an analytic framework that designers can use as a tool for thinking about how the social meaning of their design choices, intended or unintended, might influence audience reception and behavior. It has wide relevance across various topics and domains, but may be particularly useful for high stakes and contentious topics around which public data communication faces urgent challenges in reaching polarized and adversarial audiences.","keywords":["semiotics","socio-indexicality","social provenance","engagement","visualization psychology","public data communication"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/23HYX","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1006-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTAwNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYwLCJleHAiOjE3OTI0Nzk2NjB9.ppwHFIXEHVT-MwLNMP5Gy4Mm2wJdQu6kgWSvQsvbAVs","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1006","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"c35d8073-d3b1-4978-bdd4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e4f2ed49-3ceb-485d-8d71-8463039a522b","abstract":"Human-AI collaborative tools attract attentions from the data storytelling community to lower the barrier of expertise and streamline the workflow. The recent advance in large-scale generative AI techniques, e.g., large language models (LLMs) and text-to-image models, has the potential to enhance data storytelling with their power in visual and narration generation. After two years since these techniques were publicly available, it is important to reflect our progress of applying them and have an outlook for future opportunities. To achieve the goal, we compare the collaboration patterns of the latest tools with those of earlier ones using a dedicated framework for understanding human-AI collaboration in data storytelling. Through comparison, we identify consistently widely studied patterns, e.g., human-creator + AI-assistant, and newly explored or emerging ones, e.g., AI-creator + human-reviewer. The benefits of these AI techniques and implications to human-AI collaboration are also revealed. We further propose future directions to hopefully ignite innovations.","accessible_pdf":null,"authors":[{"affiliation":"Microsoft Research Asia","email":"haotian.li@microsoft.com","name":"Haotian Li"},{"affiliation":"Microsoft Research","email":"wangyun@microsoft.com","name":"Yun Wang"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"e4f2ed49-3ceb-485d-8d71-8463039a522b","image_caption":"Data journalists might be interested to learn the research tools for data storytelling and try them.","keywords":["Data storytelling","human-AI collaboration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1270-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNzAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.fGPlpHNvrHnKR8d1Rn7cqMpZm-Tk7sv8NRCmW4qY55M","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1270","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e4f2ed49-3ceb-485d-8d71","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"625a86de-1529-4724-8869-f081c7776f88","abstract":"We present a new comprehensive theory for explaining, exploring, and using pattern as a visual variable in visualization. Although patterns have long been used for data encoding and continue to be valuable today, their conceptual foundations are precarious: the concepts and terminology used across the research literature and in practice are inconsistent, making it challenging to use patterns effectively and to conduct research to inform their use. To address this problem, we conduct a comprehensive cross-disciplinary literature review that clarifies ambiguities around the use of pattern and texture. As a result, we offer a new consistent treatment of pattern as a composite visual variable composed of structured groups of graphic primitives that can serve as marks for encoding data individually and collectively. This new and widely applicable formulation opens a sizable design space for the visual variable pattern, which we formalize as a new system comprising three sets of variables: the spatial arrangement of primitives, the appearance relationships among primitives, and the retinal visual variables that characterize individual primitives. We show how our pattern system relates to existing visualization theory and highlight opportunities for visualization design. We further explore patterns based on complex spatial arrangements, demonstrating explanatory power and connecting our conceptualization to broader theory on maps and cartography. An author version and additional materials are available on OSF: osf.io/z7ae2.","accessible_pdf":null,"authors":[{"affiliation":"University of Utah","email":"hetingying.hty@gmail.com","name":"Tingying He"},{"affiliation":"City, University of London","email":"j.dykes@city.ac.uk","name":"Jason Dykes"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"petra.isenberg@inria.fr","name":"Petra Isenberg"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"tobias.isenberg@gmail.com","name":"Tobias Isenberg"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"625a86de-1529-4724-8869-f081c7776f88","image_caption":"Practitioners: Data visualization designers & developers; Cartographers & GIS professionals\n\nApplication: They can apply our theory to create richer pattern-based encodings and improve support for pattern design in visualization tools.","keywords":["Pattern","texture","visual variables","retinal variables","data visualization","Jacques Bertin"],"open_access_supplemental_link":"https://osf.io/z7ae2/","open_access_supplemental_question":"We provide a well-documented appendix that includes detailed discussions and examples to support our theory. All images we created are correctly licensed under CC BY 4.0, and the paper has been uploaded to OSF and arXiv as well as will be shared on HAL; there is no code that relates to the paper, so a replicability stamp application was not an option.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1614-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTYxNC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.XjozGB7go2sF06ZdxApVUc4miAOkU-Vto_gD-MwL7V4","preprint_link":"https://arxiv.org/abs/2508.02639","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1614","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"625a86de-1529-4724-8869","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e3b6a06f-ce81-4ceb-a411-6cd298b41326","abstract":"Given the difficulties and inherent problems of data, how do we approach data representations in a decolonial way? This annotated portfolio by Kontinentalist illustrates our evolving practice and attempt at forging a path grounded in a community manifesto that looks to challenge the need for accuracy and certainty, centring design in Indigenous vernacular and knowledge, and challenging reductivism.","accessible_pdf":null,"authors":[{"affiliation":"Kontinentalist","email":"peiying@kontinentalist.com","name":"Pei Ying Loh"},{"affiliation":"Kontinentalist","email":"nabilah@kontinentalist.com","name":"Nabilah Said"},{"affiliation":"Kontinentalist","email":"griselda@kontinentalist.com","name":"Griselda Gabriele"},{"affiliation":"Kontinentalist","email":"munirah@kontinentalist.com","name":"Mansoor Munirah"},{"affiliation":"Kontinentalist","email":"zafirah@kontinentalist.com","name":"Zafirah Zein"},{"affiliation":"Kontinentalist","email":"amanda@kontinentalist.com","name":"Amanda Teo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e3b6a06f-ce81-4ceb-a411-6cd298b41326","image_caption":null,"keywords":["Data Storytelling","Decolonial Methodologies","Coloniality","Data Feminism","Indigenous Knowledge"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1046-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzEwNDYtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1MywiZXhwIjoxNzkyNDc1ODUzfQ.45sjkm5MPkxAh7mZUB1LYPZ4ClFNAG_QDOWdOKzGRQE","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1046","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e3b6a06f-ce81-4ceb-a411","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Rejecting Colonial Practices in Data Storytelling","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"10233019-3fcc-4f01-b478-96b0ec01cf29","abstract":"Accurate and reliable visualization of spatiotemporal sensor data such as environmental parameters and meteorological conditions is crucial for informed decision-making. Traditional spatial interpolation methods, however, often fall short of producing reliable interpolation results due to the limited and irregular sensor coverage. This paper introduces a novel spatial interpolation pipeline that achieves reliable interpolation results and produces a novel heatmap representation with uncertainty information encoded. We leverage imputation reference data from Graph Neural Networks (GNNs) to enhance visualization reliability and temporal resolution. By integrating Principal Neighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), our model effectively learns the spatiotemporal dependencies. Furthermore, we propose an extrinsic, static visualization technique for interpolation-based heatmaps that effectively communicates the uncertainties arising from various sources in the interpolated map. Through a set of use cases, extensive evaluations on real-world datasets, and user studies, we demonstrate our model's superior performance for data imputation, the improvements to the interpolant with reference data, and the effectiveness of our visualization design in communicating uncertainties.","accessible_pdf":null,"authors":[{"affiliation":"East China Normal University","email":"jtchen@stu.ecnu.edu.cn","name":"Juntong Chen"},{"affiliation":"East China Normal University","email":"huayuan221@gmail.com","name":"Huayuan Ye"},{"affiliation":"East China Normal University","email":"10215102469@stu.ecnu.edu.cn","name":"He Zhu"},{"affiliation":"Zhejiang University","email":"siwei.fu@zju.edu.cn","name":"Siwei Fu"},{"affiliation":"School of Computer Science and Technology","email":"cbwang@cs.ecnu.edu.cn","name":"Changbo Wang"},{"affiliation":"East China Normal University","email":"chli@cs.ecnu.edu.cn","name":"Chenhui Li"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"10233019-3fcc-4f01-b478-96b0ec01cf29","image_caption":"Geoscientists, climate engineers/researchers, and general visualization designers.","keywords":["Spatial interpolation","spatiotemporal data","uncertainty visualization","graph neural network"],"open_access_supplemental_link":"https://github.com/jtchen2k/relmap/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1643-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTY0My1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM2LCJleHAiOjE3OTI0ODA1MzZ9.VbKEt3BC9JXh9Vy6Ok4WM2w7qQIRLQlLukPzYz7pkV0","preprint_link":"https://arxiv.org/abs/2508.01240","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1643","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"10233019-3fcc-4f01-b478","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"0ecc86cf-63c0-4483-9849-b207a438bb59","abstract":"The recent adoption of artificial intelligence in socio-technical systems raises concerns about the black-box nature of the resulting decisions in fields such as hiring, finance, admissions, etc. If data subjects\u2014such as job applicants, loan applicants, and students\u2014receive an unfavorable outcome, they may be interested in algorithmic recourse, which involves updating certain features to yield a more favorable result when re-evaluated by algorithmic decision-making. Unfortunately, when individuals do not fully understand the incremental steps needed to change their circumstances, they risk following misguided paths that can lead to significant, long-term adverse consequences. Existing recourse approaches focus exclusively on the final recourse goal but neglect the possible incremental steps to reach the goal with real-life constraints, user preferences, and model artifacts. To address this gap, we formulate a visual analytic workflow for incremental recourse planning in collaboration with AI/ML experts and contribute an interactive visualization interface that helps data subjects efficiently navigate the recourse alternatives and make an informed decision. We present a usage scenario and subjective feedback from observational studies with twelve graduate students using a real-world dataset, which demonstrates that our approach can be instrumental for data subjects in choosing a suitable recourse path.","accessible_pdf":"Accessible","authors":[{"affiliation":"Pacific Northwest National Laboratory","email":"kaustavbhatt94@gmail.com","name":"Kaustav Bhattacharjee"},{"affiliation":"New Jersey Institute of Technology","email":"jy448@njit.edu","name":"Jun Yuan"},{"affiliation":"New Jersey Institute of Technology","email":"dasgupta.aritra@gmail.com","name":"Aritra Dasgupta"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"0ecc86cf-63c0-4483-9849-b207a438bb59","image_caption":"This work would be useful for practitioners interested in developing solutions for algorithmic recourse\u2014that is, providing remedy options for individuals affected by model-driven decisions. It can also be relevant for end-users who have been impacted by such decisions and seek to understand or contest them.","keywords":["Visual analytics","recourse","machine learning"],"open_access_supplemental_link":"https://osf.io/ngzcm/","open_access_supplemental_question":"This is a novel method for designing visual analytic interfaces for algorithmic recourse. Paper will be put on arXiV soon.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1200-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMDAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.zFJn-23NfYFwKzmpkB4RdINk-Rq3INpx9RHW_6YuduE","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1200","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"0ecc86cf-63c0-4483-9849","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"ReVise: A Human-AI Interface for Incremental Algorithmic Recourse","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ebdd5b65-b5cb-4d4b-bd78-92ff8cc2f243","abstract":"Online user studies of visualizations, visual encodings, and interaction techniques are ubiquitous in visualization research. Yet, designing, conducting, and analyzing studies effectively is still a major burden. Although various packages support such user studies, most solutions address only facets of the experiment life cycle, make reproducibility difficult, or do not cater to nuanced study designs or interactions. We introduce reVISit 2, a software framework that supports visualization researchers at all stages of designing and conducting browser-based user studies. ReVISit supports researchers in the design, debug & pilot, data collection, analysis, and dissemination experiment phases by providing both technical affordances (such as replay of participant interactions) and sociotechnical aids (such as a mindfully maintained community of support). It is a proven system that can be (and has been) used in publication-quality studies \u2014 which we demonstrate through a series of experimental replications. We reflect on the design of the system via interviews and an analysis of its technical dimensions. Through this work, we seek to elevate the ease with which studies are conducted, improve the reproducibility of studies within our community, and support the construction of advanced interactive studies.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Utah","email":"zcutler@sci.utah.edu","name":"Zach Cutler"},{"affiliation":"University of Utah","email":"jwilburn@sci.utah.edu","name":"Jack Wilburn"},{"affiliation":"Worcester Polytechnic Institute","email":"hilsonshrestha@gmail.com","name":"Hilson Shrestha"},{"affiliation":"Worcester Polytechnic Institute","email":"yding5@wpi.edu","name":"Yiren Ding"},{"affiliation":"University of Utah","email":"briancbollen@sci.utah.edu","name":"Brian Bollen"},{"affiliation":"University of Utah","email":"abrar.nadib@gmail.com","name":"Khandaker Abrar Nadib"},{"affiliation":"University of Utah","email":"hetingying.hty@gmail.com","name":"Tingying He"},{"affiliation":"University of Utah","email":"mcnutt.andrew@gmail.com","name":"Andrew McNutt"},{"affiliation":"Worcester Polytechnic Institute","email":"ltharrison@wpi.edu","name":"Lane Harrison"},{"affiliation":"University of Utah","email":"alexander.lex@gmail.com","name":"Alexander Lex"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"ebdd5b65-b5cb-4d4b-bd78-92ff8cc2f243","image_caption":"Any practitioners who would like to conduct user studies may benefit from the system and the design choices presented in this paper.","keywords":["User studies","crowdsourcing","visualization experiments"],"open_access_supplemental_link":"https://osf.io/e8anx/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1777-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc3Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.q7p9qn7fCRIXbFcDdkHI76BhmzigX5nYebEPbd7cV1U","preprint_link":"https://arxiv.org/abs/2508.03876","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1777","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ebdd5b65-b5cb-4d4b-bd78","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"ReVISit 2: A Full Experiment Life Cycle User Study Framework","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"54daa89a-c0e5-4d1e-9891-4ce10711d2f9","abstract":"This work investigates the current research on in-situ visualisations for running: visualisations about data that are referred to during the running activity. We analyse 47 papers from 33 Human-Computer Interaction and Visualisation venues and identify six dimensions of a design space of in-situ running visualisations. Our analysis of this design space highlights an emerging trend: a shift from on-body, peripersonal visualisations (i.e., in the space within direct reach, such as visualisations on a smartwatch or a mobile phone display) towards extrapersonal displays (i.e., in the space beyond immediate reach, such as visualisations in immersive augmented reality displays) that integrate data in the runner\u2019s surrounding environment. We explore this opportunity by conducting a series of workshops with 10 active runners in total, eliciting design concepts for running visualisations and interactions beyond conventional 2D displays. We find that runners show a strong interest for visualisation designs that favour more context-aware, interactive, and unobtrusive experiences that seamlessly integrate with their run. These findings inform a set of design considerations for future immersive running visualisations and highlight directions for further research.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Queensland","email":"leon.li.ang@foxmail.com","name":"Ang Li"},{"affiliation":"University of Victoria","email":"cperin@uvic.ca","name":"Charles Perin"},{"affiliation":"University of Queensland","email":"g.demartini@uq.edu.au","name":"Gianluca Demartini"},{"affiliation":"University of Queensland","email":"viller@acm.org","name":"Stephen Viller"},{"affiliation":"The University of Queensland","email":"j.knibbe@uq.edu.au","name":"Jarrod Knibbe"},{"affiliation":"The University of Queensland","email":"m.cordeil@uq.edu.au","name":"Maxime Cordeil"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"54daa89a-c0e5-4d1e-9891-4ce10711d2f9","image_caption":"This paper is particularly relevant for practitioners in human-computer interaction (HCI), immersive technology designers, sports technology developers, visualisation specialists, and UX designers who aim to enhance user experiences in physical activities such as running. Professionals developing XR applications, wearable technologies, and interactive fitness solutions will find practical insights into designing context-aware, and unobtrusive visualisations.\nPractitioners can apply findings from this paper to better understand how runners interact with data during physical exertion and how immersive visualisations can be effectively integrated into running contexts. The design recommendations, frameworks, and identified gaps will help practitioners create systems that go beyond traditional monitoring and support real-time decision-making, improved performance, social connectivity, and greater engagement.","keywords":["Running","Jogging; Survey","Taxonomy; Human-Subjects Qualitative Studies; Personal Visual Analytics; Mobile; Augmented/Mixed/Extended Reality","Immersive"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1017-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTAxNy1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjcxLCJleHAiOjE3OTI0Nzk2NzF9.TUO98r5xgNoEKPD6lUKn7ppg8P3JL9EPnFkE4-Q4K8I","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1017","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"54daa89a-c0e5-4d1e-9891","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Running with Data: a Survey of the Current Research and a Design Exploration of Future Immersive Visualisations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ba183a14-a911-4177-8f18-c35ee2220c67","abstract":"Effective visualization retrieval necessitates a clear definition of similarity. Despite the growing body of work in specialized visualization retrieval systems, a systematic approach to understanding visualization similarity remains absent. We introduce the Similarity Framework for Visualization Retrieval (Safire), a conceptual model that frames visualization similarity along two dimensions: comparison criteria and representation modalities. Comparison criteria identify the aspects that make visualizations similar, which we divide into primary facets (data, visual encoding, interaction, style, metadata) and derived properties (data-centric and human-centric measures). Safire connects what to compare with how comparisons are executed through representation modalities. We categorize existing representation approaches into four groups based on their levels of information content and visualization determinism: raster image, vector image, specification, and natural language description, together guiding what is computable and comparable. We analyze several visualization retrieval systems using Safire to demonstrate its practical value in clarifying similarity considerations. Our findings reveal how particular criteria and modalities align across different use cases. Notably, the choice of representation modality is not only an implementation detail but also an important decision that shapes retrieval capabilities and limitations. Based on our analysis, we provide recommendations and discuss broader implications for multimodal learning, AI applications, and visualization reproducibility.","accessible_pdf":null,"authors":[{"affiliation":"Harvard Medical School","email":"huyen_nguyen@hms.harvard.edu","name":"Huyen N. Nguyen"},{"affiliation":"Harvard Medical School","email":"nils@hms.harvard.edu","name":"Nils Gehlenborg"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ba183a14-a911-4177-8f18-c35ee2220c67","image_caption":"Practitioners such as data scientists, visualization system developers, HCI researchers, and data journalists would be interested in this paper. Additionally, information retrieval specialists and researchers in fields that rely heavily on visual data exploration, such as business analysts and bioinformaticians, may also find it valuable.\n\nPractitioners can use the Safire framework to better define and justify what similarity means for their specific visualization retrieval tasks, ensuring that system design choices are well-articulated and aligned with their goals. By considering how different design decisions regarding criteria and modalities impact retrieval capabilities and limitations, as presented in the paper, practitioners can design or evaluate retrieval tools that match their users' needs and enable consistent comparison across tools and systems.","keywords":["Visualization retrieval","similarity framework","visualization similarity","representation modality","comparison"],"open_access_supplemental_link":null,"open_access_supplemental_question":"To promote transparency and reproducibility, our work introduces Safire, a systematic framework that makes explicit the comparison criteria and representation modalities underlying visualization retrieval systems, helping researchers clearly articulate their similarity definitions and approaches. By analyzing existing systems through this lens, we provide actionable insights and recommendations to enhance the reproducibility and extensibility of future retrieval methods.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1160-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExNjAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.fpcxP3qRyfvLxEgmMjXXI2foswF-XogWO4Edj_5L0Ac","preprint_link":"https://osf.io/preprints/osf/p47z5","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1160","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ba183a14-a911-4177-8f18","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Safire: Similarity Framework for Visualization Retrieval","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"2b7f7862-26fb-4ce7-81a9-01537d804ead","abstract":"In data-driven storytelling contexts such as data journalism and data videos, data visualizations are often presented alongside real-world imagery to support narrative context. However, these visualizations and contextual images typically remain separated, limiting their combined narrative expressiveness and engagement. Achieving this is challenging due to the need for fine-grained alignment and creative ideation. To address this, we present SceneLoom, a Vision-Language Model (VLM)-powered system that facilitates the coordination of data visualization with real-world imagery based on narrative intents. Through a formative study, we investigated the design space of coordination relationships between data visualization and real-world scenes from the perspectives of visual alignment and semantic coherence. Guided by the derived design considerations, SceneLoom leverages VLMs to extract visual and semantic features from scene images and data visualization, and perform design mapping through a reasoning process that incorporates spatial organization, shape similarity, layout consistency, and semantic binding. The system generates a set of contextually expressive, image-driven design alternatives that achieve coherent alignments across visual, semantic, and data dimensions. Users can explore these alternatives, select preferred mappings, and further refine the design through interactive adjustments and animated transitions to support expressive data communication. A user study and an example gallery validate SceneLoom's effectiveness in inspiring creative design and facilitating design externalization.","accessible_pdf":null,"authors":[{"affiliation":"Fudan University","email":"lgao.lynne@gmail.com","name":"Lin Gao"},{"affiliation":"The Hong Kong University of Science and Technology","email":"lshenaj@connect.ust.hk","name":"Leixian Shen"},{"affiliation":"Fudan Univerisity","email":"yuhengzhao_cn@163.com","name":"Yuheng Zhao"},{"affiliation":"Fudan University","email":"22307130107@m.fudan.edu.cn","name":"Jiexiang Lan"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"},{"affiliation":"Fudan University","email":"simingchen3@gmail.com","name":"Siming Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"2b7f7862-26fb-4ce7-81a9-01537d804ead","image_caption":"Data Journalists, Information Designers, Video Makers, Creative Technologists, and Researchers in Visualization, HCI, and VLM Applications","keywords":["Creativity Support","Data Communication","Scene Context","Vision-Language Model"],"open_access_supplemental_link":"https://lynnegao.me/scene-loom/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1058-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA1OC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjY0LCJleHAiOjE3OTI0Nzk2NjR9.dFyrRTP2bMTPrF15IGaXaNRGpC9tXUxp9_KGy31ZQZc","preprint_link":"https://arxiv.org/abs/2507.16466","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1058","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"2b7f7862-26fb-4ce7-81a9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"SceneLoom: Communicating Data with Scene Context","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"0f3ea5e8-4bc5-4e8a-8619-1ce981d4e8e1","abstract":"Marksmanship practices are required in various professions, including police, military personnel, hunters, as well as sports shooters, such as Olympic shooting, biathlon, and modern pentathlon. The current form of training and coaching is mostly based on repetition, where the coach does not see through the eyes of the shooter, and analysis is limited to stance and accuracy post-session. In this study, we present a shooting visualization system and evaluate its perceived effectiveness for both novice and expert shooters. To achieve this, five composite visualizations were developed using first-person shooting video recordings enriched with overlaid metrics and graphical summaries. These views were evaluated with 10 participants (5 expert marksmen, 5 novices) through a mixed-methods study including shot-count and aiming interpretation tasks, pairwise preference comparisons, and semi-structured interviews. The results show that a dashboard-style composite view, combining raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases and supported understanding across skill levels. The insights gained from this design study point to the broader value of integrating first-person video with visual analytics for coaching, and we suggest directions for applying this approach to other precision-based sports.","accessible_pdf":null,"authors":[{"affiliation":"Mid Sweden University","email":"emin.zerman@miun.se","name":"Emin Zerman"},{"affiliation":"Mid Sweden University","email":"j_c_kc@live.se","name":"Jonas Carlsson"},{"affiliation":"Mid Sweden University","email":"marten.sjostrom@miun.se","name":"M\u00e5rten Sj\u00f6str\u00f6m"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"0f3ea5e8-4bc5-4e8a-8619-1ce981d4e8e1","image_caption":"This paper will be of interest to practitioners involved in sports coaching, sports technology development, human performance analysis, and training design, particularly those working in precision sports such as marksmanship, archery, and baseball. Practitioners focusing on video-based feedback systems, sports biomechanics, and skill acquisition may also find valuable insights.\n\nThe study demonstrates how composite visualizations combined with first-person video recordings can enhance training by providing rapid, interpretable feedback that connects athletes' physical actions to their performance outcomes. Coaches can use such video-enhanced visualizations to better observe subtle performance aspects like breathing control, aiming stability, and timing. These elements are typically hard to capture in conventional training setups.\n\nPractitioners can apply these findings to develop or adapt visualization-based feedback systems for their specific sports, leveraging video recordings as a central component. The design considerations discussed (such as supporting both novices and experts and providing customization options) offer practical guidance for building effective, user-centered training tools. Additionally, the paper\u2019s insights encourage iterative design cycles that incorporate user feedback at various skill levels, which can help ensure that the visualization tools meet the distinct needs of learners and instructors.\n\nThe techniques and lessons from this study can easily be extended to other sports where timing, accuracy, and motion tracking are critical, enabling practitioners to create flexible, scalable, and impactful visual feedback systems that improve both skill acquisition and coaching effectiveness.","keywords":["Composite visualization","sports coaching","marksmanship training","first-person video","user studies"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1143-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExNDMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.zl_FCX-2FIURxwZzki6jD-bIcgoB7NQf5X0RsFBK1hc","preprint_link":"https://arxiv.org/abs/2507.00333","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1143","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"0f3ea5e8-4bc5-4e8a-8619","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3eefd0dc-4542-4d3b-8a9e-bd1530cdebab","abstract":"Dimensionality reduction techniques help analysts make sense of complex, high-dimensional spatial datasets, such as multiplexed tissue imaging, satellite imagery, and astronomical observations, by projecting data attributes into a two-dimensional space. However, these techniques typically abstract away crucial spatial, positional, and morphological contexts, complicating interpretation and limiting insights. To address these limitations, we present SEAL, an interactive visual analytics system designed to bridge the gap between abstract 2D embeddings and their rich spatial imaging context. SEAL introduces a novel hybrid-embedding visualization that preserves image and morphological information while integrating critical high-dimensional feature data. By adapting set visualization methods, SEAL allows analysts to identify, visualize, and compare selections\u2014defined manually or algorithmically\u2014in both the embedding and original spatial views, facilitating a deeper understanding of the spatial arrangement and morphological characteristics of entities of interest. To elucidate differences between selected sets of items, SEAL employs a scalable surrogate model to calculate feature importance scores, identifying the most influential features governing the position of objects within embeddings. These importance scores are visually summarized across selections, with mathematical set operations enabling detailed comparative analyses. We demonstrate SEAL's effectiveness and versatility through three case studies: colorectal cancer tissue analysis with a pharmacologist, melanoma investigation with a cell biologist, and exploration of sky survey data with an astronomer. These studies underscore the importance of integrating image context into embedding spaces when interpreting complex imaging datasets. Implemented as a standalone tool while also integrating seamlessly with computational notebooks, SEAL provides an interactive platform for spatially informed exploration of high-dimensional datasets, significantly enhancing interpretability and insight generation.","accessible_pdf":null,"authors":[{"affiliation":"Harvard University","email":"simonwarchol@g.harvard.edu","name":"Simon Warchol"},{"affiliation":"Harvard University","email":"gguo31@g.harvard.edu","name":"Grace Guo"},{"affiliation":"Harvard University","email":"jknittel@seas.harvard.edu","name":"Johannes Knittel"},{"affiliation":"Harvard Medical School","email":"dfreeman@g.harvard.edu","name":"Dan Freeman"},{"affiliation":"Harvard University","email":"usha_bhalla@g.harvard.edu","name":"Usha Bhalla"},{"affiliation":"Harvard Medical School","email":"jeremy_muhlich@hms.harvard.edu","name":"Jeremy Muhlich"},{"affiliation":"Harvard University","email":"peter_sorger@hms.harvard.edu","name":"Peter Sorger"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3eefd0dc-4542-4d3b-8a9e-bd1530cdebab","image_caption":"Practitioners who work with high-dimensional data linked to spatial or image context\u2014such as single-cell biologists, geospatial analysts, and astrophysicists\u2014can use SEAL\u2019s hybrid embedding and spatial-image views to map clusters and outliers back to real-world coordinates. This blended visualization speeds interpretation, sharpens hypothesis generation, and guides downstream analyses.","keywords":["Dimensionality Reduction","Data Visualization","Spatial Image Analysis","Explainable AI","Feature Importance Modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1936-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkzNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQwLCJleHAiOjE3OTI0ODA5NDB9.fj9aSXZ476CCXteDbZbbKdHFupieP8HKiWUOZZwi6TE","preprint_link":"https://www.biorxiv.org/content/10.1101/2025.07.19.665696","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1936","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3eefd0dc-4542-4d3b-8a9e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"SEAL: Spatially-resolved Embedding Analysis with Linked Imaging Data","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"707ca454-9b92-46e1-88a4-98f997fb6434","abstract":"Deep learning (DL) models have proven to be suitable for various applications, achieving state-of-the-art performance. Despite that, they experience notable performance drops when subjected to realistic transformations in input data. Analyzing model behavior under input transformations is essential to preemptively identify possible\nbreaking points and understand what image characteristics might cause them before their failure in the real world. We introduce SEG-RobustEye, a visual analytics (VA) design developed to assist in the evaluation of the robustness of segmentation models for endoscopy images under realistic input transformations. SEG-RobustEye is based on ProactiV [13], a VA framework designed for understanding the behaviour of image-to-image translation models. SEG-RobustEye is a tailored instance of the framework and an extension of ProactiV for medical images, in concrete endoscopic images. These require visual designs that enhance the relevant features for such medical applications, which are different from general natural scenes. SEG-RobustEye is designed to discover features that affect the model behaviour under specific transformations.\nSEG-RobustEye connects the provided perspectives by ProactiV, i.e., global and instance level, and extends to subgroup level patterns. Subgroup level patterns facilitate the discovery and generalizability of selected subgroups of instances. The value of our approach was verified against real-world cases in endoscopy imaging by DL developers as proof of concept of the potential of SEG-RobustEye and, by extension, of ProactiV.\n\n[13] V. Prasad, R. J. van Sloun, A. Vilanova, and N. Pezzotti. ProactiV: Studying deep learning model behavior under input transformations. IEEE Transactions on Visualization and Computer Graphics, 30(8):5651\u20135665, 2024","accessible_pdf":null,"authors":[{"affiliation":"Eindhoven University of Technolocy","email":"popa.ada@gmail.com","name":"Andreea Melania Popa"},{"affiliation":"Eindhoven University of Technology","email":"v.prasad@tue.nl","name":"Vidya Prasad"},{"affiliation":"Eindhoven University of Technolocy","email":"t.j.m.jaspers@tue.nl","name":"Tim J.M. Jaspers"},{"affiliation":"Eindhoven University of Technology","email":"fvdsommen@tue.nl","name":"Fons van der Sommen"},{"affiliation":"Eindhoven University of Technology","email":"a.vilanova@tue.nl","name":"Anna Vilanova"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"707ca454-9b92-46e1-88a4-98f997fb6434","image_caption":"This paper is relevant for deep learning model developers and researcher and particularly for researcher that work with medical image segmentation. This work can be applied to explore the robustness of an already trained model on a predefined set of relevant input corruptions. It can be applied for any model as it is model agnostic and only needs the optimization model to be updated based on the used evaluation metric and the model loss.","keywords":["Visual analytics","explainable AI","medical imaging","input transformations","robustness analysis","model behavior","deep learning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1153-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExNTMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.K1KTe2jMcH7yEQw5Rf0SlWkHGrrXRl5qcSZu6ynXiLY","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1153","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"707ca454-9b92-46e1-88a4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"SEG-RobustEye: Understanding medical image segmentation models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"84ef8d69-bdba-43e8-a078-ae651a3de164","abstract":"Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and a user study demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.","accessible_pdf":null,"authors":[{"affiliation":"East China Normal University","email":"51275902056@stu.ecnu.edu.cn","name":"Nan Xiang"},{"affiliation":"East China Normal University","email":"51215901019@stu.ecnu.edu.cn","name":"Tianyi Liang"},{"affiliation":"East China Normal University","email":"hwhuang@stu.ecnu.edu.cn","name":"Haiwen Huang"},{"affiliation":"East China Normal University","email":"52265901032@stu.ecnu.edu.cn","name":"Shiqi Jiang"},{"affiliation":"School of Computer Science and Technology","email":"2690210766@qq.com","name":"Hao Huang"},{"affiliation":"East China Normal University","email":"yifeihuang17@gmail.com","name":"Yifei Huang"},{"affiliation":"East China Normal University","email":"lychen@sei.ecnu.edu.cn","name":"Liangyu Chen"},{"affiliation":"School of Computer Science and Technology","email":"cbwang@cs.ecnu.edu.cn","name":"Changbo Wang"},{"affiliation":"East China Normal University","email":"chli@cs.ecnu.edu.cn","name":"Chenhui Li"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"84ef8d69-bdba-43e8-a078-ae651a3de164","image_caption":"This work primarily targets practitioners in 3D content creation, including digital artists, industrial designers, and CAD software developers. The proposed system (Sel3DCraft) offers valuable practical applications, as professionals can adopt its AI-assisted selection paradigm to reduce repetitive operations and gain inspiration. Furthermore, the comparative study results provide evidence-based insights for designing more intuitive Text-to-3D interfaces, benefiting users while informing tool development strategies across the industry.","keywords":["Prompt engineering","text-to-3D generation","shape exploration","visualization design","visual perception"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1770-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc3MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.hiyLTH_krbi6CjMLdDCd0Tjp5_ux1TVwp1RhWZMYUBY","preprint_link":"https://arxiv.org/abs/2508.00428","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1770","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"84ef8d69-bdba-43e8-a078","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"c1893c22-b83f-4f15-82d7-c8f37bd65213","abstract":"Recovering a continuous colormap from a single 2D scalar field visualization can be quite challenging, especially in the absence of a corresponding color legend. In this paper, we propose a novel colormap recovery approach that extracts the colormap from a color-encoded 2D scalar field visualization by simultaneously predicting the colormap and underlying data using a decoupling-and-reconstruction strategy. Our approach first separates the input visualization into colormap and data using a decoupling module, then reconstructs the visualization with a differentiable color-mapping module. To guide this process, we design a reconstruction loss between the input and reconstructed visualizations, which serves both as a constraint to ensure strong correlation between colormap and data during training, and as a self-supervised optimizer for fine-tuning the predicted colormap of unseen visualizations during inferencing. To ensure smoothness and correct color ordering in the extracted colormap, we introduce a compact colormap representation using cubic B-spline curves and an associated color order loss. We evaluate our method quantitatively and qualitatively on a synthetic dataset and a collection of real-world visualizations from the VIS30K dataset~\\cite{Chen21}. Additionally, we demonstrate its utility in two prototype applications-colormap adjustment and colormap transfer-and explore its generalization  to  visualizations with color legends and ones encoded using discrete color palettes.","accessible_pdf":"Accessible","authors":[{"affiliation":"Shandong University","email":"liuhongxu_0227@163.com","name":"Hongxu Liu"},{"affiliation":"Shandong University","email":"xinyu.chen0468@gmail.com","name":"Xinyu Chen"},{"affiliation":"Shandong University","email":"zhenghaoyang1229@163.com","name":"Haoyang Zheng"},{"affiliation":"Shandong University","email":"manyili@sdu.edu.cn","name":"Manyi Li"},{"affiliation":"Shandong University","email":"liuzhenfannn@163.com","name":"Zhenfan Liu"},{"affiliation":"University of Maryland College Park","email":"fumeng.p.yang@gmail.com","name":"Fumeng Yang"},{"affiliation":"Renmin University of China","email":"cloudseawang@gmail.com","name":"Yunhai Wang"},{"affiliation":"Shandong Univ.","email":"changhe.tu@gmail.com","name":"Changhe Tu"},{"affiliation":"Shandong University","email":"qiong.zn@sdu.edu.cn","name":"Qiong Zeng"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"c1893c22-b83f-4f15-82d7-c8f37bd65213","image_caption":"Practitioners such as data scientists, visualization designers, and researchers in scientific fields would benefit from this paper. The proposed colormap recovery method enhances data presentation by predicting missing colormaps and underlying data, making it particularly valuable for in-the-wild visualizations. This technique improves the interpretability and accessibility of visualizations, especially when essential metadata, like a color legend, is absent.","keywords":["Colormap","color design","2D scalar field visualization","reverse engineering"],"open_access_supplemental_link":"osf.io/gb2tx/?view_only=4d2a269b59bc4144b2806ec2d1a34e11","open_access_supplemental_question":"This work addresses the challenge of recovering a continuous colormap from a 2D scalar field visualization without a corresponding color legend. We propose a novel approach that simultaneously predicts the colormap and underlying data using a decoupling-and-reconstruction strategy, offering a new method for colormap recovery in visualization.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1143-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTE0My1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.XZfSaHK_vlSlVM0LxtbJw6vSVTI2_Wvpj7LozA-tuZY","preprint_link":"http://arxiv.org/abs/2507.20632","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1143","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"c1893c22-b83f-4f15-82d7","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualizations without a Legend","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"21590a9a-129a-4f34-b457-0e490dc57bce","abstract":"Transformer-based language models have demonstrated remarkable capabilities across various tasks, yet their internal mechanisms\u2014such as layered representations, distributed attention, and evolving token semantics\u2014remain challenging to interpret. We present Semantic Pathway, an interactive visual analytics tool designed to reveal how token representations evolve across layers in autoregressive Transformer models such as GPT-2. The system integrates layerwise semantic trajectories, attention overlays, and output probability views into a unified interface, enabling users to trace how meaning accumulates and decisions emerge during generation. To reduce visual and interaction complexity, Semantic Pathway incorporates attention-based influence filtering, optional nearest-token projections, and a Compare Mode for analyzing divergence across alternate outputs. The design prioritizes interpretability and usability, supporting both fine-grained inspection and high-level exploration of sequence modeling behavior. This work contributes to ongoing efforts to make language models more interpretable, educationally accessible, and open to diagnostic insight.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"mithilesh.singh@stonybrook.edu","name":"Mithilesh Singh"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"21590a9a-129a-4f34-b457-0e490dc57bce","image_caption":"This paper may be of interest to data scientists, machine learning engineers, NLP researchers, and educators working with large language models (LLMs). Practitioners can apply the insights and visualization techniques from Semantic Pathway to better understand how transformer models evolve meaning across layers, diagnose unexpected model behaviors, or build educational tools that explain LLM internals to non-experts. The interactive interface supports exploratory analysis and debugging by revealing how token representations and attention evolve during the generation process.","keywords":["Large Language Models","Interpretability","Hidden States","Attention Weights","Semantic Pathway Visualization","Token Influence","Interactive Visualization","Transformer Models"],"open_access_supplemental_link":"https://vimeo.com/1080448380/2f38ba5dfc","open_access_supplemental_question":"We provide a detailed walkthrough video demonstrating all features of our interactive tool, and highlight our design decisions for interpretability. The system and visual encodings are thoroughly documented, supporting reproducibility and educational reuse.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1322-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMjItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.293z46D0cm29u8H4CRVo-s8MPU-8KszEHAacj1tGLhc","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1322","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"21590a9a-129a-4f34-b457","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Semantic Pathway: An Interactive Visualization of Hidden States and Token Influence in LLMs","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"deb36a29-0227-4522-a7d0-42623a49081e","abstract":"While word clouds pack a lot of data into a relatively small area, it is unclear whether readers actually benefit from all of that information, or if they are only processing a few of the words shown. We sought to determine if words outside of a reader's central vision were contributing to their interpretation by leveraging the semantic priming effect: a phenomenon in which participants will more quickly recognize a word if they have been recently primed with a word that is semantically related. We presented participants with word clouds containing related and unrelated words to see whether they might prime this lexical decision task more strongly than single words alone. We showed that the peripheral contents of a cloud do affect participant performance at this task, though more work is needed to understand the impacts of these differences in real-world settings.","accessible_pdf":"Accessible","authors":[{"affiliation":"Stanford University","email":"xingyiz@stanford.edu","name":"Xingyi Zhang"},{"affiliation":"Carleton College","email":"jakejasmer@gmail.com","name":"Jake Jasmer"},{"affiliation":"Carleton College","email":"masaddee@carleton.edu","name":"Ethan Masadde"},{"affiliation":"Carleton College","email":"ealexander@carleton.edu","name":"Eric Alexander"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"deb36a29-0227-4522-a7d0-42623a49081e","image_caption":"This paper applies to practitioners working on summarizing text documents or collections of text documents. There has been disagreement about whether some common means of visualizing such data are effective, and word clouds are both one of the most common methods and one of the most debated. This work contributes to that debate and may help practitioners make choices that will help the accuracy and legibility of their visualizations of text data.","keywords":["word clouds","semantic priming","text perception"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1328-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMjgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.NBcPgSOzSL5Gt9OKgsTLzB-oY8OaqT2ZYOWTlm_E8nc","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1328","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"deb36a29-0227-4522-a7d0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Semantic Priming in Word Clouds","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1acd711b-69a4-486f-9955-0f6adb58292b","abstract":"Recent work suggests that shape can encode quantitative data via a mapping between value and spatial frequency (SF). However, the set-size effect when perceiving multiple SF based items remains unclear. While automatic feature extraction has been found to be less affected by set size (number of items in a group), higher-level processes for making perceptual decisions tend to require increased cognitive demand. To investigate the set-size effect on comparing integrated SF based items, we used a risk-based scenario to assess discrimination performance. Participants were asked to discriminate between pairs of maps containing multiple SF glyphs, in which each glyph represents one of four discrete levels (none, low, medium, high), forming an aggregate \u201crisk strength\u201d per map. The set size was also adjusted across conditions, ranging from small (3 items) to large (7 items). Discrimination sensitivity is modeled with a logistic function and response time with a mixed-effect linear model. Results show that smaller set sizes and lower overall strength enable more precise discrimination, with faster response times for larger differences between maps. Incorporating set size and overall strength into the logistic model, we found that these variables both independently and jointly influence discrimination sensitivity. We suggest these results point towards capacity-limited processes rather than purely automatic ensemble coding. Our findings highlight the importance of set size and overall signal strength when presenting multiple SF glyphs in data visualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"King's College London","email":"yiran.2.li@kcl.ac.uk","name":"Yiran Li"},{"affiliation":"King's College London","email":"shan.shao@kcl.ac.uk","name":"Shan Shao"},{"affiliation":"King's College London","email":"peter.baudains@kcl.ac.uk","name":"Peter Baudains"},{"affiliation":"King's College London","email":"andrew.meso@kcl.ac.uk","name":"Andrew Meso"},{"affiliation":"King's College London","email":"nickholliman@gmail.com","name":"Nick Holliman"},{"affiliation":"King's College London","email":"alfie.abdulrahman@gmail.com","name":"Alfie Abdul-Rahman"},{"affiliation":"Kings College London","email":"rita.borgo@kcl.ac.uk","name":"Rita Borgo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"1acd711b-69a4-486f-9955-0f6adb58292b","image_caption":"UX / HCI researchers and designers of decision-support tools: Our evidence of capacity-limited read-out helps determine how many glyphs a dashboard can display before precision falls, guiding choices about interface complexity, glyph density, and progressive-reveal interactions.\n\nData scientists and analysts who communicate risk: Our task mirrors real-world comparisons in traffic, natural-hazard, and climate-impact mapping. The findings offer concrete limits for showing regional risks without overwhelming viewers.\n\nEducators in psychology and visualization: This work is a classroom-ready case of applying psychometric methods to design questions, ideal for teaching rigorous evaluation of visual encodings.","keywords":["Radial spatial frequency","Vizent glyph","set size","visual discrimination","aggregation","psychometric function"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1557-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTU1Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.-mIPQ1Q4jGxAk_bAzXyEsxvtMViKvi956y_im0tvbWs","preprint_link":"https://kclpure.kcl.ac.uk/portal/en/publications/set-size-matters-capacity-limited-perception-of-grouped-spatial-f","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1557","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1acd711b-69a4-486f-9955","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Set Size Matters: Capacity-Limited Perception of Grouped Spatial-Frequency Glyphs","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"cdeb2b03-68fd-4816-8611-544145e35ba2","abstract":"Connected scatterplots visualize time-series data by connecting the points on a scatterplot based on temporal sequence. Viewers are prone to misinterpret the direction of time in these visualizations, possibly because they encode time with an unexpected rule \u2013 along the connected line (TIME IS A LINE) instead of from left to right (RIGHT IS LATER) as conventional in line charts. In this paper, we use the connected scatterplot to illustrate a perspective on visualization comprehension centered around expectations of encoding rules. People have initial expectations of encoding rules for visualizations that can stem from conventional practices or metaphors, and these expectations have been recognized as a potential factor influencing visualization comprehension. We present three preregistered experiments (n = 1429 in total) demonstrating two kinds of design interventions to strengthen the correct expectation for time and testing their effectiveness in reducing errors for understanding realistic connected scatterplots. We found that visual treatments that suppress the incorrect chart-type expectation and directional cues that emphasize the correct expectation both led viewers to expect TIME IS A LINE more. An explicit directional cue (arrows), ideally redundantly encoded with another cue (trace-line effect or animation), was most effective for reducing misinterpretations. Our findings not only provide practical guidelines for designing connected scatterplots but also contribute theoretical insights to inform the design of novel visualizations that challenge interpretability by defying expectations.","accessible_pdf":null,"authors":[{"affiliation":"Northeastern University","email":"wenxuxmn@gmail.com","name":"Wen Xu"},{"affiliation":"Northeastern University","email":"l.padilla@northeastern.edu","name":"Lace Padilla"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"cdeb2b03-68fd-4816-8611-544145e35ba2","image_caption":"This paper will be of interest for data journalists in particular. We provide several design recommendations that can be directly applied to create connected scatterplots, a popular visualization format for data journalism, that are less prone to misinterpretation.","keywords":["Visualization","cognition","graphical convention","conceptual metaphor","connected scatterplot"],"open_access_supplemental_link":"https://osf.io/k45es/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1602-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTYwMi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.XRlPRT1Uq5QO46PYJQykXSZe_XNOhyvCejR_pJcA3HE","preprint_link":"https://osf.io/preprints/osf/4mc2b","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1602","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"cdeb2b03-68fd-4816-8611","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Shifting Expectations for Encoding Rules Mitigates Misinterpretation of Connected Scatterplots","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"89bf8d86-33c7-4bf0-b790-98bf9272ee2e","abstract":"Hagrid is a state-of-the-art space-filling-curve-based method for gridifying scatterplots. However, it exhibits limitations in preserving the global structures of scatterplots with areas of varying density due to the incompatibility of adapting the granularity level of the underlying space-filling curve to regions with different densities. To compensate for this shortcoming, we introduce SiGrid that combines Hagrid with the Sector-Based Regularization (SBR) technique. SiGrid applies SBR to generate a scatterplot with a more uniform and generally lower density as an intermediate step. This intermediate scatterplot can then be fed to Hagrid for improved results. We quantitatively evaluate SiGrid by comparing it to Hagrid over a set of 502 scatterplots of different sizes, ranging from 50 to 10000 points per dataset, using relevant quality metrics. While generally slower, the results demonstrate that SiGrid outperforms Hagrid regarding the quality metrics of rank-wise neighborhood preservation (trustworthiness), ordering preservation, and pairwise distance preservation (cross-correlation).","accessible_pdf":null,"authors":[{"affiliation":"University of Stuttgart","email":"rene.cutura@visus.uni-stuttgart.de","name":"Rene Cutura"},{"affiliation":"University of M\u00fcnster","email":"hennes.rave@uni-muenster.de","name":"Hennes Rave"},{"affiliation":"University of Stuttgart","email":"quynh.ngo@visus.uni-stuttgart.de","name":"Quynh Ngo"},{"affiliation":"University of M\u00fcnster","email":"molchano@uni-muenster.de","name":"Vladimir Molchanov"},{"affiliation":"University of M\u00fcnster","email":"linsen@uni-muenster.de","name":"Lars Linsen"},{"affiliation":"University of Stuttgart","email":"weiskopf@visus.uni-stuttgart.de","name":"Daniel Weiskopf"},{"affiliation":"University of Stuttgart","email":"michael.sedlmair@visus.uni-stuttgart.de","name":"Michael Sedlmair"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"89bf8d86-33c7-4bf0-b790-98bf9272ee2e","image_caption":"People creating visualizations using glyphs or images in scatterplots.","keywords":["Scatterplot","Space-filling curve","Grid layout","Neighborhood preservation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1261-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNjEtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU3NywiZXhwIjoxNzkyNDgyNTc3fQ.IQiQXWywbtp-OrvfnAnCXn-kn6sRg7wdMRu469m6Zk0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1261","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"89bf8d86-33c7-4bf0-b790","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"SiGrid: Gridifying Scatterplots with Sector-Based Regularization and Hagrid","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"470f58d3-7a7a-47fd-baf3-82d155ac725b","abstract":"Simulacra Naturae is a data-driven media installation that explores collective care through the entanglement of biological computation, material ecologies, and generative systems. The work translates pre-recorded neural activity from brain organoids, lab-grown three-dimensional clusters of neurons, into a multi-sensory environment composed of generative visuals, spatial audio, living plants, and fabricated clay artifacts. These biosignals, streamed through a real-time system, modulate emergent agent behaviors inspired by natural systems such as termite colonies and slime molds. Rather than using biosignals as direct control inputs, Simulacra Naturae treats organoid activity as a co-creative force, allowing neural rhythms to guide the growth, form, and atmosphere of a generative ecosystem. The installation features computationally fabricated clay prints embedded with solenoids, adding physical sound resonances to the generative surround composition. The spatial environment, filled with live tropical plants and a floor-level projection layer featuring real-time generative AI visuals, invites participants into a sensory field shaped by nonhuman cognition. By grounding abstract data in living materials and embodied experience, Simulacra Naturae reimagines visualization as a practice of care, one that decentralizes human agency and opens new spaces for ethics, empathy, and ecological attunement within hybrid computational systems.","accessible_pdf":null,"authors":[{"affiliation":"University of California, Santa Barbara","email":"nefeli@ucsb.edu","name":"Nefeli Manoudaki"},{"affiliation":"University of California Santa Barbara","email":"merttoka@ucsb.edu","name":"Mert Toka"},{"affiliation":"University of California, Santa Barbara","email":"iason@ucsb.edu","name":"Iason Paterakis"},{"affiliation":"University of California Santa Barbara","email":"diarmid@ucsb.edu","name":"Diarmid Flatley"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"470f58d3-7a7a-47fd-baf3-82d155ac725b","image_caption":null,"keywords":["Artificial life","symbiosis","brain organoids","collective intelligence","generative ecosystem."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1116-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzExMTYtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg2NCwiZXhwIjoxNzkyNDc1ODY0fQ.BnWj123e9So2nRhmm7Vin7u64bH8M0uKAXvtvM-F7O0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1116","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"470f58d3-7a7a-47fd-baf3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"fe3d1eaa-d502-47c2-9939-61ad180dc260","abstract":"Current multimodal large language models (MLLMs), while effective in natural image understanding, struggle with visualization understanding due to their inability to decode the data-to-visual mapping and extract structured information. To address these challenges, we propose SimVec, a novel simplified vector format that encodes chart elements such as mark type, position, and size. The effectiveness of SimVec is demonstrated by using MLLMs to reconstruct chart information from SimVec formats. Then, we build a new visualization dataset, SimVecVis, to enhance the performance of MLLMs in visualization understanding, which consists of three key dimensions: bitmap images of charts, their SimVec representations, and corresponding data-centric question-answering (QA) pairs with explanatory chain-of-thought (CoT) descriptions. We finetune state-of-the-art MLLMs (e.g., MiniCPM and Qwen-VL), using SimVecVis with different dataset dimensions. The experimental results show that it leads to substantial performance improvements of MLLMs with good spatial perception capabilities (e.g., MiniCPM) in data-centric QA tasks. Our dataset and source code are available at: https://github.com/VIDA-Lab/SimVecVis.","accessible_pdf":null,"authors":[{"affiliation":"Nanyang Technological University","email":"can.liu.1996@gmail.com","name":"Can Liu"},{"affiliation":"UNIVERSITY OF ST ANDREWS","email":"dachunlin00@gmail.com","name":"Chunlin Da"},{"affiliation":"Nanjing University","email":"xxlong@nju.edu.cn","name":"Xiaoxiao Long"},{"affiliation":"Tsinghua University","email":"yuxiao@fondant.design","name":"Yuxiao Yang"},{"affiliation":"Huawei Technologies Co., Ltd","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"fe3d1eaa-d502-47c2-9939-61ad180dc260","image_caption":"- Visualization Researchers / Engineers: Focus on visualization system design and chart generation tools (e.g., Vega-Lite, D3.js). They benefit from methods that convert charts into structured, machine-readable data.\n- Multimodal AI Researchers (MLLMs): Study vision-language models (e.g., GPT-4o, MiniCPM, Qwen-VL) and their ability to understand structured visual inputs like charts. SimVec offers a useful intermediate representation to improve performance.\n- Digital Humanities Scholars: Work with historical or hand-drawn charts. The approach supports digitizing and extracting data from imperfect or scanned visualizations.","keywords":["Visualization LLM","Multimodal LLM","Chart QA"],"open_access_supplemental_link":"https://github.com/VIDA-Lab/SimVecVis","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1151-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExNTEtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.hA93Lfmx9JZVtaKEui__tFFLLOHrcq_6LP5QDLivr6Y","preprint_link":"https://arxiv.org/abs/2506.21319","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1151","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"fe3d1eaa-d502-47c2-9939","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"2b91d1cf-ebf6-46b2-9cbc-cc51da3a56ed","abstract":"Tens of thousands of people have represented data by creating data-encoding textile pieces like blankets, scarves, and more. A prototypical example is the temperature blanket, which represents the weather through rows or blocks of different colors mapped to temperature ranges. While researchers have used fiber arts mediums to create exploratory projects, data visualization and physicalization research has largely not engaged with examples from this enormous and diverse community. We explore the pace of data textiles, or fiber arts that encode information, by surveying creators (i.e., data fiber artists) on their projects and processes. We create a corpus of 159 examples of data textiles and present a schema characterizing the data encoding methods used in these projects. We also gather insights into creators\u2019 data workflows as well as their motives and discoveries through making with their data. Creators of data textiles use distinct processes to map their data, building fabric from component structures and substructures while using material properties like color and texture. From diverse data-tracking procedures, creators use and relate to data in varied ways. Working on these pieces also contributes to the creators\u2019 personal growth and data understanding. Our findings point to new opportunities for visualization, including opportunities to support fiber artists with tools formatted to their needs and opportunities to incorporate concepts from data textiles into other types of visualization (e.g., using texture, structural layouts, colorways).","accessible_pdf":"Accessible","authors":[{"affiliation":"Northeastern University","email":"purdue.sy@northeastern.edu","name":"Sydney Purdue"},{"affiliation":"Northeastern University","email":"eduardopuertac@gmail.com","name":"Eduardo Puerta"},{"affiliation":"Northeastern University","email":"e.bertini@northeastern.edu","name":"Enrico Bertini"},{"affiliation":"Northeastern University","email":"m.tory@northeastern.edu","name":"Melanie Tory"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"2b91d1cf-ebf6-46b2-9cbc-cc51da3a56ed","image_caption":"Designers of visualizations, physicalizations, or other data-representation projects will be the most interested in reading this paper. With our categorizations of encoding methods, designers can take inspiration from these works towards creating novel visualizations, and these practitioners may also find areas of interest in ways to engage with understanding data over time. Data artists will be interested in ways they could use textiles methods in their work. Researchers interested in developing new visual encodings or encoding schemas will also find relevant portions in this paper.","keywords":["Data physicalization","textiles","data textiles","craft","VisArt","data practices","visual encoding design"],"open_access_supplemental_link":"https://osf.io/t9jhu/","open_access_supplemental_question":"We collected a corpus of 159 examples of data textiles, and we provide this corpus, including all submitted images for the pieces as well as a summary of the description provided, as a part of our supplemental materials. Notably, given the nature of these pieces as artistic objects, we asked our participants for their preferred attribution, and we attribute 142 projects to the name of that creator's choice, with an additional 14 attributed anonymously, and 3 held confidential.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1367-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTM2Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.Kf2vrxi9GDw__hHgexqJvfNUVYVde3lSJDW_lG9obfs","preprint_link":"https://osf.io/preprints/osf/eyw2r_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1367","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"2b91d1cf-ebf6-46b2-9cbc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Stitching Meaning: Practices of Data Textile Creators","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f9fb5a2a-c9bd-48f5-b5be-8a75e1b77b02","abstract":"Analyzing literature involves tracking interactions between characters, locations, and themes. Visualization has the potential to facilitate the mapping and analysis of these complex relationships, but capturing structured information from unstructured story data remains a challenge. As large language models (LLMs) continue to advance, we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques. Toward this goal, we introduce an LLM-driven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts. We then apply this pipeline to create Story Ribbons, an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels. Through pipeline evaluations and user studies with Story Ribbons on 36 literary works, we demonstrate the potential of LLMs to streamline narrative visualization creation and reveal new insights about familiar stories. We also describe current limitations of AI-based systems, and interaction motifs designed to address these issues.","accessible_pdf":"Accessible","authors":[{"affiliation":"Harvard University","email":"catherineyeh@g.harvard.edu","name":"Catherine Yeh"},{"affiliation":"Harvard University","email":"tarakmenon@gmail.com","name":"Tara Menon"},{"affiliation":"Harvard University","email":"aryarobinsingh@gmail.com","name":"Robin Singh Arya"},{"affiliation":"Harvard University","email":"hairong.he03@gmail.com","name":"Helen He"},{"affiliation":"Harvard University","email":"weigel@fas.harvard.edu","name":"Moira Weigel"},{"affiliation":"Harvard University","email":"viegas@google.com","name":"Fernanda Viegas"},{"affiliation":"Harvard","email":"wattenberg@gmail.com","name":"Martin Wattenberg"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f9fb5a2a-c9bd-48f5-b5be-8a75e1b77b02","image_caption":"We believe our work would interest many practitioners, such as literary scholars, educators, digital humanitarians, and writers. For example, practitioners could use Story Ribbons\u2019 interactive visualizations and custom analytic features to further explore and gain new insights into literary works, generate points of discussion, and more generally understand how characters, locations, and themes unfold across a piece of writing. Our system also offers a model for how to analyze long texts with LLMs, while calibrating trust with explanations.","keywords":["Narrative visualization","interactive literary analysis","large language models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1516-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTUxNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM0LCJleHAiOjE3OTI0ODA1MzR9.ld6p2HDKL-TQmF6-XYJMMaWWXpRnfibZ18egC8H67nI","preprint_link":"https://arxiv.org/abs/2508.06772","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1516","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f9fb5a2a-c9bd-48f5-b5be","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Story Ribbons: Reimagining Storyline Visualizations with Large Language Models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"5a5e8c22-ecab-4548-b9f8-3b84454ee079","abstract":"The visualization and analysis of street and pedestrian networks are important to various domain experts, including urban planners, climate researchers, and health experts. This has led to the development of new techniques for street and pedestrian network visualization, expanding possibilities for effective data presentation and interpretation. Despite their increasing adoption, there is no established design framework to guide the creation of these visualizations while addressing the diverse requirements of various domains. When exploring a feature of interest, domain experts often need to transform, integrate, and visualize a combination of thematic data (e.g., demographic, socioeconomic, pollution) and physical data (e.g., zip codes, street networks), often spanning multiple spatial and temporal scales. This not only complicates the process of visual data exploration and system implementation for developers but also creates significant entry barriers for experts who lack a background in programming. With this in mind, in this paper, we reviewed 45 studies utilizing street-overlaid visualizations to understand how they are applied in practice. Through qualitative coding of these visualizations, we analyzed three key aspects of street and pedestrian network visualization usage: their analytical purposes, the visualization approaches employed, and the data sources used in their creation. Building on this design space, we introduce StreetWeave, a declarative grammar for designing custom visualizations of multivariate spatial network data across multiple resolutions. We demonstrate how StreetWeave can be used to create various street-overlaid visualizations, enabling effective exploration and analysis of spatial data. StreetWeave is available at https://urbantk.org/streetweave.","accessible_pdf":null,"authors":[{"affiliation":"University of Illinois at Chicago","email":"ssraba2@uic.edu","name":"Sanjana Srabanti"},{"affiliation":"University of Illinois at Chicago","email":"g.elisabeta.marai@gmail.com","name":"G. Elisabeta Marai"},{"affiliation":"University of Illinois Chicago","email":"fabiom@uic.edu","name":"Fabio Miranda"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"5a5e8c22-ecab-4548-b9f8-3b84454ee079","image_caption":"StreetWeave is an open-source visualization grammar that enables data scientists and urban planners to create street-overlaid visualizations.","keywords":["Urban visual analytics","design space","street-overlaid visualization","visualization grammar"],"open_access_supplemental_link":"http://urbantk.org/streetweave","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1934-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkzNC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.5-UjEtEWkUzT9JCApako-xOIlRsUnD3RyvBiCKcegyg","preprint_link":"https://arxiv.org/abs/2508.07496","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1934","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"5a5e8c22-ecab-4548-b9f8","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"39db4241-5a8e-4bf7-abaf-b638cfbfee08","abstract":"Structural analysis is essential in modern industrial design, where engineers iteratively refine geometry models based on stress simulations to achieve optimized designs. However, comparing stress distributions across multiple model variants remains challenging due to the complexity of stress fields, which are high-dimensional, unevenly distributed, and dependent on intricate geometric structures. Existing tools primarily support single-model analysis and lack dedicated functionalities for multi-model comparison. As a result, engineers must rely on manual, cognitively demanding visual inspections, making it difficult to systematically identify and interpret stress variations across design iterations. To address these limitations, we propose StressDiffVis, a visual analytics approach that facilitates stress field comparison across multiple structural models. StressDiffVis employs a volumetric representation to encode stress distributions while minimizing occlusion, enabling voxel-wise difference analysis for model comparison. To support localized analysis, we introduce model segmentation, grouping voxels with similar stress patterns across models. StressDiffVis integrates these techniques into an interactive interface with a tree view, organizing models by the iterative design process, and a comparison view, using a matrix layout for detailed comparisons. We demonstrate the effectiveness of StressDiffVis through two case studies illustrating its utility in comparative stress analysis. In addition, expert interviews confirm its potential to enhance engineering workflows.","accessible_pdf":null,"authors":[{"affiliation":"South China University of Technology","email":"jiabaoh20@outlook.com","name":"Jiabao Huang"},{"affiliation":"South China University of Technology","email":"zkdeng@scut.edu.cn","name":"Zikun Deng"},{"affiliation":"South China University of Technology","email":"sehanlinsong@mail.scut.edu.cn","name":"Hanlin Song"},{"affiliation":"South China University of Technology","email":"xiangch.05@outlook.com","name":"Xiang Chen"},{"affiliation":"Greater Bay Area National Center of Technology Innovation","email":"gaoshaowu@ncti-gba.cn","name":"Shaowu Gao"},{"affiliation":"South China University of Technology","email":"ycai@scut.edu.cn","name":"Yi Cai"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"39db4241-5a8e-4bf7-abaf-b638cfbfee08","image_caption":"This paper is relevant to structural engineers, CAE practitioners, and simulation scientists involved in iterative design and stress analysis. StressDiffVis enables efficient comparison of stress distributions across multiple design alternatives through volumetric visualization and matrix-based comparison. These techniques may also inspire applications in other domains, such as fluid dynamics and electromagnetic field studies, where comparing model variants is essential.","keywords":["Stress analysis","comparative visualization","volume visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1351-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTM1MS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQxLCJleHAiOjE3OTI0ODAzNDF9.939qILiy7oJfqSsW8b56iVc_CfDhxJ0DR4MHh9nWkfs","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1351","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"39db4241-5a8e-4bf7-abaf","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"StressDiffVis: Visual Analytics for Multi-Model Stress Comparison","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b5d2c402-def7-4b9d-a6df-ac4ad7572111","abstract":"Connectomics, a subfield of neuroscience, aims to map and analyze synapse-level wiring diagrams of the nervous system. While recent advances in deep learning have accelerated automated neuron and synapse segmentation, reconstructing accurate connectomes still demands extensive human proofreading to correct segmentation errors.\nWe present SynAnno, an interactive tool designed to streamline and enhance the proofreading of synaptic annotations in large-scale connectomics datasets. SynAnno integrates into existing neuroscience workflows by enabling guided, neuron-centric proofreading. To address the challenges posed by the complex spatial branching of neurons, it introduces a structured workflow with an optimized traversal path and a 3D mini-map for tracking progress. In addition, SynAnno incorporates fine-tuned machine learning models to assist with error detection and correction, reducing the manual burden and increasing proofreading efficiency.\nWe evaluate SynAnno through a user and case study involving seven neuroscience experts. Results show that SynAnno significantly accelerates synapse proofreading while reducing cognitive load and annotation errors through structured guidance and visualization support. The source code and interactive demo are available at: https://github.com/PytorchConnectomics/SynAnno.","accessible_pdf":"Accessible","authors":[{"affiliation":"Technical University of Munich","email":"leander.lauenburg@gmail.com","name":"Leander Lauenburg"},{"affiliation":"Harvard University","email":"jakob.troidl@googlemail.com","name":"Jakob Troidl"},{"affiliation":"Boston College","email":"gohaina@bc.edu","name":"Adam Gohain"},{"affiliation":"Harvard University","email":"linzudi@gmail.com","name":"Zudi Lin"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"},{"affiliation":"Boston College","email":"donglai.wei@bc.edu","name":"Donglai Wei"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b5d2c402-def7-4b9d-a6df-ac4ad7572111","image_caption":"Neuroscientists, Neurobiologists, Computational Biologists, Bioinformaticians","keywords":["Connectomics","Synaptic Annotations","Neuron-Centric","Proofreading Workflow"],"open_access_supplemental_link":"https://github.com/PytorchConnectomics/SynAnno","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1718-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTcxOC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.p8RvwwJZfF_AJqanzDsQp9uJYg8NTZVCECjbOMxinh8","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1718","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b5d2c402-def7-4b9d-a6df","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"SynAnno: Interactive Guided Proofreading of Synaptic Annotations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"5f806c3f-27b1-47e9-81ce-9255b7532d0f","abstract":"Team-based combat scenarios are prevalent in various real-world applications like video gaming. Analyzing tactics in these scenarios is essential for gaining insights into game processes and improving combat behaviors. The decision-making data in team-based combat include character actions, movement trajectories, and event sequences. Existing studies face challenges in visualizing and analyzing combat tactics due to the complexity and the multifaceted characteristics of the decision-making data. To address these challenges, we introduce TactiVis, a visual analytics system designed for analyzing combat decision-making behavior. Using MOBA game as a representative case of team-based combat, TactiVis adopts a macro-to-micro tactics visual analytics framework consisting of three stages: match-level analysis, event-level understanding, and character-level comparison. In the TactiVis system, we introduce the v-storyline visualization, which encodes positions along the vertical axis to reveal tactical patterns. Case studies and a usability study demonstrate the utility and usability of TactiVis for helping analysts understand combat patterns and analyze tactics.","accessible_pdf":"Accessible","authors":[{"affiliation":"Beijing Institute of Technology","email":"3120215511@bit.edu.cn","name":"Hancheng Zhang"},{"affiliation":"Beijing Institute of Technology","email":"liguozhengsdu@gmail.com","name":"Guozheng Li"},{"affiliation":"Shenzhen University","email":"lumin.vis@gmail.com","name":"Min Lu"},{"affiliation":"Beijing Normal University","email":"jinchengli@bnu.edu.cn","name":"Jincheng Li"},{"affiliation":"Beijing Institute of Technology","email":"liuchi02@gmail.com","name":"Chi Harold Liu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"5f806c3f-27b1-47e9-81ce-9255b7532d0f","image_caption":"Practitioners such as game analysts, AI or reinforcement learning researchers would be interested in the paper. They can apply methods to analyze team-based tactics, visualize combat behavior, and improve agent training through better tactical data interpretation and selection.","keywords":["Team-based combat","storyline visualization","MOBA games","tactic analysis."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1897-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTg5Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.phqcoqqddmEmjvX8DfrsFJL5bzy5VR0u4p-opLiRSjo","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1897","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"5f806c3f-27b1-47e9-81ce","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"TactiVis: Towards Better Understanding of Team-based Combat Tactics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"22ccb527-7a1d-49e1-81b8-9e4f335089e1","abstract":"Accounting for individual differences can improve the effectiveness of visualization design. While the role of visual attention in visualization interpretation is well recognized, existing work often overlooks how this behavior varies based on visual literacy levels. Based on data from a 235-participant user study covering three visualization tests (mini-VLAT, CALVI, and SGL), we show that distinct attention patterns in visual data exploration can correlate with participants' literacy levels: While experts (high-scorers) generally show a strong attentional focus, novices (low-scorers) focus less and explore more. We then propose two computational models leveraging these insights: Lit2Sal -- a novel visual saliency model that predicts observer attention given their visualization literacy level, and Sal2Lit -- a model to predict visual literacy from human visual attention data. Our quantitative and qualitative evaluation demonstrates that Lit2Sal outperforms state-of-the-art saliency models with literacy-aware considerations. Sal2Lit predicts literacy with 86% accuracy using a single attention map, providing a time-efficient supplement to literacy assessment that only takes less than a minute. Taken together, our unique approach to consider individual differences in salience models and visual attention in literacy assessments paves the way for new directions in personalized visual data communication to enhance understanding.","accessible_pdf":null,"authors":[{"affiliation":"Georgia Institute of Technology","email":"minsuk@gatech.edu","name":"Minsuk Chang"},{"affiliation":"University of Stuttgart","email":"yao.wang@vis.uni-stuttgart.de","name":"Yao Wang"},{"affiliation":"University of Washington","email":"wwill@cs.washington.edu","name":"Huichen Wang"},{"affiliation":"Georgia Institute of Technology","email":"zhouyuanhong0510@gmail.com","name":"Yuanhong Zhou"},{"affiliation":"University of Stuttgart","email":"andreas.bulling@vis.uni-stuttgart.de","name":"Andreas Bulling"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"22ccb527-7a1d-49e1-81b8-9e4f335089e1","image_caption":"Vision Scientists \u2013 The study shows how people with different visualization skills look at charts differently, strongly related to vision science.\n\nVisualization Designers / UX Researchers \u2013 The Lit2Sal and Sal2Lit models help create personalized visualizations that match users' skill levels, making charts more effective and reducing confusion. Would be interested to be applied in these fields.\n\nEducational Researchers and Learning Scientists \u2013 Our model provides a quick, inexpensive way to estimate data literacy through eye tracking, enabling real-time feedback in educational tools.","keywords":["Visualization Literacy Assessment","Visual Attention and Saliency","Visual Saliency Models"],"open_access_supplemental_link":"https://osf.io/2crb9/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1490-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ5MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQzLCJleHAiOjE3OTI0ODAzNDN9.TOjg6Ca6wD-bJXxcT1bB2AkjIc4pBHM4GEdLORzcO6M","preprint_link":"https://arxiv.org/abs/2508.03713","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1490","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"22ccb527-7a1d-49e1-81b8","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"77c7ae29-07ac-4a7b-90d1-5ca6794c6632","abstract":"Advancements in volume visualization (VolVis) focus on extracting insights from 3D volumetric data by generating visually compelling renderings that reveal complex internal structures. Existing VolVis approaches have explored non-photorealistic rendering techniques to enhance the clarity, expressiveness, and informativeness of visual communication. While effective, these methods often rely on complex predefined rules and are limited to transferring a single style, restricting their flexibility. To overcome these limitations, we advocate the representation of VolVis scenes using differentiable Gaussian primitives combined with pretrained large models to enable arbitrary style transfer and real-time rendering. However, conventional 3D Gaussian primitives tightly couple geometry and appearance, leading to suboptimal stylization results. To address this, we introduce TexGS-VolVis, a textured Gaussian splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives, extending each Gaussian with additional texture and shading attributes, resulting in higher-quality, geometry-consistent stylization and enhanced lighting control during inference. Despite these improvements, achieving flexible and controllable scene editing remains challenging. To further enhance stylization, we develop image- and text-driven non-photorealistic scene editing tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing with fine-grained control. We evaluate TexGS-VolVis both qualitatively and quantitatively across various volume rendering scenes, demonstrating its superiority over existing methods in terms of efficiency, visual quality, and editing flexibility.","accessible_pdf":null,"authors":[{"affiliation":"University of Notre Dame","email":"ktang2@nd.edu","name":"Kaiyuan Tang"},{"affiliation":"University of Notre Dame","email":"kai@nd.edu","name":"Kuangshi Ai"},{"affiliation":"The Hong Kong University of Science and Technology","email":"junhanvis@outlook.com","name":"Jun Han"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"77c7ae29-07ac-4a7b-90d1-5ca6794c6632","image_caption":"This work will appeal to practitioners interested in scene representation, Gaussian splatting, and vision-language model techniques, particularly those working in the fields of scientific visualization, computer graphics, and 3D vision.\n\nPractitioners can utilize TexGS-VolVis to perform real-time, expressive editing of volume visualization scenes, including style transfer via image or text prompts, as well as control over color, opacity, and lighting. They can also learn from this paper how to design scene representation models using textured 2D Gaussian and how to integrate cutting-edge vision-language models (e.g., CLIP) into their pipelines to enable flexible and powerful scene editing.","keywords":["Novel view synthesis","style transfer","textured Gaussian splatting","vision-language model","volume visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1274-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI3NC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MjY5LCJleHAiOjE3OTI0ODAyNjl9.MPeWD6fqi0A7ECzgMyCfvQfjGXIP6Jd365AutzMPC_Q","preprint_link":"https://arxiv.org/abs/2507.13586","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1274","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"77c7ae29-07ac-4a7b-90d1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"64c1bc8e-6529-47d5-8f6e-c194f4766ec2","abstract":"Studies of visual semantics for information visualization aim to understand observers\u2019 expectations about the meaning of visual features (e.g., color, texture) because visualizations that align with those expectations are easier to interpret. Previous work on visual semantics focused primarily on color, with the implicit assumption that color semantics is unaffected by changes in the size of the visualization (given sufficient perceptual discriminability across sizes). Changing size from small scale (e.g., small figures in a paper) to large scale (e.g., large figures in a slide presentation) is straightforward for visualizations that have solid colored regions, but can be more complicated for visualizations with heterogeneous textures because there are multiple ways to scale textures\u2014zooming or repeating texture elements. Previous work suggested that original textures were more perceptually similar to repeat-scaled rather than zoom-scaled textures. Here, we found that texture semantics was preserved after both types of enlargement, suggesting that texture semantics is robust to scaling, at least for geometric textures in which elements are visible at all scales.","accessible_pdf":null,"authors":[{"affiliation":"University of Wisconsin - Madison","email":"zoeshoward@gmail.com","name":"Zoe Howard"},{"affiliation":"University of Wisconsin - Madison","email":"kschloss@wisc.edu","name":"Karen Schloss"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"64c1bc8e-6529-47d5-8f6e-c194f4766ec2","image_caption":"Designers who are interested in understanding the semantics (meaning) of visual features can use our results to inform their use of texture in data visualizations.","keywords":["information visualization","perceptual semantics","visual reasoning","texture perception","visual communication"],"open_access_supplemental_link":"https://osf.io/s7ghe/?view_only=66b92b5dd3e74ee3a231e9b4c9e786a6","open_access_supplemental_question":"The experiment stimuli, data, and analysis code are posted on OSF","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1227-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMjctZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NSwiZXhwIjoxNzkyNDgyNTY1fQ.JmYrtOadvstpBJ62cBZyvrjV5bY_UT6U0jv2eRbDCrM","preprint_link":"https://osf.io/preprints/osf/4uadx_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1227","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"64c1bc8e-6529-47d5-8f6e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Texture Semantics is Robust to Scaling","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"84ab0ade-9529-4942-b5e1-ca953e56c880","abstract":"In this paper, we present a novel compression framework, TFZ, that preserves the topology of 2D symmetric and asymmetric second-order tensor fields defined on flat triangular meshes. A tensor field assigns a tensor\u2014a multi-dimensional array of numbers \u2014 to each point in space. Tensor fields, such as the stress and strain tensors, and the Riemann curvature tensor, are essential to both science and engineering. The topology of tensor fields captures the core structure of data, and is useful in various disciplines, such as graphics (for manipulating shapes and textures) and neuroscience (for analyzing brain structures from diffusion MRI). Lossy data compression may distort the topology of tensor fields, thus hindering downstream analysis and visualization tasks. TFZ ensures that certain topological features are preserved during lossy compression. Specifically, TFZ preserves degenerate points essential to the topology of symmetric tensor fields and retains eigenvector and eigenvalue graphs that represent the topology of asymmetric tensor fields. TFZ scans through each cell, preserving the local topology of each cell, and thereby ensuring certain global topological guarantees. We showcase the effectiveness of our framework in enhancing the lossy scientific data compressors SZ3 and SPERR.","accessible_pdf":null,"authors":[{"affiliation":"University of Utah","email":"u1472279@utah.edu","name":"Nathaniel Gorski"},{"affiliation":"University of Kentucky","email":"xliang@uky.edu","name":"Xin Liang"},{"affiliation":"The Ohio State University","email":"guo.2154@osu.edu","name":"Hanqi Guo"},{"affiliation":"University of Utah","email":"wang.bei@gmail.com","name":"Bei Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"84ab0ade-9529-4942-b5e1-ca953e56c880","image_caption":"The main practitioners of interest would be anybody who works with large amounts of tensor field data and employs topological applications. They would learn techniques for storing their data efficiently without significantly hindering topological use cases.","keywords":["Lossy compression","tensor fields","topology preservation","topological data analysis","topology in visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1929-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkyOS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQ2LCJleHAiOjE3OTI0ODA5NDZ9.fYJBfbEGEc5BiszOmrZH_2VraKk6dKNk-SSHcT10Xcs","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1929","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"84ab0ade-9529-4942-b5e1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3b5e9042-feed-4d23-814b-7242a997aea1","abstract":"Large language models (LLMs) are increasingly used to support data analysis and visualization tasks, but remain prone to hallucinations. Recent work suggests that multi-agent systems (MAS) can mitigate hallucinations by enabling internal validation and cross-verification. However, learning effective MAS coordination strategies to mitigate hallucination remains challenging, particularly for newcomers, due to the wide range of coordination strategies and the lack of interactive, hands-on learning tools. To address this, we present The Agentopia Times, an educational game that teaches hallucination mitigation through active experimentation with MAS coordination strategies. The Agentopia Times simulates a newsroom where LLM agents collaborate to create data-driven narratives, with users tasked with adjusting communication protocols to manage hallucinated content. The game features a structured mapping between MAS coordination and familiar gameplay mechanics, providing immediate feedback on hallucination outcomes. Through use cases and preliminary user feedback, we demonstrate how The Agentopia Times enables users to explore and mitigate hallucination in MAS.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota, Twin Cities","email":"lu000661@umn.edu","name":"Yilin Lu"},{"affiliation":"University of Minnesota","email":"du000288@umn.edu","name":"Shurui Du"},{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3b5e9042-feed-4d23-814b-7242a997aea1","image_caption":"Target Practitioners:\nThis work will interest AI educators, data science instructors, simulation scientists, LLM application developers, and visualization researchers who are involved in the development or teaching of multi-agent AI systems, human-AI interaction, or explainable AI.\n\nPractical Impact:\nPractitioners can apply the game-based learning framework introduced in THE AGENTOPIA TIMES to teach complex coordination strategies and how those strategies contribute to hallucination mitigation in multi-agent LLM systems through interactive experimentation. The system provides an intuitive platform to explore how hallucinations propagate and can be mitigated through architectural design, which can benefit instructional settings, training modules, and interactive demos for responsible AI practices. The open-source platform also allows for adaptation to other domains requiring an understanding of AI failure modes and system transparency.","keywords":["LLM","visualization generation","educational game","LLM hallucination","Multi-Agent"],"open_access_supplemental_link":"https://github.com/Visual-Intelligence-UMN/The-Agentopia-Times","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1222-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMjItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.zBmaWZa-gGhwfXB-2lz_3zRZ5TAFlsYT6_m9tXcsryc","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1222","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3b5e9042-feed-4d23-814b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"The Agentopia Times: Understanding and Mitigating Hallucinations in Multi-Agent LLM Systems via Data Journalism Gameplay","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ca2928b5-c272-464f-b52b-a3c0a4f7bc80","abstract":"The Fire We Share proposes a care-centered, consequence-aware visualization framework that engages wildfire data not as static metrics, but as living archives of ecological and social entanglement. By combining plant-inspired forms, event-based mapping, and layered storytelling, the project foregrounds fire as a shared temporal condition that cuts across natural cycles, human voices, and policy contexts. Through ritualized interaction with pinecone metaphors and fractured tree rings, participants encounter wildfire not only as data, but as memory, testimony, and ethical relation. Rather than simplifying information into digestible visuals, The Fire We Share reimagines wildfire as a textured, wounded archive\u2014embodied, relational, and radically ethical.","accessible_pdf":null,"authors":[{"affiliation":"California State University","email":"chenwangdesign@gmail.com","name":"chen wang"},{"affiliation":"California State University","email":"mengtanlin@csu.fullerton.edu","name":"Mengtan Lin"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ca2928b5-c272-464f-b52b-a3c0a4f7bc80","image_caption":null,"keywords":["wildfire visualization; ecological memory;\nritual interface; data as grief and care;\nalgorithmic storytelling; broken symmetry;\nrelational design"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1125-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzExMjUtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg2MSwiZXhwIjoxNzkyNDc1ODYxfQ.E2_u4UaOxPXiSTujrTMp5hzWqSpozsm7aO6dRlwUqwE","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1125","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ca2928b5-c272-464f-b52b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"The Fire We Share","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"093872c4-bec7-4c27-bb8e-c89465a4381b","abstract":"Color is a powerful tool in data visualization, but for individuals with color vision deficiencies (CVD), hue can become a barrier rather than an aid. In this paper, we examine how real-world visualizations are perceived across vision profiles through three complementary studies. Study 1 assessed how normal vision participants rated 46 visualizations shown in original and simulated red/green colorblind versions. Study 2 collected matched responses from participants with diagnosed CVD. Study 3 involved in-depth interviews exploring how users interpret, adapt to, and evaluate inaccessible designs. Across studies, we find that simulations capture directional perceptual shifts but fail to reflect the interpretive breakdowns and emotional work described by real CVD users. Factor analysis reveals two dominant perceptual dimensions: functional utility and affective experience. While normal vision participants prioritize functional clarity, CVD users rely more on structural cues and emotional resonance, particularly when color is unreliable. Qualitative insights show that perceptual breakdowns occur not only in high-interference charts but also when redundant encoding or layout scaffolding is missing. We synthesize these findings and offer empirically grounded design recommendations to guide inclusive visualization practices. Our results argue that accessibility must go beyond color correction, embracing structural clarity, redundancy, and real-user validation to ensure inclusive visual communication.","accessible_pdf":null,"authors":[{"affiliation":"Arizona State University","email":"zjian115@asu.edu","name":"Zhuojun Jiang"},{"affiliation":"Northeastern University","email":"aarunku5@asu.edu","name":"Anjana Arunkumar"},{"affiliation":"Arizona State University","email":"cbryan16@asu.edu","name":"Chris Bryan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"093872c4-bec7-4c27-bb8e-c89465a4381b","image_caption":"This paper would be of interest to data visualization designers, UX researchers, and data scientists. The proposed design guidelines can help practitioners create more effective and accessible visualizations for communicating complex datasets to diverse audiences.","keywords":["Information Visualization","Perception & Cognition","Colorblindness","Accessibility"],"open_access_supplemental_link":"https://osf.io/ymucn/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1755-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc1NS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.V9hZJVb13hsji2Sm1aDmON8BR7dQmjmBsi0j7O4uoD0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1755","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"093872c4-bec7-4c27-bb8e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"The Hue-Man Factor: An Empirical Evaluation of Visualization Perception and Accessibility Across Color Vision Profiles","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"bf10121c-1a25-4391-b766-450e53b2348e","abstract":"When a reader encounters a word in English, they split the word into smaller orthographic units in the process of recognizing\nits meaning. For example, \u201crough\u201d, when split according to phonemes, is decomposed as r-ou-gh (not as r-o-ugh or r-ough), where\neach group of letters corresponds to a sound. Since there are many ways to segment a group of letters, this constitutes a computational\noperation that has to be solved by the reading brain, many times per minute, in order to achieve the recognition of words in text\nnecessary for reading. In English, the irregular relationships between groups of letters and sounds, and the wide variety of possible\ngroupings make this operation harder than in more regular languages such as Italian. If this segmentation takes a significant amount of\ntime in the process of recognizing a word, it is conceivable that providing segmentation information in the text itself could help the\nreading process by reducing its computational cost. In this paper we explore whether and how different visual interventions from the\nvisualization literature could communicate segmentation information for reading and word recognition. We ran a series of pre-registered\nlexical decision experiments with 192 participants that tested five main types of visual segmentations: outlines, spacing, connections,\nunderlines and color. The evidence indicates that, even with a moderate amount of training, these visual interventions always slow\ndown word identification, but each to a different extent (between 32.7ms\u2014color technique\u2014and 70.7ms\u2014connection technique).\nThese findings are important because they indicate that, at least for typical adult readers with a moderate amount of specific training in\nthese visual interventions, accelerating the lexical decision task is unlikely. Importantly, the results also offer an empirical measurement\nof the cost of a common set of visual manipulations of text, which can be useful for practitioners seeking to visualize alongside or within\ntext without impacting reading performance. Finally, the interaction between typographically encoded information and visual variables\npresented unique patterns that deviate from existing theories, suggesting new directions for future inquiry.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Victoria","email":"matttermuende@gmail.com","name":"Matthew Termuende"},{"affiliation":"Microsoft","email":"kevlar@microsoft.com","name":"Kevin Larson"},{"affiliation":"University of Victoria","email":"nacenta@gmail.com","name":"Miguel Nacenta"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"bf10121c-1a25-4391-b766-450e53b2348e","image_caption":"Visualization designers, reading researchers, and designers of reading enhancements would be interested in reading this paper. Visualization designers and researchers will learn that adding marks near the text or modifying the text itself comes at the cost of slowing down word recognition, and that their design choices affect the extent of the slowdown. The results can also be used to select the types of interventions that are less intrusive. Reading researchers and reading enhancement designers will learn that word  recognition is likely to be impeded rather than enhanced by visually segmenting words into smaller phonological units.","keywords":["Reading","Word Recognition","Text Visualization","Text Interaction","Phonological Cues"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/79Y5S","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1789-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTc4OS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.cCIeKxG8EIJjp6eEwrcD6XGjlAyO2a32RTsz7A_ohyM","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1789","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"bf10121c-1a25-4391-b766","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"The Impact of Visual Segmentation on Lexical Word Recognition","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"79767fe9-582a-4d94-8cd6-2af68466de1d","abstract":"As the volume of scientific literature continues to expand exponentially, traditional research tools struggle to keep pace\u2014often reinforcing disciplinary silos and limiting opportunities for discovery. The Knowledge Cosmos re-imagines research exploration through an interactive, 3D visualization platform that treats science not as a static repository, but as a navigable universe. By spatializing 17 million academic papers based on semantic similarity, the platform enables users to explore the structure of knowledge intuitively, uncover interdisciplinary connections, and identify underexplored intellectual gaps. Drawing on the principles of play, immersion, and serendipity, The Knowledge Cosmos democratizes the bird\u2019s eye view of research and encourages curiosity-driven inquiry across a wide range of users including students, educators, independent thinkers, and lifelong learners. This paper outlines the conceptual foundations, design, and technological infrastructure of the platform, shares insights from preliminary usability testing, and discusses future directions to scale its potential as a catalyst for interdisciplinary exploration and knowledge creation.","accessible_pdf":null,"authors":[{"affiliation":"Independent Academic","email":"amcgail2@gmail.com","name":"Alec McGail"},{"affiliation":"Harvard University","email":"rifaa_tajani@mde.harvard.edu","name":"Rifaa Tajani"},{"affiliation":"Independent Academic","email":"nikitasridhar14@gmail.com","name":"Nikita Sridhar"},{"affiliation":"University of Texas at Austin","email":"stephlijiabao@gmail.com","name":"Jiabao Li"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"79767fe9-582a-4d94-8cd6-2af68466de1d","image_caption":null,"keywords":["Large-scale data visualization","Science and technology visualization","User-centered design and evaluation","Knowledge visualization and concept mapping","Interdisciplinary research","3D and spatial interaction","Visual encoding and design"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1124-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzExMjQtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg2NCwiZXhwIjoxNzkyNDc1ODY0fQ.HK78YQq8B80iC0uK_3X2Fo1sI9MKcstpIuiTcg_-fE0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1124","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"79767fe9-582a-4d94-8cd6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"The Knowledge Cosmos: An Immersive Platform for Interdisciplinary Research Discovery","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1628b4eb-473e-4157-9d19-44990b75efc7","abstract":"Information visualizations are powerful tools that help users quickly identify patterns, trends, and outliers, facilitating informed decision-making.  However, when visualizations incorporate deceptive design elements\u2014such as truncated or inverted axes, unjustified 3D effects, or violations of best practices\u2014they can mislead viewers and distort understanding, spreading misinformation. While some deceptive tactics are obvious, others subtly manipulate perception while maintaining a fa\u00e7ade of legitimacy. As Vision-Language Models (VLMs) are increasingly used to interpret visualizations, especially by non-expert users, it is critical to understand how susceptible these models are to deceptive visual designs. In this study, we conduct an in-depth evaluation of VLMs' ability to interpret misleading visualizations. By analyzing over 16,000  responses from ten different models across eight distinct types of misleading chart designs, we demonstrate that most VLMs are deceived by them. This leads to altered interpretations of charts, despite the underlying data remaining the same. Our findings highlight the need for robust safeguards in VLMs against visual misinformation.","accessible_pdf":null,"authors":[{"affiliation":"York University","email":"ridwanmahbub26@gmail.com","name":"Ridwan Mahbub"},{"affiliation":"York University","email":"saidulis@yorku.ca","name":"Mohammed Saidul Islam"},{"affiliation":"York University","email":"tahmid20@yorku.ca","name":"Md Tahmid Rahman Laskar"},{"affiliation":"York University","email":"mizanurr@yorku.ca","name":"Mizanur Rahman"},{"affiliation":"University of Alberta","email":"mnayeem@ualberta.ca","name":"Mir Tafseer Nayeem"},{"affiliation":"York University","email":"enamulh@yorku.ca","name":"Enamul Hoque"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"1628b4eb-473e-4157-9d19-44990b75efc7","image_caption":"data journalists, journalists, visualization experts, visualization designers","keywords":["Misleading Visualizations","Large Language Models","Vision Language Models","Taxonomy","Evaluation"],"open_access_supplemental_link":"https://github.com/vis-nlp/visDeception","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1327-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMjctZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.LeLhSyEiAlnxdYusYhLJCT1u5mNgebND6NqBFmybxWU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1327","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1628b4eb-473e-4157-9d19","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"a005eefe-c93e-4d8d-9e90-468a635c8887","abstract":"Advancements in accessibility technologies such as low-cost swell form printers or refreshable tactile displays promise to allow blind or low-vision (BLV) people to analyze data by transforming visual representations directly to tactile representations.\nHowever, it is possible that design guidelines derived from experiments on the visual perception system may not be suited for the tactile perception system. We investigate the potential mismatch between familiar visual encodings and tactile perception in an exploratory study into the strategies employed by BLV people to measure common graphical primitives converted to tactile representations. First, we replicate the Cleveland and McGill study on graphical perception using swell form printing with eleven BLV subjects. Then, we present results from a group interview in which we describe the strategies used by our subjects to read four common chart types. While our results suggest that familiar encodings based on visual perception studies can be useful in tactile graphics, our subjects also expressed a desire to use encodings designed explicitly for BLV people.  Based on this study, we identify gaps between the perceptual expectations of common charts and the perceptual tools available in tactile perception. Then, we present a set of guidelines for the design of tactile graphics that accounts for these gaps. Supplemental material is available at https://osf.io/3nsfp/?view_only=7b7b8dcbae1d4c9a8bb4325053d13d9f","accessible_pdf":"Accessible","authors":[{"affiliation":"Brandeis University","email":"areenkh@brandeis.edu","name":"Areen Khalaila"},{"affiliation":"Worcester Polytechnic Institute","email":"ltharrison@wpi.edu","name":"Lane Harrison"},{"affiliation":"Boston College","email":"nam.wook.kim@bc.edu","name":"Nam Wook Kim"},{"affiliation":"Brandeis University","email":"dylancashman@brandeis.edu","name":"Dylan Cashman"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"a005eefe-c93e-4d8d-9e90-468a635c8887","image_caption":"accessibility researchers and designers of accessible interfaces","keywords":["Accessibility","Tactile Graphics","Graphical Perception"],"open_access_supplemental_link":"https://osf.io/3nsfp/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1508-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTUwOC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM0LCJleHAiOjE3OTI0ODA1MzR9.A8wN8Hnpst4TL8qLghFcp4B3ohz1YnO5y8KYjsboQ3A","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1508","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"a005eefe-c93e-4d8d-9e90","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"They Aren't Built For Me: An Exploratory Study of Strategies for Measurement of Graphical Primitives in Tactile Graphics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"fbac208d-6896-420b-8276-ab913bb0e617","abstract":"This artwork presents an interdisciplinary interaction installation that visualizes collective online mourning behavior in China. By focusing on commemorative content posted on Sina Weibo following the deaths of seven prominent Chinese authors, the artwork employs data scraping, natural language processing, and 3D modeling to transform fragmented textual expressions into immersive digital monuments. Through the analysis of word frequencies, topic models, and user engagement metrics, the system constructs a semantic-visual landscape that reflects both authorial legacies and collective memory. This research contributes to the fields of digital humanities, visualization design, and digital memorial architecture by proposing a novel approach for preserving and reactivating collective memory in the digital age.","accessible_pdf":null,"authors":[{"affiliation":"Harbin Institute of Technology","email":"lingyupeng6@163.com","name":"Lingyu Peng"},{"affiliation":"Harbin institute of technology (Shenzhen)","email":"2634641504@qq.com","name":"Chang Ge"},{"affiliation":"Harbin Institute of Technology, Shenzhen","email":"582546102@qq.com","name":"Liying Long"},{"affiliation":"Future Design School","email":"li1179327296@163.com","name":"xin Li"},{"affiliation":"Harbin Institute of Technology","email":"1140084087@qq.com","name":"Xiao Hu"},{"affiliation":"Harbin Institute of Technology","email":"867103556@qq.com","name":"Pengda Lu"},{"affiliation":"Harbin Institute of Technology","email":"liqingchuan@hit.edu.cn","name":"Qingchuan Li"},{"affiliation":"Harbin Institute of Technology","email":"wu.jiangyue@outlook.com","name":"Jiangyue Wu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"fbac208d-6896-420b-8276-ab913bb0e617","image_caption":null,"keywords":["Online mourning","Collective memory","Artistic Data Visualization","Text visualization","Interaction installation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1049-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzEwNDktZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1NCwiZXhwIjoxNzkyNDc1ODU0fQ.C0AO9JqpGbJBHhvaFcsN8U8sLcdypBse6_ssl4rmpWk","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1049","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"fbac208d-6896-420b-8276","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Tides of Memory: Digital Echoes of Netizen Remembrance","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"8347663a-7e88-43e2-a557-e7b2179fd4e5","abstract":"Visualizing multiple time series presents fundamental tradeoffs between scalability and visual clarity. Time series capture the behavior of many large-scale real-world processes, from stock market trends to urban activities. Users often gain insights by visualizing them as line charts, juxtaposing or superposing multiple time series to compare them and identify trends and patterns. However, existing representations struggle with scalability: when covering long time spans, leading to visual clutter from too many small multiples or overlapping lines. We propose TiVy, a new algorithm that summarizes time series using sequential patterns. It transforms the series into a set of symbolic sequences based on subsequence visual similarity using Dynamic Time Warping (DTW), then constructs a disjoint grouping of similar subsequences based on the frequent sequential patterns. The grouping result, a visual summary of time series, provides uncluttered superposition with fewer small multiples. Unlike common clustering techniques, TiVy extracts similar subsequences (of varying lengths) aligned in time. We also present an interactive time series visualization that renders large-scale time series in real-time. Our experimental evaluation shows that our algorithm (1) extracts clear and accurate patterns when visualizing  time series data, (2) achieves a significant speed-up (1000X) compared to a straightforward DTW clustering. We also demonstrate the efficiency of our approach to explore hidden structures in massive time series data in two usage scenarios.","accessible_pdf":null,"authors":[{"affiliation":"Adobe Research","email":"ychan@adobe.com","name":"Gromit Yeuk-Yin Chan"},{"affiliation":"University of Sao Paulo","email":"gnonato@icmc.usp.br","name":"Luis Gustavo Nonato"},{"affiliation":"Paris Descartes University","email":"themis@mi.parisdescartes.fr","name":"Themis Palpanas"},{"affiliation":"New York University","email":"csilva@nyu.edu","name":"Claudio Silva"},{"affiliation":"New York University","email":"juliana.freire@nyu.edu","name":"Juliana Freire"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"8347663a-7e88-43e2-a557-e7b2179fd4e5","image_caption":"time series researchers,\nneuroscientist,\ndata scientist","keywords":["Time Series Visualization","Sub-sequence Clustering"],"open_access_supplemental_link":"https://github.com/GromitC/TiVy","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1930-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkzMC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.d9tLs0WVdP18T-ttWzACmcbTh0Ew8xnYrbI2Mvxw_hU","preprint_link":"https://arxiv.org/pdf/2507.18972","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1930","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"8347663a-7e88-43e2-a557","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"TiVy: Time Series Visual Summary for Scalable Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"2bf2b86a-9987-4815-8c2a-8911b3588071","abstract":"We propose a design space for embedded data visualizations in urban environments, structured across multiple dimensions that articulate key contextual and representational characteristics. The design space is grounded in situated and immersive visualization theory, urban informatics, and co-creative ideation workshops. Its dimensions describe different aspects of how data visualizations relate to the physical urban environment and to the viewer. We illustrate the applicability of the design space using mappings to speculative embedded urban visualizations. This conceptual contribution is intended to support designers and researchers in structuring, analyzing, and generating embedded urban visualizations, and serves as a basis for future extensions.","accessible_pdf":null,"authors":[{"affiliation":"Technische Hochschule Mannheim","email":"t.nagel@hs-mannheim.de","name":"Till Nagel"},{"affiliation":"Technische Hochschule Mannheim","email":"c.huber@hs-mannheim.de","name":"Christoph Huber"},{"affiliation":"Technische Hochschule Mannheim","email":"3000176@stud.hs-mannheim.de","name":"Mona Eder"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2bf2b86a-9987-4815-8c2a-8911b3588071","image_caption":"This paper will be of interest to practitioners working at the intersection of data, space, and design. These audiences range from visualization designers and AR developers to urban planners and spatial analysts. The proposed design space offers a structured framework to help analyze, compare, and develop embedded urban visualizations, supporting more context-aware approaches to communicating data in public environments.","keywords":["Situated Visualization","Urban Data","Framework."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1271-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNzEtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.beUo1JBqvTadxlnjsRKKC27U5joaohiji5_HqeXy29A","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1271","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"2bf2b86a-9987-4815-8c2a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Toward a Design Space for Embedded Urban Data Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"55b60ade-1288-4e41-a5db-4bbff4d517eb","abstract":"Visualization as a discipline often grapples with generalization by reasoning about how study results on the efficacy of a tool in one context might apply to another context. This work offers an account of the logic of generalization in visualization research and argues that it struggles in particular with applications of visualization as a decision aid. We use decision theory to define the dimensions on which decision problems can vary, and we present an analysis of heterogeneity in scenarios where visualization supports decision-making. Our findings identify utility as a focal and under-examined concept in visualization research on decision-making, demonstrating how the visualization community's logic of generalization might benefit from using decision theory as a lens for understanding context variation.","accessible_pdf":null,"authors":[{"affiliation":"University of Chicago","email":"kalea@uchicago.edu","name":"Alex Kale"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"55b60ade-1288-4e41-a5db-4bbff4d517eb","image_caption":"Practitioners who use visualization to support decision-making, especially in complex organizational settings, will find in this work a primer on a helpful conceptual framework, decision theory. Specific benefits of decision theory as a lens for visualization practice include helping practitioners reason about situated design goals with multiple stakeholders and recognizing whether the results of a given empirical study of decision-making with visualization are applicable to their particular situation.","keywords":["Decision theory","visualization","epistemology."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1198-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExOTgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ._oNDEbL7BZePe_uOfrFLzp54_b3jKEQX2f7x5s18Py0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1198","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"55b60ade-1288-4e41-a5db","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Toward a Logic of Generalization about Visualization as a Decision Aid","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b3c081c3-338e-4dd1-8d57-3a35f981facc","abstract":"Traditional instance-based model analysis focuses mainly on misclassified instances. However, this approach overlooks the varying difficulty associated with different instances. Ideally, a robust model should recognize and reflect the challenges presented by intrinsically difficult instances. It is also valuable to investigate whether the difficulty perceived by the model aligns with that perceived by humans. To address this, we propose incorporating instance difficulty into the deep neural network evaluation process, specifically for supervised classification tasks on image data. Specifically, we consider difficulty measures from three perspectives -- data, model, and human -- to facilitate comprehensive evaluation and comparison. Additionally, we develop an interactive visual tool, DifficultyEyes, to support the identification of instances of interest based on various difficulty patterns and to aid in analyzing potential data or model issues. Case studies demonstrate the effectiveness of our approach.","accessible_pdf":null,"authors":[{"affiliation":"Eindhoven University of Technology","email":"l.meng1@tue.nl","name":"Linhao Meng"},{"affiliation":"Eindhoven University of Technology","email":"s.j.v.d.elzen@tue.nl","name":"Stef van den Elzen"},{"affiliation":"Eindhoven University of Technology","email":"a.vilanova@tue.nl","name":"Anna Vilanova"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b3c081c3-338e-4dd1-8d57-3a35f981facc","image_caption":"This paper will interest data scientists, ML practitioners, and visual analytics researchers involved in model development and evaluation. Practitioners can apply the proposed methods to better understand model behavior, identify difficult cases, and improve decision-making through human-in-the-loop analysis.","keywords":["Visualization","deep neural network","difficulty"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1194-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExOTQtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.12gbtAwKJxQRdmGHMQQs5BSrpMrzYa-Ws8MI7RW9wDk","preprint_link":"https://doi.org/10.48550/arXiv.2507.00881","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1194","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b3c081c3-338e-4dd1-8d57","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Towards Difficulty-Aware Analysis of Deep Neural Networks","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"513ae265-0ca2-4c62-b8ee-70676f0b815d","abstract":"Decision-making is a central yet under-defined goal in visualization research. While existing task models address decision processes, they often neglect the conditions framing a decision. To better support decision-making tasks, we propose a characterization scheme that describes decision problems through key properties of the data, users, and task context. This scheme helps visualization researchers specify decision-support claims more precisely and informs the design of appropriate visual encodings and interactions. We demonstrate the utility of our approach by applying it to characterize decision tasks targeted by existing design studies, highlighting opportunities for future research in decision-centric visualization.","accessible_pdf":null,"authors":[{"affiliation":"University of Rostock","email":"lena.cibulski@uni-rostock.de","name":"Lena Cibulski"},{"affiliation":"University of Rostock","email":"stefan.bruckner@gmail.com","name":"Stefan Bruckner"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"513ae265-0ca2-4c62-b8ee-70676f0b815d","image_caption":"Developing a systematic understanding of real-world decisions in a data-rich environment is of significant value for practitioners who make data-based decisions in various application fields, like simulation scientists, medical experts, financial specialists, engineers, and many more. They can use the variables proposed in our framework to describe key properties of their decisions as a starting point for in-depth data analysis and consultation with project partners or consultants.","keywords":["Decision-centric visualization","task characterization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1189-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExODktZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOSwiZXhwIjoxNzkyNDgxOTM5fQ.GEu50kpvpzqIZBAgvou00nFR8qIx_Iwi9rLELbLLKP4","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1189","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"513ae265-0ca2-4c62-b8ee","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Towards Understanding Decision Problems as a Goal of Visualization Design","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9d6c7874-e4fa-449b-88aa-4b3937241e7d","abstract":"Constructing cell developmental trajectories is a critical task in single-cell RNA sequencing (scRNA-seq) analysis, enabling the inference of potential cellular progression paths. However, current automated methods are limited to establishing cell developmental trajectories within individual samples, necessitating biologists to manually link cells across samples to construct complete cross-sample evolutionary trajectories that consider cellular spatial dynamics. This process demands substantial human effort due to the complex spatial correspondence between each pair of samples.\nTo address this challenge, we first proposed a GNN-based model to predict cross-sample cell developmental trajectories. We then developed TrajLens, a visual analytics system that supports biologists in exploring and refining the cell developmental trajectories based on predicted links. Specifically, we designed the visualization that integrates features on cell distribution and developmental direction across multiple samples, providing an overview of the spatial evolutionary patterns of cell populations along trajectories. Additionally, we included contour maps superimposed on the original cell distribution data, enabling biologists to explore them intuitively. To demonstrate our system's performance, we conducted quantitative evaluations of our model with two case studies and expert interviews to validate its usefulness and effectiveness.","accessible_pdf":"Accessible","authors":[{"affiliation":"Sichuan University","email":"wangqipengscu@stu.scu.edu.cn","name":"Qipeng Wang"},{"affiliation":"Singapore Management University","email":"haywardryan@foxmail.com","name":"Shaolun Ruan"},{"affiliation":"The Hong Kong University of Science and Technology","email":"rshengac@connect.ust.hk","name":"Rui Sheng"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"},{"affiliation":"Sichuan University","email":"zhumin@scu.edu.cn","name":"Min Zhu"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9d6c7874-e4fa-449b-88aa-4b3937241e7d","image_caption":"Biologists interested in bioinformatics","keywords":["Visual Analytics","Single-cell RNA Sequencing","Cell Developmental Trajectories"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/ETFJ2","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1247-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI0Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.9iRGag6Yqhwayc6GprCKUN-T9kpYqvse_UPxIZdhrHQ","preprint_link":"http://arxiv.org/abs/2507.15620","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1247","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9d6c7874-e4fa-449b-88aa","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"TrajLens: Visual Analysis for Constructing Cell Developmental Trajectories in Cross-Sample Exploration","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3f0a5ad9-2440-47b4-a741-f6b92c58bf8d","abstract":"Eligibility criteria play a critical role in clinical trials by determining the target patient population, which significantly influences the outcomes of medical interventions. However, current approaches for designing eligibility criteria have limitations to support interactive exploration of the large space of eligibility criteria. They also ignore incorporating detailed characteristics from the original electronic health record (EHR) data for criteria refinement. To address these limitations, we proposed TrialCompass, a visual analytics system integrating a novel workflow, which can empower clinicians to iteratively explore the vast space of eligibility criteria through knowledge-driven and outcome-driven approaches. TrialCompass supports history-tracking to help clinicians trace the evolution of their adjustments and decisions when exploring various forms of data (i.e., eligibility criteria, outcome metrics, and detailed characteristics of original EHR data) through these two approaches. This feature can help clinicians comprehend the impact of eligibility criteria on outcome metrics and patient characteristics, which facilitates systematic refinement of eligibility criteria. Using a real-world dataset, we demonstrated the effectiveness of TrialCompass in providing insights into designing eligibility criteria for septic shock and sepsis-associated acute kidney injury. We also discussed the research prospects of applying visual analytics to clinical trials.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology","email":"rshengac@connect.ust.hk","name":"Rui Sheng"},{"affiliation":"Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI)","email":"wangxbzb@gmail.com","name":"Xingbo Wang"},{"affiliation":"Zhejiang University","email":"wangjiachen@zju.edu.cn","name":"Jiachen Wang"},{"affiliation":"The Hong Kong University of Science and Technology","email":"suffvier@gmail.com","name":"Xiaofu Jin"},{"affiliation":"The Hong Kong University of Science and Technology","email":"szh@connect.ust.hk","name":"Zhonghua SHENG"},{"affiliation":"Weill Cornell Medical College","email":"zhx2005@med.cornell.edu","name":"Zhenxing Xu"},{"affiliation":"Weill Cornell Medical College","email":"sur4002@med.cornell.edu","name":"Suraj Rajendran"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"},{"affiliation":"Weill Cornell Medicine","email":"few2001@med.cornell.edu","name":"Fei Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3f0a5ad9-2440-47b4-a741-f6b92c58bf8d","image_caption":"Clinicians and clinical researchers, HCI Researchers, Visualization Researchers","keywords":["Visual Analytics","Healthcare","Clinical Trials","Decision Making","Electronic Health Record (EHR)"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1291-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI5MS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.M8s7_u2IK_2vA9QF7LV2C3cieOtZRWA0NHbhKhiGQDo","preprint_link":"https://arxiv.org/pdf/2507.12298","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1291","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3f0a5ad9-2440-47b4-a741","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"565a0b31-3df7-4d49-859a-2a96dda32a65","abstract":"The analysis of 3D symmetric second-order tensor fields often relies on topological features such as degenerate tensor lines, neutral surfaces, and their generalization to mode surfaces, which reveal important structural insights into the data. However, uncertainty in such fields is typically visualized using derived scalar attributes or tensor glyph representations, which often fail to capture the global behavior. Recent advances have introduced uncertain topological features for tensor field ensembles by focusing on degenerate tensor locations. Yet, mode surfaces, including neutral surfaces and arbitrary mode surfaces are essential to a comprehensive understanding of tensor field topology. In this work, we present a generalization of uncertain degenerate tensor features to uncertain mode surfaces of arbitrary mode values, encompassing uncertain degenerate tensor lines as a special case. Our approach supports both surface and line geometries, forming a unified framework for analyzing uncertain mode-based topological features in tensor field ensembles. We demonstrate the effectiveness of our method on several real-world simulation datasets from engineering and materials science.","accessible_pdf":"Accessible","authors":[{"affiliation":"RWTH Aachen University","email":"gerrits@vis.rwth-aachen.de","name":"Tim Gerrits"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"565a0b31-3df7-4d49-859a-2a96dda32a65","image_caption":"Practitioners working with symmetric second-order tensor data, e.g., simulation scientists and medical personell, are provided with a novel visualization tool for the analysis of uncertain topological features in tensor field ensembles.","keywords":["Second-Order Tensors","Symmetric Tensors","Tensor Topology","Tensor Mode","Uncertainty"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1108-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMDgtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.KSqPMwwk_cP6JQxRIISJLT5_RkfMypfcMzTj2NZVut4","preprint_link":"https://arxiv.org/pdf/2506.23406","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1108","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"565a0b31-3df7-4d49-859a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Uncertain Mode Surfaces in 3D Symmetric Second-Order Tensor Field Ensembles","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"367cd3fe-b8ff-4bee-bd9f-ae8e51feeb9e","abstract":"Principal Component Analysis (PCA) is perhaps the most popular linear projection technique for dimensionality reduction. We consider PCA under the assumption that the high-dimensional data points are equipped with Gaussian uncertainty. Several approaches to such uncertainty-aware PCA have been developed recently in the visualization community. Since PCA is a discontinuous map, a small uncertainty in the data points can result in a huge uncertainty in the projected points. We show that the uncertainty of the data points also creates uncertainty in the eigenvectors of the covariance matrix that defines the PCA projection. We present a closed-form expression to quantify eigenvector uncertainty. Based on this, we propose a 3D glyph that supports the decision whether existing solutions for uncertainty-aware PCA are sufficient, or whether a more expensive sampling-based approach is required. We apply our approach to several test data sets.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Magdeburg","email":"lukas.friesecke@ovgu.de","name":"Lukas Friesecke"},{"affiliation":"University of Magdeburg","email":"christian.braune@ovgu.de","name":"Christian Braune"},{"affiliation":"University of Magdeburg","email":"roessl@isg.cs.uni-magdeburg.de","name":"Christian Roessl"},{"affiliation":"University of Magdeburg","email":"theisel@ovgu.de","name":"Holger Theisel"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"367cd3fe-b8ff-4bee-bd9f-ae8e51feeb9e","image_caption":"Data Scientists - We propose a 3D glyph hat supports the decision whether existing solutions for uncertainty-aware PCA are sufficient, or whether a more expensive sampling-based approach is required. Data Scientists could use our published source code, to check their own data and make an informed decision based on the results.","keywords":["PCA","dimensionality reduction","uncertainty visualization"],"open_access_supplemental_link":"https://github.com/lfriesecke/uncertainty-aware-pca-revisited","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1838-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTgzOC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.01u2Umdkxw7JNk8WeHFbkfJEpkiN-895MM3jV8RO9Sk","preprint_link":"https://vc.cs.ovgu.de/assets/publications/2025/Friesecke_2025_VIS.pdf","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1838","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"367cd3fe-b8ff-4bee-bd9f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Uncertainty-Aware PCA Revisited","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"7b762da7-ec92-461b-ab0a-95e7e278234d","abstract":"Aortic dissection is a life-threatening cardiovascular disease characterized by blood entering the media layer of the aortic vessel wall. This creates a second flow channel, known as the false lumen, which weakens the aortic wall and can potentially lead to fatal aortic rupture. Current risk stratification of aortic dissections is primarily based on morphological features of the aorta. However, hemodynamics also play a significant role in disease progression, though their investigation and visualization remain challenging. Common flow visualizations often experience visual clutter, especially when dealing with the intricate morphologies of aortic dissections. In this work, we implement and evaluate different approaches to visualizing the flow in aortic dissections effectively. We employ three techniques, namely streaklines with depth-dependent halos, transparent streaklines, and smoke surfaces. The latter is a technique based on streak surfaces, enhanced with opacity modulations, to produce a smoke-like appearance that improves visual clarity. We adapt the original opacity modulation of smoke surfaces to visualize flow even within the complex geometries of aortic dissections, thereby enhancing visual fidelity. To effectively capture dissection hemodynamics, we developed customized seeding structures that adapt to the shape of the surrounding lumen. Our evaluation, conducted via an online questionnaire, included medical professionals, fluid simulation experts, and visualization specialists. By analyzing results across these groups, we highlight differences in preference and interpretability, offering insight into domain-specific needs. No single visualization technique emerged as the best overall. Smoke surfaces provide the best overall clarity and visual realism. However, participants found streaklines with halos to be the best for quantifying flow, dispite them introducing significant visual clutter. Transparent streaklines serve as a middle ground, offering improved clarity over halos while maintaining some level of detail. Across all participant groups, smoke surfaces were rated as the most visually appealing and lifelike, with medical professionals highlighting their resemblance to contrast-agent injections used in clinical practice.","accessible_pdf":null,"authors":[{"affiliation":"Otto-von-Guericke-University","email":"aar.schr@gmail.com","name":"Aaron Schroeder"},{"affiliation":"Otto-von-Guericke University","email":"kai.ostendorf@st.ovgu.de","name":"Kai Ostendorf"},{"affiliation":"Stanford University School of Medicine","email":"baeumler@stanford.edu","name":"Kathrin Baeumler"},{"affiliation":"Stanford University","email":"mastro@stanford.edu","name":"Domenico Mastrodicasa"},{"affiliation":"Stanford University School of Medicine","email":"d.fleischmann@stanford.edu","name":"Dominik Fleischmann"},{"affiliation":"University of Magdeburg","email":"bernhard@isg.cs.uni-magdeburg.de","name":"Bernhard Preim"},{"affiliation":"University of Magdeburg","email":"theisel@ovgu.de","name":"Holger Theisel"},{"affiliation":"Stanford University School of Medicine","email":"gmistelb@stanford.edu","name":"Gabriel Mistelbauer"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"7b762da7-ec92-461b-ab0a-95e7e278234d","image_caption":"Radiologists and vascular surgeons could discover new possibilities for the visualization of the data that is already available to them and develop new diagnostic and treatment planning strategies. Fluid simulation and flow visualization scientists could discover new ways to validate their simulation results and develop further visualization techniques respectively.","keywords":["Medical Visualization","Flow Visualization","Hemodynamics","Aortic Dissection."],"open_access_supplemental_link":"https://github.com/aaschr/vis_2025_smoke_surfaces_for_AD","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1426-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQyNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.n04tE_duG-TebUw9-7_nNJdM4hQRtLKEtBefAmCoRtg","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1426","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"7b762da7-ec92-461b-ab0a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Understanding Aortic Dissection Hemodynamics: Evaluating Adapted Smoke Surfaces Against Streakline-Based Techniques","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"7a68a26e-4b20-444f-a76a-eee7a08ab3be","abstract":"Understanding the behavior of large language models (LLMs) is crucial for ensuring their safe and reliable use. However, existing explainable AI (XAI) methods for LLMs primarily rely on word-level explanations, which are often computationally inefficient and misaligned with human reasoning processes. Moreover, these methods often treat explanation as a one-time output, overlooking its inherently interactive and iterative nature. In this paper, we present LLM Analyzer, an interactive visualization system that addresses these limitations by enabling intuitive and efficient exploration of LLM behaviors through counterfactual analysis. Our system features a novel algorithm that generates fluent and semantically meaningful counterfactuals via targeted removal and replacement operations at user-defined levels of granularity. These counterfactuals are used to compute feature attribution scores, which are then integrated with concrete examples in a table-based visualization, supporting dynamic analysis of model behavior. A user study with LLM practitioners and interviews with experts demonstrate the system\u2019s usability and effectiveness, emphasizing the importance of involving humans in the explanation process as active participants rather than passive recipients.","accessible_pdf":"Accessible","authors":[{"affiliation":"ETH Z\u00fcrich","email":"furui.cheng@inf.ethz.ch","name":"Furui Cheng"},{"affiliation":"ETH Zurich","email":"vzouhar@inf.ethz.ch","name":"Vil\u00e9m Zouhar"},{"affiliation":"ETH Z\u00fcrich","email":"robin.chan@inf.ethz.ch","name":"Robin Chan"},{"affiliation":"University of Konstanz","email":"daniel.fuerst@uni-konstanz.de","name":"Daniel F\u00fcrst"},{"affiliation":"IBM Research AI","email":"hendrik@strobelt.com","name":"Hendrik Strobelt"},{"affiliation":"ETH Z\u00fcrich","email":"melassady@ai.ethz.ch","name":"Mennatallah El-Assady"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"7a68a26e-4b20-444f-a76a-eee7a08ab3be","image_caption":"large language model (application) researchers and developers; \nresearchers in explainable AI and human-AI interaction","keywords":["Counterfactual","Explainable Artificial Intelligence","Large Language Model","Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":"The paper contributes an interactive visualization system for analyzing large language model behaviors together with an algorithm for meaning textual counterfactual generation.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1701-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTcwMS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM4LCJleHAiOjE3OTI0ODA5Mzh9.rf9LusEe57KY5mJV8uAa1_Azz-sQfLE2UhXKG3p4rgg","preprint_link":"https://arxiv.org/abs/2405.00708","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1701","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"7a68a26e-4b20-444f-a76a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"8a0db17e-c752-4f4d-a713-082d59c2d1f7","abstract":"This paper investigates why recent generative AI models outperform humans in data visualization knowledge tasks. Through systematic comparative analysis of responses to visualization questions, we find that differences exist between two ChatGPT models and human outputs over rhetorical structure, knowledge breadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more advanced model, displays a hybrid of characteristics from both humans and ChatGPT-3.5. The two models were generally favored over human responses, while their strengths in coverage and breadth, and emphasis on technical and task-oriented visualization feedback collectively shaped higher overall quality. Based on our findings, we draw implications for advancing user experiences based on the potential of LLMs and human perception over their capabilities, with relevance to broader applications of AI.","accessible_pdf":"Accessible","authors":[{"affiliation":"Boston College","email":"yongsu.ahn@pitt.edu","name":"Yongsu Ahn"},{"affiliation":"Boston College","email":"nam.wook.kim@bc.edu","name":"Nam Wook Kim"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"8a0db17e-c752-4f4d-a713-082d59c2d1f7","image_caption":"visualization practitioners, developers\nUI practitioners, developers\nAI and data scientists","keywords":["Generative AI","LLM","Visualization","Question and answering","ChatGPT"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1360-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNjAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.CItm4np-ohryy6bxu8zULjt9tYO9iC4qef_Vztazmdw","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1360","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"8a0db17e-c752-4f4d-a713","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"19140b17-2652-4f77-a749-4088ec870052","abstract":"When designed deliberately, data visualizations can become powerful persuasive tools, influencing viewers' opinions, values, and actions. While researchers have begun studying this issue (e.g., to evaluate the effects of persuasive visualization), we argue that a fundamental mechanism of persuasion resides in rhetorical construction, a perspective inadequately addressed in current visualization research. To fill this gap, we present a focused analysis of octopus maps, a visual genre that has maintained persuasive power across centuries and achieved significant social impact. Employing rhetorical schema theory, we collected and analyzed 90 octopus maps spanning from the 19th century to contemporary times. We closely examined how octopus maps implement their persuasive intents and constructed a design space that reveals how visual metaphors are strategically constructed and what common rhetorical strategies are applied to components such as maps, octopus imagery, and text. Through the above analysis, we also uncover a set of interesting findings. For instance, contrary to the common perception that octopus maps are primarily a historical phenomenon, our research shows that they remain a lively design convention in today\u2019s digital age. Additionally, while most octopus maps stem from Western discourse that views the octopus as an evil symbol, some designs offer alternative interpretations, highlighting the dynamic nature of rhetoric across different sociocultural settings. Lastly, drawing from the lessons provided by octopus maps, we discuss the associated ethical concerns of persuasive visualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"Iowa State University","email":"daocheng@iastate.edu","name":"Daocheng Lin"},{"affiliation":"Fudan University","email":"wangyifanlea@gmail.com","name":"Yifan Wang"},{"affiliation":"Shanghai Jiao Tong University","email":"flora20@sjtu.edu.cn","name":"Yutong Yang"},{"affiliation":"Fudan University","email":"xingyulan96@gmail.com","name":"Xingyu Lan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"19140b17-2652-4f77-a749-4088ec870052","image_caption":"We proposed an analysis of the visual rhetoric on the octopus maps, which is a famous persuasive visualization in history, offering insights for practitioners such as data journalists, visual designers, government communicators, and educators. While octopus maps depict certain countries or regions in a highly political and ideological manner, which raises ethical concerns about persuasive visualization, we would like to emphasize that the primary goal of this research is not to assist designers in creating or reproducing propaganda maps. In particular, we do not advocate that readers of this work employ such design techniques for malicious purposes or to spread hatred and misinformation. Instead, we hope that our research, as well as the associated corpus and website materials, can be browsed and learned by the public, helping to enhance their visualization literacy and increase their ability to identify the potential manipulative rhetoric of visualizations. Our summarization of the rhetorical techniques may also serve as a checklist, helping data journalists and designers to become aware of potential emotional overstatement and information distortion in their work. It can also inform visualization educators in developing pedagogy on how to design faithful and trustworthy data visualizations.","keywords":["Persuasive Visualization","Map","Visual Rhetoric","Visualization Rhetoric","Metaphorical Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1249-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI0OS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.6U22K2ZtJSEsfeeqDuoAA3z00K3FgmXLzKs1NIKhi1c","preprint_link":"https://arxiv.org/abs/2507.11903","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1249","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"19140b17-2652-4f77-a749","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f8717d69-b5a1-497d-9241-6b1931d4c195","abstract":"With the growing availability of urban data and the increasing complexity of societal challenges, visual analytics has become essential for deriving insights into pressing real-world problems. However, analyzing such data is inherently complex and iterative, requiring expertise across multiple domains. The need to manage diverse datasets, distill intricate workflows, and integrate various analytical methods presents a high barrier to entry, especially for researchers and urban experts who lack proficiency in data management, machine learning, and visualization. Advancements in large language models offer a promising solution to lower the barriers to the construction of analytics systems by enabling users to specify intent rather than define precise computational operations. However, this shift from explicit operations to intent-based interaction introduces challenges in ensuring alignment throughout the design and development process. Without proper mechanisms, gaps can emerge between user intent, system behavior, and analytical outcomes. To address these challenges, we propose Urbanite, a framework for human-AI collaboration in urban visual analytics. Urbanite leverages a dataflow-based model that allows users to specify intent at multiple scopes, enabling interactive alignment across the specification, process, and evaluation stages of urban analytics. Based on findings from a survey to uncover challenges, Urbanite incorporates features to facilitate explainability, multi-resolution definition of tasks across dataflows, nodes, and parameters, while supporting the provenance of interactions. We demonstrate Urbanite's effectiveness through usage scenarios created in collaboration with urban experts. Urbanite is available at https://urbantk.org/urbanite.","accessible_pdf":null,"authors":[{"affiliation":"University of Illinois","email":"gmorei3@uic.edu","name":"Gustavo Moreira"},{"affiliation":"University of Illinois Chicago","email":"lferr10@uic.edu","name":"Leonardo Ferreira"},{"affiliation":"University of Illinois","email":"carolvfs@illinois.edu","name":"Carolina Veiga"},{"affiliation":"University of California, Berkeley","email":"maryamh@mit.edu","name":"Maryam Hosseini"},{"affiliation":"University of Illinois Chicago","email":"fabiom@uic.edu","name":"Fabio Miranda"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f8717d69-b5a1-497d-9241-6b1931d4c195","image_caption":"Urbanite is a dataflow framework that helps urban planners, architects, and climate scientists create transparent, AI-assisted workflows for the analysis of complex urban datasets.","keywords":["Urban analytics","urban data","dataflow","large language models","visualization framework","visualization system."],"open_access_supplemental_link":"http://urbantk.org/urbanite","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1938-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkzOC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.sdcz8kojO2cf73tDXdC9X9i8bdCZDobv3GYqrMXbpmk","preprint_link":"https://arxiv.org/abs/2508.07390","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1938","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f8717d69-b5a1-497d-9241","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"055a075f-b15e-43ec-becb-cc09ad944d2c","abstract":"We investigate whether tactile charts support comprehension and learning of complex visualizations for blind and low-vision (BLV) individuals and contribute four tactile chart designs and an interview study. Visualizations are powerful tools for conveying data, yet BLV individuals typically can rely only on assistive technologies\u2014primarily alternative texts\u2014to access this information. Prior research shows the importance of mental models of chart types for interpreting these descriptions, yet BLV individuals have no means to build such a mental model based on images of visualizations. Tactile charts show promise to fill this gap in supporting the process of building mental models. Yet studies on tactile data representations mostly focus on simple chart types, and it is unclear whether they are also appropriate for more complex charts as would be found in scientific publications. Working with two BLV researchers, we designed 3D-printed tactile template charts with exploration instructions for four advanced chart types: UpSet plots, violin plots, clustered heatmaps, and faceted line charts. We then conducted an interview study with 12 BLV participants comparing whether using our tactile templates improves mental models and understanding of charts and whether this understanding translates to novel datasets experienced through alt texts. Thematic analysis shows that tactile models support chart type understanding and are the preferred learning method by BLV individuals. We also report participants' opinions on tactile chart design and their role in BLV education.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Utah","email":"hetingying.hty@gmail.com","name":"Tingying He"},{"affiliation":"University of Utah","email":"maggie.mccracken@psych.utah.edu","name":"Maggie McCracken"},{"affiliation":"University College London","email":"d.hajas@ucl.ac.uk","name":"Daniel Hajas"},{"affiliation":"University of Utah","email":"sarah.creem@psych.utah.edu","name":"Sarah Creem-Regehr"},{"affiliation":"University of Utah","email":"alexander.lex@gmail.com","name":"Alexander Lex"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"055a075f-b15e-43ec-becb-cc09ad944d2c","image_caption":"Practitioners\n- Accessibility experts\n- Educators of blind and low-vision individuals\n- Inclusive design practitioners\n\nApplications:\n- Use our tactile chart designs to teach complex visualizations to BLV learners\n- Support development of instructional materials for BLV individuals\n- Inform inclusive design of data tools and alt text strategies","keywords":["Accessibility","tactile representations"],"open_access_supplemental_link":"https://osf.io/9dwgq/","open_access_supplemental_question":"We provide an extensive, well-documented 40-page appendix that includes materials and detailed descriptions of the full design and user study processes.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1514-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTUxNC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM0LCJleHAiOjE3OTI0ODA1MzR9.aSvthK_PPPti0AIDaUneLQ5TiPEpBAAehhWpc4LMq2U","preprint_link":"https://arxiv.org/abs/2507.21462","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1514","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"055a075f-b15e-43ec-becb","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f2b6c739-7eed-4161-a955-230c614bae12","abstract":"Designing and building visual analytics (VA) systems is a complex, iterative process that requires the seamless integration of data processing, analytics capabilities, and visualization techniques. While prior research has extensively examined the social and collaborative aspects of VA system authoring, the practical challenges of developing these systems remain underexplored. As a result, despite the growing number of VA systems, there are only a few structured knowledge bases to guide their design and development. To tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that systematically reviews and categorizes the fundamental building blocks of urban VA systems, a domain particularly rich and representative due to its intricate data and unique problem sets. Applying this methodology to an initial set of 20 systems, we identify and organize their core components into a multi-level structure, forming an initial knowledge base with a structured blueprint for VA system development. To scale this effort, we leverage a large language model to automate the extraction of these components for other 81 papers (completing a corpus of 101 papers), assessing its effectiveness in scaling knowledge base construction. We evaluate our method through interviews with experts and a quantitative analysis of annotation metrics. Our contributions provide a deeper understanding of VA systems' composition and establish a practical foundation to support more structured, reproducible, and efficient system development. VA-Blueprint is available at https://urbantk.org/va-blueprint.","accessible_pdf":null,"authors":[{"affiliation":"University of Illinois Chicago","email":"lferr10@uic.edu","name":"Leonardo Ferreira"},{"affiliation":"University of Illinois Chicago","email":"gmorei3@uic.edu","name":"Gustavo Moreira"},{"affiliation":"University of Illinois Chicago","email":"fabiom@uic.edu","name":"Fabio Miranda"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f2b6c739-7eed-4161-a955-230c614bae12","image_caption":"VA-Blueprint offers tool and system builders a structured reference for designing and developing visual analytics solutions, particularly in complex urban domains. By organizing proven building blocks and workflows, it helps practitioners identify essential components, reuse effective patterns, and accelerate the creation of reproducible VA systems.","keywords":["Visual analytics","large language models","knowledge base","system development","urban visual analytics."],"open_access_supplemental_link":"http://urbantk.org/va-blueprint","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/2001-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMjAwMS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQwLCJleHAiOjE3OTI0ODA5NDB9.MuKfDsKo4n0xQKGTACUc-PYTNkxKAN84t8s8ZUmKaFg","preprint_link":"https://arxiv.org/abs/2508.07497","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2001","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f2b6c739-7eed-4161-a955","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"7baf358e-7ab0-44c2-b0c3-dbab651a9268","abstract":"The fields of astronomy and astrophysics are evolving and incorporating technologies to effectively view and explore 3D data visualizations, including Augmented Reality (AR). An analysis of the feasibility of using AR in journal publications for 3D visualizations took place two years ago, in 2023, when Adams et al. evaluated whether the perceived workload between AR and non-AR technologies was comparable. Given that the use of AR for astronomy journal publications was, and still is, in its infancy, the original study had to utilize data intended for K-12 education that had similar interactions and data types as a proxy for real-world data that could be visualized in future astronomy publications. In this paper, we present the results of a conceptual replication study of Adams et al.\u2019s work to validate whether their findings hold with real astronomy stimuli. We found in our replication that many of the trends in the original study hold true, but that the workload experienced by participants was significantly higher under multiple conditions when using real-world data. Additionally, we found that the tradeoff between engagement and workload was as prevalent in the replication as it was in the original study. Our results provide a new framing for researchers to understand the tradeoffs of immersive visualization technologies and the increased workload of pairing these tools with complex, scientific stimuli. All Supplemental Material in our study is available at https://osf.io/j8urq/.","accessible_pdf":"Accessible","authors":[{"affiliation":"Northeastern University","email":"18mcreamer@gmail.com","name":"Mackenzie Creamer"},{"affiliation":"Harvard-Smithsonian Center for Astrophysics","email":"jonathan.carifio@cfa.harvard.edu","name":"Jonathan Carifio"},{"affiliation":"Harvard-Smithsonian Center for Astrophysics","email":"agoodman@cfa.harvard.edu","name":"Alyssa Goodman"},{"affiliation":"Northeastern University","email":"m.borkin@neu.edu","name":"Michelle Borkin"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"7baf358e-7ab0-44c2-b0c3-dbab651a9268","image_caption":"Practitioners who would be most interested in this paper include astronomers and astrophysicists, STEM educators, and scientists working with 3D data. Astronomers can apply our findings to integrate augmented reality (AR) visualizations into journal publications, enabling readers to explore data interactively. STEM educators may use the material to present complex datasets in an engaging, student-paced format. Scientists beyond the fields of astronomy and astrophysics would also use our results to determine whether the increased engagement from AR visualization justifies its complexity for their 3D datasets.","keywords":["Augmented Reality","Human Subjects-Empirical Study","Astronomy Visualization","Replication Study","AR-Enhanced Publications"],"open_access_supplemental_link":"https://osf.io/j8urq/files/osfstorage","open_access_supplemental_question":"Our study is a direct replication, accompanied by extensive supplemental materials that enhance transparency. These include a detailed comparison of methodological differences with the original study, fully documented source code for all analyses, and an additional experiment quantifying the impact of software variance.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1080-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwODAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.i7QF7lAjCN2vHknZnXr2JW3hJCXmoSatx9gI_gYLTKE","preprint_link":"Currently in moderation, was being edited up until July 1, 2025: https://osf.io/preprints/osf/v6sfg_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1080","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"7baf358e-7ab0-44c2-b0c3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Validation through Replication of Augmented Reality as a Visualization Technique for Scholarly Publications in Astronomy","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"dfb477b8-fd21-4346-b7db-27bbbc2b16a0","abstract":"We present a novel diffusion-based framework for synthesizing 2D vector fields from sparse, coherent inputs (i.e., streamlines) while maintaining physical plausibility. Our method employs a conditional denoising diffusion probabilistic model with classifier-free guidance, enabling progressive reconstruction that preserves both geometric and physical constraints. Experimental results demonstrate our method's ability to synthesize plausible vector fields that adhere to physical laws while maintaining fidelity to sparse input observations, outperforming traditional optimization-based approaches in terms of flexibility and physical consistency.","accessible_pdf":null,"authors":[{"affiliation":"University of Houston","email":"nguyenpkk95@gmail.com","name":"Nguyen Phan"},{"affiliation":"University of Houston","email":"ricsf2000@gmail.com","name":"Ricardo Morales Vargas"},{"affiliation":"University of Houston","email":"sdelaesp@cougarnet.uh.edu","name":"Sebastian Espriella"},{"affiliation":"University of Houston","email":"guoning.chen@gmail.com","name":"Guoning Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"dfb477b8-fd21-4346-b7db-27bbbc2b16a0","image_caption":"This paper is primarily for vector field domain experts who need to study complex flow behavior. Additionally, artists and designers interested in special effects can apply this method to intuitively create realistic and complex flow phenomena by simply sketching a few guiding streamlines.","keywords":["Vector field synthesis","diffusion models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1320-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMjAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.sp7dCZ-h7wrEeT7Z05xRDMPcHzXcLcuNQmriEhTHcm0","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1320","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"dfb477b8-fd21-4346-b7db","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Vector Field Synthesis with Sparse Streamlines Using Diffusion Model","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"be118d02-63d2-4463-a135-af2c23073ad2","abstract":"Virtual sampling has been proposed as an alternative of pre-integration to reduce artifacts caused by an insufficient sampling of the ray profiles in volume-rendering applications. According to the originally recommended implementation, a 3D tricubic B-spline reconstruction is applied to evaluate true samples along the rays, while between the true samples, virtual samples are calculated by using a 1D Catmull-Rom spline interpolation. The virtual samples can be evaluated much faster, but they can still well approximate the true samples. Therefore, the rendering performance can be drastically improved at the cost of a very slight quality degradation. Nevertheless, in this paper, we show that the original virtual sampling scheme does not exploit the fact, that the applied tricubic B-spline reconstruction filter together with its analytic derivative filters provide Hermite samples. Therefore, it is more natural to use Hermite interpolation along the rays rather than Catmull-Rom spline interpolation. From a sampling-theoretical point of view, this modification requires half of the true samples to guarantee the same reconstruction quality. Furthermore, concerning the evaluation of the true samples, we also investigate a recently published GPU-accelerated triquadratic B-spline filtering as an alternative of the tricubic B-spline filtering.","accessible_pdf":null,"authors":[{"affiliation":"Budapest University of Technology and Economics","email":"cseb@iit.bme.hu","name":"Bal\u00e1zs Cs\u00e9bfalvi"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"be118d02-63d2-4463-a135-af2c23073ad2","image_caption":"Everybody who develops volume-rendering algorithms would be potentially interested in reading this paper. The presented technique can be applied in different application fields of volume rendering, such as visualization of 3D medical data or physical simulation data, for example. The paper describes how to make a reasonable compromise between image quality and rendering speed.","keywords":["Direct volume rendering","reconstruction filtering","virtual sampling."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1216-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMTYtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.0hmpLgsDYzCYVHxOrjywM57shSH_CTZ3NeNabvtRjuY","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1216","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"be118d02-63d2-4463-a135","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Virtual Ray Sampling for Direct Volume Rendering using Hermite Interpolation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"7ecd3f2d-f7b0-4b86-aa66-248cc17c0720","abstract":"Chart corpora, which comprise data visualizations and their semantic labels, are crucial for advancing visualization research. However, the labels in most existing corpora are high-level (e.g., chart types), hindering their utility for broader applications in the era of AI. In this paper, we contribute VISANATOMY, a corpus containing 942 real-world SVG charts produced by over 50 tools, encompassing 40 chart types and featuring structural and stylistic design variations. Each chart is augmented with multi-level fine-grained labels on its semantic components, including each graphical element\u2019s type, role, and position, hierarchical groupings of elements, group layouts, and visual encodings. In total, VISANATOMY provides labels for more than 383k graphical elements. We demonstrate the richness of the semantic labels by comparing VISANATOMY with existing corpora. We illustrate its usefulness through four applications: semantic role inference for SVG elements, chart semantic decomposition, chart type classification, and content navigation for accessibility. Finally, we discuss research opportunities to further improve VISANATOMY.","accessible_pdf":null,"authors":[{"affiliation":"University of Maryland","email":"cchen24@terpmail.umd.edu","name":"Chen Chen"},{"affiliation":"University of Maryland","email":"hbako@virginia.edu","name":"Hannah Bako"},{"affiliation":"University of Maryland","email":"peihong@umd.edu","name":"Peihong Yu"},{"affiliation":"University of Maryland","email":"hookerj100@gmail.com","name":"John Hooker"},{"affiliation":"University of Maryland","email":"jjoyal@terpmail.umd.edu","name":"Jeffrey Joyal"},{"affiliation":"University of Maryland","email":"wang.c.simon@gmail.com","name":"Simon Wang"},{"affiliation":"University of Maryland","email":"skim1270@terpmail.umd.edu","name":"Samuel Kim"},{"affiliation":"University of Maryland","email":"aprilwushuang@gmail.com","name":"Jessica Wu"},{"affiliation":"University of Maryland","email":"ading1@terpmail.umd.edu","name":"Aoxue Ding"},{"affiliation":"University of Maryland","email":"lsandeep@terpmail.umd.edu","name":"Lara Sandeep"},{"affiliation":"University of Maryland, College Park","email":"alexychen2002@gmail.com","name":"Alex Chen"},{"affiliation":"University of Maryland","email":"csinha@terpmail.umd.edu","name":"Chayanika Sinha"},{"affiliation":"University of Maryland","email":"leozcliu@umd.edu","name":"Zhicheng Liu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"7ecd3f2d-f7b0-4b86-aa66-248cc17c0720","image_caption":"Researchers who work on AI models for data visualizations can benefit from the paper. They can use the proposed dataset in the paper to train or fine-tune models.","keywords":["Chart","SVG","data visualization","corpus","dataset","multilevel fine-grained semantic labels"],"open_access_supplemental_link":"https://osf.io/962xc/?view_only=adbb315fd8794f6dac6b9625d385900f","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1411-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQxMS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQxLCJleHAiOjE3OTI0ODAzNDF9.nNb-4ZBKmhhzpW89-kqT1nX5J9Eq8c1DfdvN7fZ2HLg","preprint_link":"https://arxiv.org/abs/2410.12268","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1411","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"7ecd3f2d-f7b0-4b86-aa66","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"VisAnatomy: An SVG Chart Corpus with Fine-Grained Semantic Labels","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"88a99aaa-b89c-494c-b810-2113f43914ab","abstract":"The dissemination of visualizations is primarily in the form of raster images, which often results in the loss of critical information such as source code, interactive features, and metadata. While previous methods have proposed embedding metadata into images to facilitate Visualization Image Data Retrieval (VIDR), most existing methods lack practicability since they are fragile to common image tampering during online distribution such as cropping and editing. To address this issue, we propose VisGuard, a tamper-resistant VIDR framework that reliably embeds metadata link into visualization images. The embedded data link remains recoverable even after substantial tampering upon images. We propose several techniques to enhance robustness, including repetitive data tiling, invertible information broadcasting, and an anchor-based scheme for crop localization. VisGuard enables various applications, including interactive chart reconstruction, tampering detection, and copyright protection. We conduct comprehensive experiments on VisGuard's superior performance in data retrieval accuracy, embedding capacity, and security against tampering and steganalysis, demonstrating VisGuard's competence in facilitating and safeguarding visualization dissemination and information conveyance.","accessible_pdf":null,"authors":[{"affiliation":"East China Normal University","email":"huayuan221@gmail.com","name":"Huayuan Ye"},{"affiliation":"East China Normal University","email":"jtchen@stu.ecnu.edu.cn","name":"Juntong Chen"},{"affiliation":"East China Normal University","email":"10195102459@stu.ecnu.edu.cn","name":"Shenzhuo Zhang"},{"affiliation":"Tsinghua University","email":"yp-zhang22@mails.tsinghua.edu.cn","name":"Yipeng Zhang"},{"affiliation":"School of Computer Science and Technology","email":"cbwang@cs.ecnu.edu.cn","name":"Changbo Wang"},{"affiliation":"East China Normal University","email":"chli@cs.ecnu.edu.cn","name":"Chenhui Li"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"88a99aaa-b89c-494c-b810-2113f43914ab","image_caption":"Visualization designers, publishers or general public who uses charts in daily life.","keywords":["Visualization image data retrieval","image steganography","tampering resistance","tampering detection."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1083-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTA4My1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjY1LCJleHAiOjE3OTI0Nzk2NjV9.zqXgObPxdLlnQAz-210OIrdifPPkQLXgx_NQkeoNEFo","preprint_link":"https://arxiv.org/pdf/2507.14459","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1083","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"88a99aaa-b89c-494c-b810","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"e8ed00a7-2d3b-46b8-9edd-6853dd9c2f8c","abstract":"Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.","accessible_pdf":null,"authors":[{"affiliation":"Southern University of Science and Technology","email":"wanghc1999@gmail.com","name":"Huanchen Wang"},{"affiliation":"Southern University of Science and Technology","email":"zhangwc2024@mail.sustech.edu","name":"Wencheng Zhang"},{"affiliation":"Southern University of Science and Technology","email":"wangzq_2021@outlook.com","name":"Zhiqiang Wang"},{"affiliation":"George Mason University","email":"zlu6@gmu.edu","name":"Zhicong Lu"},{"affiliation":"Southern University of Science and Technology","email":"mayx@sustech.edu.cn","name":"Yuxin Ma"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"e8ed00a7-2d3b-46b8-9edd-6853dd9c2f8c","image_caption":"Data Scientists in Multi-Modal Model: Use the framework to evaluate the effects of various corruption types, uncover data patterns, and guide augmentation strategies to enhance the robustness of existing vision-language models;\nMachine Learning Researchers: Leverage the multi-level analysis capabilities of the framework to diagnose performance issues and refine models for deployment in real-world applications, ensuring resilience to corrupted or shifted data;\nHCI Specialists: Explore the design principles of the framework to create user-centric tools that facilitate intuitive understanding of model behavior, empowering practitioners to make data-driven decisions.","keywords":["Visual analytics","multi-modal model","corruption robustness","image captioning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1416-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQxNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.XzGo4WkjVuHu6RCZrkWlt1Gdsoer8uDJi0VPjCJ2Fgw","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1416","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"e8ed00a7-2d3b-46b8-9edd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"abf4110d-55e9-4824-875f-5b8a60547704","abstract":"Data visualization tasks often require multi-step reasoning, and the interpretive strategies experts use-such as decomposing complex goals into smaller subtasks and selectively attending to key chart regions-are rarely made explicit. ViStruct is an automated pipeline that simulates these expert behaviours by breaking high-level visual questions into structured analytic steps and highlighting semantically relevant chart areas. Leveraging large language and vision-language models, ViStruct identifies chart components, maps subtasks to spatial regions, and presents visual attention cues to externalize expert-like reasoning flows. While not designed for direct novice instruction, ViStruct provides a replicable model of expert interpretation that can inform the development of future visual literacy tools. We evaluate the system on 45 tasks across 12 chart types and validate its outputs with trained visualization users, confirming its ability to produce interpretable and expert-aligned reasoning sequences.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Toronto","email":"oliver@dgp.toronto.edu","name":"Oliver Huang"},{"affiliation":"University of Toronto","email":"cnobre@cs.toronto.edu","name":"Carolina Nobre"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"abf4110d-55e9-4824-875f-5b8a60547704","image_caption":"This work will be especially valuable for data visualization educators, instructional designers, and researchers developing tools to enhance data literacy. ViStruct presents a system that simulates expert-like reasoning by breaking down visualization tasks and highlighting semantically relevant regions of charts. While it is not a guidance tool on its own, ViStruct offers an automated model that illustrates how experts interpret visualizations. This provides a foundation for creating instructional materials, interactive learning environments, or support mechanisms aimed at beginners.\n\nEducators can use ViStruct to introduce students to structured analytic processes and region-level attention strategies, which are often implicit and challenging to teach. A key direction for future research is to evaluate how this approach impacts cognitive engagement and understanding among novice users, particularly in educational settings that emphasize visual literacy.","keywords":["Data Visualization","Task Decomposition","Large Language Models(LLMs)","Guidance System","Computer Vision"],"open_access_supplemental_link":"https://github.com/hivelabuoft/ViStruct","open_access_supplemental_question":"Our system and code are publicly available as an open-source interactive platform at https://vi-struct.vercel.app, including annotated examples, chart inputs, and model outputs. We have also open-sourced the expert review evaluation data used in our study to support transparency and facilitate future comparative work.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1016-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEwMTYtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.q0WGtvb4fix9pviJi3g3Ylw9-T1BI0Fj_frfQQn3j14","preprint_link":"https://arxiv.org/abs/2506.21762","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1016","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"abf4110d-55e9-4824-875f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b5fec96f-039e-4af2-93d6-898b9a2caece","abstract":"Comparing tensors and identifying their (dis)similar structures is fundamental in understanding the underlying phenomena for complex data. Tensor decomposition methods help analysts extract tensors\u2019 essential characteristics and aid in visual analytics for tensors. In contrast to dimensionality reduction (DR) methods designed only for analyzing a matrix (i.e., second-order tensor), existing tensor decomposition methods do not support flexible comparative analysis. To address this analysis limitation, we introduce a new tensor decomposition method, named tensor unified linear comparative analysis (TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA integrates discriminant analysis and contrastive learning schemes for tensor decomposition, enabling flexible comparison of tensors. We also introduce an effective method to visualize a core tensor extracted from TULCA into a set of 2D visualizations. We integrate TULCA\u2019s functionalities into a visual analytics interface to support analysts in interpreting and refining the TULCA results. We demonstrate the efficacy of TULCA and the visual analytics interface with computational evaluations and two case studies, including an analysis of log data collected from a supercomputer.","accessible_pdf":"Accessible","authors":[{"affiliation":"Kobe University","email":"237x012x@stu.kobe-u.ac.jp","name":"Naoki Okami"},{"affiliation":"Kobe University","email":"kazuzaka53@gmail.com","name":"Kazuki Miyake"},{"affiliation":"Kobe University","email":"naohisa.sakamoto@people.kobe-u.ac.jp","name":"Naohisa Sakamoto"},{"affiliation":"RIKEN Center for Computational Science","email":"jorji@riken.jp","name":"Jorji Nonaka"},{"affiliation":"Link\u00f6ping University","email":"tfujiwara@ucdavis.edu","name":"Takanori Fujiwara"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b5fec96f-039e-4af2-93d6-898b9a2caece","image_caption":"- Data scientists and operational staff who needs to monitor and analyze time-series datta, such as supercomputer operational staff and machine maintenance staff.\n\n- Data scientists learn a new tensor decomposition method that can be used to interactively compare multiple tensor groups (e.g., groups of multivariate time series). Operational staff can see a state-of-the-art example of how multivariate time series can be interactively analyzed, using visualizations and our new tensor decomposition method.","keywords":["Tensor decomposition","tensor analysis","contrastive learning","dimensionality reduction","interpretability","supercomputer"],"open_access_supplemental_link":"https://github.com/vizlab-kobe/tulca","open_access_supplemental_question":"We provide an open-source library and an open-source visual interface, with easy installation and thoroughly documented source code (following the Python standard). In addition, to help replicate our results, we provide datasets and source code used for evaluations.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/2057-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMjA1Ny1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTQyLCJleHAiOjE3OTI0ODA5NDJ9.A4oXA6aAO4_4putdti3bVlxMkP84uFG28OjykVN3mJY","preprint_link":"https://arxiv.org/abs/2507.19988","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2057","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b5fec96f-039e-4af2-93d6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visual Analytics Using Tensor Unified Linear Comparative Analysis","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"440d12ea-914c-49b0-a380-163dab6a3e41","abstract":"Understanding user interactions in digital systems is essential in analyzing user behaviors and improving system usability. However, a collection of interaction sequences is often large and unstructured, making it challenging to uncover interaction patterns. To address this challenge, we introduce a visual analytics approach that integrates hierarchical clustering and process mining techniques to support analysts in exploring unstructured, large interaction sequence data. Our system employs a tailored dynamic time warping-based similarity measure to enable comparison of interaction sequences. Based on the sequence similarities, we provide stepwise, interactive navigation of clustering results with contextual visual cues for refinement and validation. We further apply process mining to characterize derived clusters. Through these hierarchical clustering and process mining steps, analysts can progressively uncover meaningful interaction patterns while utilizing visual guidance and incorporating domain expertise. We demonstrate our system's effectiveness and applicability through two case studies involving system designers, developers, and domain experts.","accessible_pdf":"Accessible","authors":[{"affiliation":"Link\u00f6ping University","email":"peilin.yu@liu.se","name":"Peilin Yu"},{"affiliation":"Link\u00f6ping University","email":"aida.vitoria@liu.se","name":"Aida Nordman"},{"affiliation":"Link\u00f6ping University","email":"tfujiwara@ucdavis.edu","name":"Takanori Fujiwara"},{"affiliation":"Link\u00f6ping University","email":"marta.koc-januchta@liu.se","name":"Marta Koc-Januchta"},{"affiliation":"Link\u00f6ping University","email":"konrad.schonborn@liu.se","name":"Konrad Sch\u00f6nborn"},{"affiliation":"Link\u00f6ping University","email":"lonni.besancon@gmail.com","name":"Lonni Besan\u00e7on"},{"affiliation":"Link\u00f6ping University","email":"katerina.vrotsou@liu.se","name":"Katerina Vrotsou"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"440d12ea-914c-49b0-a380-163dab6a3e41","image_caption":"This paper will be of particular interest to visual analytics researchers, user experience (UX) designers, data scientists, and educational technologists who work with interaction log data and public exhibits. Practitioners in these fields often need to understand how users engage with digital systems in order to, e.g., identify user engagement trends, usability bottlenecks, or refine interface designs by analyzing real user interactions. The reVISID system presented in this work offers a method for uncovering exploration strategies and behavioral patterns using hierarchical clustering and process mining.","keywords":["Pattern discovery in interaction logs","visual analytics","dynamic time warping","hierarchical clustering","process mining"],"open_access_supplemental_link":"https://osf.io/6az29/","open_access_supplemental_question":"Our work provides sufficient supplemental materials.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1170-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTE3MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.fjvC1whFo5umZjkhWY9gbl35J3BW0HR9KscQ_5Ja5a4","preprint_link":"https://doi.org/10.31219/osf.io/n5dxe_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1170","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"440d12ea-914c-49b0-a380","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visual Extraction of Interaction Patterns Guided by Hierarchical Clustering and Process Mining","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"a5a6c51f-9a92-459b-bbf9-0192d4c7f09f","abstract":"Compact visualizations are often used to detect expected and unexpected data characteristics at a glance. The literature uses the terms fingerprints, thumbnails, or sketches for these bite-sized visualizations. To date, these visualizations have not been described in context and their discussion is fragmented across publications. In this paper, we propose the term Visual Fingerprints for these visualizations, highlight the task of visual detection for which they are designed, outline their visual properties, and discuss their commonalities and differences with related concepts. In doing so, we aim to bundle the existing research on these visualizations into a common concept and term that serves as a foundation and stepping stone for future developments in this direction.","accessible_pdf":"Accessible","authors":[{"affiliation":"Aarhus University","email":"meredithc@cs.au.dk","name":"Meredith Siang-Yun Chou"},{"affiliation":"Aarhus University","email":"hjschulz@cs.au.dk","name":"Hans-J\u00f6rg Schulz"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a5a6c51f-9a92-459b-bbf9-0192d4c7f09f","image_caption":"This work provides the visualization community/ researchers with a foundation for future empirical studies and theoretical refinement. Practitioners, such as visualization designers, can refer it as a starting point for designing tailor-made VFPs.","keywords":["Charts","Diagrams","and Plots"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1263-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNjMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU3MywiZXhwIjoxNzkyNDgyNTczfQ.jCc2okSnW47EWPg9O6JfDh4sH7E5m7muUc_fT76drlQ","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1263","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"a5a6c51f-9a92-459b-bbf9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visual Fingerprints for Detecting Data Characteristics","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"0e4ce5d7-ea72-4776-af1e-e42301ba40b7","abstract":"Deep Learning Super Sampling (DLSS) is a relatively new \nmachine learning technology for accelerating 3D graphics, by \nrendering at lower resolutions, and inferring what the higher \nresolution output should look like. However, DLSS was designed for, \nand trained on, video games. Can this technology, intended for the \nentertainment market, be repurposed to accelerate high-resolution \nscientific and geospatial visualization? This paper evaluates the visual \naccuracy costs of inferring higher resolution data visualizations with \nDLSS upscaling. A related technology, Deep Learning Anti-aliasing \n(DLAA) is similarly evaluated. This evaluation focused on 3D \ngeospatial applications, with (primarily non-photorealistic) terrain \nrendering and point cloud rendering, though the results are applicable \nto a wider range of scientific visualization data types. The results \ndemonstrate that DLSS/DLAA can significantly impact visual \naccuracy, and should be avoided for visualizations requiring accurate \nportrayal of fine details.","accessible_pdf":null,"authors":[{"affiliation":"University of New Hampshire","email":"kindratberegovyi@gmail.com","name":"Kindrat Beregovyi"},{"affiliation":"University of New Hampshire","email":"tbutkie@gmail.com","name":"Thomas Butkiewicz"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"0e4ce5d7-ea72-4776-af1e-e42301ba40b7","image_caption":"This work would be of interest to anyone who creates 3D visualizations and is tempted to use DLSS to increase performance. This work will help them make informed choices about when it is and is not appropriate to use DLSS and DLAA.","keywords":["Machine Learning Techniques; Computer Graphics Techniques; Guidelines; Geospatial Data; Terrain","Point Clouds; Deep Learning Super Sampling","DLSS","Upscaling; DLAA; Anti-Aliasing"],"open_access_supplemental_link":"https://vislab-ccom.unh.edu/downloads/DLSS_Paper_Data.zip","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1232-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMzItZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.8KQvn1BtQk_GafjVV_bzmQUico58d0MtTzmSSX8_Thc","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1232","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"0e4ce5d7-ea72-4776-af1e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visual Integrity in the Age of AI: An Evaluation of DLSS and DLAA in Geospatial Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"49b63c40-81c7-41ec-a38a-9d433769c1c6","abstract":"This paper presents Visualization Badges, graphical labels shown alongside visualizations to communicate provenance and design considerations to enhance understandability and transparency. Badges may, for example, highlight a major finding, disclose that an axis has been truncated, or warn of possible visual artifacts. Inspired by nutrition and energy labels on product packaging, visualization badges aim (i) to allow visualization authors to justify and disclose analysis and design decisions and (ii) to make readers aware of important information when viewing and interpreting visualizations. Collectively, visualization badges aim to foster trust in visualizations and prevent readers from drawing incorrect conclusions. Based on a series of co-design workshops, we define and evaluate the concept of visualization badges and formulate a conceptual framework for analysis, application, and further research. Our framework includes a catalog of 132 visualization badges, categorization schemes, design options for their visual representations, applied visualization examples, and guidelines for their use. We hope that visualization badges will help communicate data and collectively improve communication, visualization literacy, and the quality of visualization techniques. Our badges, workshops, and guidelines can be found online https://vis-badges.github.io.","accessible_pdf":null,"authors":[{"affiliation":"Inria","email":"valentin.edelsbrunner@inria.fr","name":"Valentin Edelsbrunner"},{"affiliation":"The University of Edinburgh","email":"jinrui.w@outlook.com","name":"Jinrui Wang"},{"affiliation":"University of Edinburgh","email":"alexis.pister@hotmail.com","name":"Alexis Pister"},{"affiliation":"School of Law (PeaceRep)","email":"tvancisi@ed.ac.uk","name":"Tomas Vancisin"},{"affiliation":"The University of Edinburgh","email":"sian.phillips@ed.ac.uk","name":"Sian Phillips"},{"affiliation":"University of Oxford","email":"min.chen@oerc.ox.ac.uk","name":"Min Chen"},{"affiliation":"Inria","email":"bbach@inf.ed.ac.uk","name":"Benjamin Bach"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"49b63c40-81c7-41ec-a38a-9d433769c1c6","image_caption":"Who?\n - Data-vis / UX designers\n - Data journalists\n - BI & analytics engineers\n - Domain scientists / policy analysts\n - Educators teaching visual literacy\n - Data scientists\n\nHow to apply?\n - We provide guidelines on how to apply our work.\n - We outline the design space for Visualization Badges.\n - Our supplementary website offers 132 ready-to-reuse Visualization Badges.","keywords":["Data Visualization","Communication","Transparency"],"open_access_supplemental_link":"https://vis-badges.github.io/#/about","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1844-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTg0NC1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.pgFadhJA-adx5H6kV_g54wdUGcuNZHfxr_hGIJO18yc","preprint_link":"https://hal.science/view/index/docid/5199752","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1844","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"49b63c40-81c7-41ec-a38a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visualization Badges: Communicating Design and Provenance through Graphical Labels Alongside Visualizations","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"6a8c1148-d065-4251-9183-2955b9445817","abstract":"In contemporary information ecologies saturated with misinformation, disinformation, and a distrust of science itself, public data communication faces significant hurdles. Although visualization research has broadened criteria for effective design, governing\nparadigms privilege the accurate and efficient transmission of data. Drawing on theory from linguistic anthropology, we argue that such approaches\u2014focused on encoding and decoding propositional content\u2014cannot fully account for how people engage with visualizations and why particular visualizations might invite adversarial or receptive responses. In this paper, we present evidence that data visualizations communicate not only semantic, propositional meaning\u2014meaning about data\u2014but also social, indexical meaning\u2014meaning beyond data. From a series of ethnographically-informed interviews, we document how readers make rich and varied assessments of a visualization\u2019s \u201cvibes\u201d\u2014inferences about the social provenance of a visualization based on its design features. Furthermore, these social attributions have the power to influence reception, as readers\u2019 decisions about how to engage with a visualization concern not only content, or even aesthetic appeal, but also their sense of alignment or disalignment with the entities they imagine to be involved in its production and circulation. We argue these inferences hinge on a function of human sign systems that has thus far been little studied in data visualization: socio-indexicality, whereby the formal features (rather than the content) of communication evoke social contexts, identities, and characteristics. Demonstrating the presence and significance of this socio-indexical function in visualization, this paper offers both a conceptual foundation and practical intervention for troubleshooting breakdowns in public data communication.","accessible_pdf":"Accessible","authors":[{"affiliation":"Massachusetts Institute of Technology","email":"mjmorgen@mit.edu","name":"Michelle Morgenstern"},{"affiliation":"Massachusetts Institute of Technology","email":"amyraefoxphd@gmail.com","name":"Amy Fox"},{"affiliation":"Massachusetts Institute of Technology","email":"gmj@mit.edu","name":"Graham Jones"},{"affiliation":"MIT","email":"arvindsatya@mit.edu","name":"Arvind Satyanarayan"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"6a8c1148-d065-4251-9183-2955b9445817","image_caption":"Practitioners involved in communicating scientific data to general publics would be interested in reading this paper. The paper contributes a conceptual model that can be applied to help diagnose and repair the roots of communication breakdowns that might not otherwise be evident, providing a framework for practitioners to consider what socio-indexical inferences their visualization design choices might provoke and the consequences of those inferences on audience reception and engagement. It is of relevance to any topic or data domain, but perhaps particularly useful for more high stakes and contentious topics (e.g. health and climate) around which public data communication faces urgent challenges in reaching polarized media environments with adversarial audiences that increasingly disavow the validity of science itself.","keywords":["Semiotics","Socio-indexicality","Attitudes","Reception","Engagement","Visualization Psychology","Public Data Communication"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/ERC6P","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1005-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTAwNS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNjYwLCJleHAiOjE3OTI0Nzk2NjB9.ZOJB6EdhINgTraf8Z6pkEd-S-OyLkGaPZWIcyaJEFRU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1005","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"6a8c1148-d065-4251-9183","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visualization Vibes: The Socio-Indexical Function of Visualization Design","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"a744e0f2-d891-4fd6-b3a0-d6e2f9227708","abstract":"With the increasing complexity and diversity of climate models, climate scientists often turn to multiple tools to understand and assess relationships between observations and model output. To explore ways to overcome that workflow complexity, we partnered with the Climate Modeling Alliance (CliMA) to develop CliMAScope. CliMAScope is an open source, interactive, web-based land surface climate model visualization tool that aims to unify analysis, comparison, validation, and correlation mapping of observational data with climate land model output. This paper presents a design study of CliMAScope, and explains how the tool's visual and interactive elements support the goal of comparing seamless climate model outputs to observational data.","accessible_pdf":"Accessible","authors":[{"affiliation":"Havard University","email":"linhphaaam@gmail.com","name":"Linh Pham"},{"affiliation":"Harvard University","email":"inbox.kevin.hu@gmail.com","name":"Kevin Hu"},{"affiliation":"University of Wisconsin - Madison","email":"ntwhite@wisc.edu","name":"Nathan White"},{"affiliation":"ArtCenter College of Design","email":"junaline4@gmail.com","name":"Minyoung Joo"},{"affiliation":"California Institute of Technology","email":"arenchon@caltech.edu","name":"Alexandre Renchon"},{"affiliation":"California Institute of Technology","email":"tapio@caltech.edu","name":"Tapio Schneider"},{"affiliation":"Jet Propulsion Lab","email":"krys.t.blackwood@jpl.nasa.gov","name":"Krys Blackwood"},{"affiliation":"California Institute of Technology","email":"santiago@caltech.edu","name":"Santiago Lombeyda"},{"affiliation":"California Institute of Technology (Caltech)","email":"hmushkin@caltech.edu","name":"Hillary Mushkin"},{"affiliation":"California Institute of Technology","email":"sd@scottdavidoff.com","name":"Scott Davidoff"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"a744e0f2-d891-4fd6-b3a0-d6e2f9227708","image_caption":"This paper will interest climate scientists, Earth system modelers, environmental data analysts, visualization researchers, and science communicators, as well as members of the broader public engaged in climate and sustainability issues. Practitioners working with complex, high-dimensional climate data \u2013 such as simulation scientists, policy researchers, and interactive tool designers \u2013 can apply insights from this work to develop more intuitive, comparative, and exploratory tools for model validation and communication.\n\nCliMAScope demonstrates how interactive visual analytics can bridge the gap between raw climate model output and meaningful interpretation. Its spatial-temporal-correlation views and expressive encoding styles provide pathways for practitioners to identify ecological trends, evaluate model behavior, and generate new scientific hypotheses. The system also supports more accessible communication of climate data, helping non-experts better grasp the dynamics and implications of Earth system models.","keywords":["visual analytics","design study climate model"],"open_access_supplemental_link":"https://github.com/datavisprogram/climascope","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1113-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExMTMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NTkzOCwiZXhwIjoxNzkyNDgxOTM4fQ.KP01XFXzVjCT0Hw8Jmgn_Q7fZC7TcmLE65vNkVRy1YI","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1113","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"a744e0f2-d891-4fd6-b3a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visualizing Climate Model Outputs with CliMAScope","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"918b3f1e-6e9b-43f0-bb8b-5bad6b3956ef","abstract":"Smartwatches increasingly serve as daily data analysis tools, yet their visualization designs face unique challenges: small screens, diverse dial shapes (round/square), and mobile usage contexts (sitting/walking). These factors may interact in unexpected ways, potentially compromising data readability during real-world use. Through a controlled experiment (N=32) comparing bar vs. radial visualizations across dial shapes and motion states, we found: (1) walking significantly reduces estimation accuracy, (2) bar charts consistently outperform radial variants by better supporting linear perception, and (3) radial bar graphs on square dials create particularly poor affordances than other combination. Our work provides the empirical evidence for how smartwatch dial factors, usage contexts and visualization design jointly impact data interpretation, offering concrete design guidelines for smartwatch visualization. Supplemental material is available at https://osf.io/k7fa3/.","accessible_pdf":null,"authors":[{"affiliation":"Xi\u2019an Jiaotong-Liverpool University","email":"zhouxuan.xia21@student.xjtlu.edu.cn","name":"Zhouxuan Xia"},{"affiliation":"Xi\u2019an Jiaotong-Liverpool University","email":"fengyuan.liao22@student.xjtlu.edu.cn","name":"Fengyuan Liao"},{"affiliation":"Xi\u2019an Jiaotong-Liverpool University","email":"jinyuan.du18@student.xjtlu.edu.cn","name":"Jinyuan Du"},{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"yu.liu02@xjtlu.edu.cn","name":"Yu Liu"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"918b3f1e-6e9b-43f0-bb8b-5bad6b3956ef","image_caption":"This paper will be of interest to smartwatch interface designers and developers who aim to optimize the presentation of the data. Additionally, practitioners involved in mobile and wearable data visualization design can benefit from the insights provided.","keywords":["Smartwatch Visualization","Mobile Visualization","Human-Computer Interaction"],"open_access_supplemental_link":"https://osf.io/k7fa3/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1152-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzExNTItZG9jLnBkZiIsImlhdCI6MTc2MDk0NTk0OSwiZXhwIjoxNzkyNDgxOTQ5fQ.ChXdspRXNuuR09INzZiaiiOKyYu5bTO2xODQ4Rx9hQ4","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1152","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"918b3f1e-6e9b-43f0-bb8b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visualizing on the Wrist: Impact of Motion, Dial Shape and Visualization Type on Smartwatch","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"04cf6d82-a777-4312-a4b0-d3f4b9479602","abstract":"We investigate the impact of visualization methods and interaction mechanisms in Voting Advice Applications (VAAs) by comparing two designs: the traditional design in Kieskompas and an alternative design composed by a visualization -- metaphoric map -- based on dimensionality reduction of opinion data and an interaction mechanism allowing to adjust answers on the questions. In a user study with 382 participants from the Netherlands, we assessed their effects on vote choice, political knowledge, ease of use, and interpretation of opinion space. The results show no evidence that visualization and interaction mechanisms influence vote choice. The metaphoric map led to lower perceived and factual political knowledge and was harder to interpret, likely due to its higher visualization literacy demands. Notably, participants using metaphoric map offered more objective, non-ideological interpretations of their political position, suggesting potential to foster unbiased political comprehension. Supplementary materials are available at: https://osf.io/zvhsg/?view_only=ee2d7d315c254b358b9be077214cc18e.","accessible_pdf":null,"authors":[{"affiliation":"Utrecht University","email":"dverboom@vallei-veluwe.nl","name":"Damion E. Verboom"},{"affiliation":"Utrecht University","email":"t.mtsentlintze@uu.nl","name":"Tamara Mchedlidze"},{"affiliation":"Utrecht University","email":"e.oral@uu.nl","name":"Ba\u015fak Oral"},{"affiliation":"Utrecht University","email":"evanthia.dimara@gmail.com","name":"Evanthia Dimara"},{"affiliation":"Saxion University","email":"anadanielaperesrebelo@gmail.com","name":"Daniela Peres Rebelo"},{"affiliation":"Tilburg University","email":"n.kamoen@tilburguniversity.edu","name":"Naomi Kamoen"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"04cf6d82-a777-4312-a4b0-d3f4b9479602","image_caption":"Political scientists, VAA developers, and data journalists. Political scientists can learn about the importance of different visualization designs on political decisions. VAA developers can get inspired by the insights for better visualization and interaction design. importance of visualization political scientist. Data journalists can get inspired by metaphorical maps, and they can use it in other contexts.","keywords":["VAAs","visualization","interaction","dimensionality reduction","social information","voting"],"open_access_supplemental_link":"https://osf.io/zvhsg/?view_only=ee2d7d315c254b358b9be077214cc18e","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1299-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyOTktZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.JZaWUWKWHy92eJah-CMaE8GWiWNvtAnUDY5wdWNy7NA","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1299","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"04cf6d82-a777-4312-a4b0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visualizing Opinion Space in Voting Advice Applications: A User Study","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3c025133-4562-4d9a-aff2-725dd723a2a2","abstract":"To understand multiplayer gameplay behavior in a bomb-laying game, this paper explores the visual representation of player movement and events using space-time cubes. Complementing a previous two-dimensional event visualization, the approach focuses on contextualizing the player trajectories on the board with important events. Leveraging extended reality technology, the three-dimensional space-time-cube representation of a game session can be placed like columns in the virtual space. Various techniques support the interactive exploration of the spatiotemporal data. We demonstrate insights that can be found through the analysis of AI agent play behavior.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Bamberg","email":"niko-sebastian.nannemann@stud.uni-bamberg.de","name":"Niko S. Nannemann"},{"affiliation":"University of Bamberg","email":"shivamworking@gmail.com","name":"Shivam Agarwal"},{"affiliation":"University of Bamberg","email":"fabian.beck@uni-bamberg.de","name":"Fabian Beck"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3c025133-4562-4d9a-aff2-725dd723a2a2","image_caption":"Practitioners who would be most interested in this paper include game analysts and gamers, AI researchers, and immersive analytics developers. The visualization approach enables them to explore spatiotemporal gameplay data in extended reality, revealing patterns and strategies of AI agents that are difficult to spot in traditional 2D visualizations. Practitioners can apply this method to better evaluate and improve AI behavior, analyze player strategies, or adapt it to other board-based or team-based simulations involving movement and events over time.","keywords":["Game analytics","space-time cube","event visualization","immersive analytics","extended reality."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1213-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMTMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.mbN0sJJQum44iC1tASkkEnDdglVh6IAClT1cjzUFEz4","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1213","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3c025133-4562-4d9a-aff2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visualizing Player Movement and Game Events through Space-Time Cubes in Extended Reality","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"de570d65-b80f-4e25-9d54-88f11281e2c5","abstract":"Effective data visualizations enhance perception, support cognitive processing, and facilitate informed decision-making by aligning with human perceptual strengths. Conversely, poorly designed visualizations can impede comprehension, introduce interpretive bias, and diminish the perceived credibility of the conveyed message. This paper investigates the extent to which visual embellishments influence perceived message credibility in data visualizations. We conducted two crowdsourced experiments to examine both holistic and component-level effects of embellishment. In the first experiment, participants evaluated the relative credibility of plain bar charts versus two embellished variants\u2014cartoon-style and image-style\u2014across topics. Participants provided both comparative judgments and qualitative feedback. In the second experiment, we systematically isolated the influence of specific design elements\u2014color, font, and bar style\u2014on credibility perceptions through controlled variations. Our findings reveal that the impact of embellishments on perceived message credibility is complex and context-dependent. While certain embellishments, such as the use of color and image style bars, enhanced credibility, others\u2014most notably hand-drawn fonts and cartoon-style bars\u2014significantly undermined it. By operationalizing trust through the lens of message credibility, this work offers empirical insight into the design factors that shape viewers' perceptions. We conclude by proposing actionable design guidelines to support the creation of visualizations that are effective for communication and credible.","accessible_pdf":null,"authors":[{"affiliation":"Georgia Institute of Technology","email":"hsong300@gatech.edu","name":"Hayeong Song"},{"affiliation":"Georgia Institute of Technology","email":"aeree@gatech.edu","name":"Aeree Cho"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"},{"affiliation":"Georgia Institute of Technology","email":"john.stasko@cc.gatech.edu","name":"John Stasko"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"de570d65-b80f-4e25-9d54-88f11281e2c5","image_caption":"Practitioners involved in data communication, visualization design, and decision support would find this paper especially valuable. This includes:\n\nData journalists \u2013 who craft visual stories and must balance credibility with visual appeal.\n\nData scientists and analysts \u2013 who design visualizations to communicate findings to stakeholders or non-technical audiences.\n\nUX/UI designers \u2013 particularly those designing dashboards or visual data interfaces for communication and credibility.\n\nPractitioners can apply the insights in several concrete ways:\n\nDesign Credible Visuals:\nUse the paper\u2019s findings to avoid visual elements (e.g., cartoon styles, hand-drawn fonts) that reduce credibility, especially when communicating serious or data-driven content.\n\nTailor Visuals to Context:\nRecognize that visual design choices should be context-dependent\u2014what works in one domain (e.g., children\u2019s education) may backfire in another (e.g., financial reporting).\n\nApply Design Guidelines:\nLeverage the proposed design recommendations to improve the trustworthiness and interpretability of data visualizations used in reports, media, dashboards, or presentations.","keywords":["Data Visualization","Visual Embellishment","Trust","Credibility","Chart Design","Perception"],"open_access_supplemental_link":"https://osf.io/ph4b8/files/osfstorage","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1250-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTI1MC1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQ0LCJleHAiOjE3OTI0Nzk3NDR9.6O5ZpGyaAQKRtIhJYhpg8mdVK0R-5x5rWPgk7buPuhg","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1250","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"de570d65-b80f-4e25-9d54","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Visualizing Trust: How Chart Embellishments Influence Perceptions of Credibility","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"9e05775a-056c-42f7-a3c2-e81281c5045a","abstract":"We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities\u2014such as threshold-based filtering, slice extraction, and statistical analysis\u2014through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system\u2019s adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., \u201cvisualize the skull\u201d or \u201chighlight tissue boundaries\u201d). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.","accessible_pdf":null,"authors":[{"affiliation":"Los Alamos National Laboratory","email":"ayanju04@gmail.com","name":"Ayan Biswas"},{"affiliation":"Los Alamos National Laboratory","email":"tlturton@lanl.gov","name":"Terece Turton"},{"affiliation":"Los Alamos National Laboratory","email":"ranasinghe@lanl.gov","name":"Nishath Ranasinghe"},{"affiliation":"Los Alamos National Laboratory","email":"smjones@lanl.gov","name":"Shawn Jones"},{"affiliation":"Los Alamos National Laboratory","email":"love@lanl.gov","name":"Bradley Love"},{"affiliation":"Los Alamos National Laboratory","email":"wjones@lanl.gov","name":"William Jones"},{"affiliation":"Los Alamos National Laboratory","email":"hagberg@lanl.gov","name":"Aric Hagberg"},{"affiliation":"The Ohio State University","email":"shen.94@osu.edu","name":"Han-Wei Shen"},{"affiliation":"Los Alamos National Laboratory","email":"ndebard@lanl.gov","name":"Nathan Debardeleben"},{"affiliation":"Los Alamos National Laboratory","email":"earl@lanl.gov","name":"Earl Lawrence"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"9e05775a-056c-42f7-a3c2-e81281c5045a","image_caption":"simulation scientists, data scientists\n\nThey can design a similar system to expedite the process of insight generation from their data.","keywords":["Scientific data","Large language models","Agentic workflows","Natural language","Feature-based visualization."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1626-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTYyNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.IsW0mEAI5AbfiO2_ftTuEwciJGY9_7h8hTZ62OxOv2I","preprint_link":"https://arxiv.org/abs/2507.21124","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1626","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"9e05775a-056c-42f7-a3c2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"4e824cf3-2aef-487f-b8a9-b0262fb2132d","abstract":"Data-driven articles have emerged as a powerful method for transforming complex datasets into accessible narratives. \nExisting approaches, such as interactive visualizations and scrollytelling, enhance engagement and simplify complexity by guiding users through data stories.\n\\rvis{However, these designs often prioritize narrative structure over reader-driven exploration, which may limit opportunities for autonomous interaction.}\nTo address this limitation, existing literature explored features that dynamically link textual content to corresponding elements in interactive graphs. \nIn this study, we investigate the impact of such tools on the reading experience, specifically the hedonic and pragmatic dimensions, in addition to loyalty intentions.\nWe implemented Vizualink, a text-graph linking feature, and examined its impact through an online study with 360 participants.\nOur results indicate that Vizualink enhances the reading experience, which in turn positively impacts readers' intention to return for more content, recommend the article, and consider paying for access.\nA thematic analysis of participant comments highlighted enhanced interactivity and criticism regarding potential hindrances to user experience.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Neuch\u00e2tel","email":"abdessalam.ouaazki@unine.ch","name":"Abdessalam Ouaazki"},{"affiliation":"University of Neuch\u00e2tel","email":"adrian.holzer@unine.ch","name":"Adrian Holzer"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"4e824cf3-2aef-487f-b8a9-b0262fb2132d","image_caption":"This paper would be of interest to data journalists, UX designers, and data visualization practitioners, who could apply its insights to enhance reader engagement and usability in interactive, data-driven articles.","keywords":["Vizualink","Text-Graph Integration","Data-Driven Articles","Interactive Systems"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1217-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMTctZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NywiZXhwIjoxNzkyNDgyNTY3fQ.WEmHCtWiDb3I6PxAQD5TANxlF9wR3SJAvk3JvEb8z3M","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1217","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"4e824cf3-2aef-487f-b8a9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Vizualink: Evaluating Usability and Loyalty Impacts of Text-Graph Integration in Data-Driven Articles","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f84ebbf0-98bb-4a65-9640-3161c7449f60","abstract":"We present VolMoVis, a method for dynamic tomographic reconstruction that supports real-time volume generation and volumetric motion visualization from 2D projections. Visualizing the motion of 3D anatomical structures, such as organs and tumors, is critical for computer-aided interventions. However, conventional 4D volumetric reconstruction methods typically produce a limited set of volumes at discrete phases, suffering from low temporal resolution. Moreover, it often requires extensive segmentation of 3D structures or regions for visualizing volumetric data, making it challenging to segment and visualize dynamic volumes in real-time. To address these challenges, VolMoVis framework employs a continuous implicit neural representation that decomposes the dynamic volumetric data into a static reference volume and a continuous deformation field. This decomposition, along with an efficient deformation network, enables our framework to achieve real-time volume generation and volumetric visualization of continuous anatomical motions. We evaluate VolMoVis on both 4D digital phantoms and real patient datasets, demonstrating its effectiveness for accurate anatomical reconstruction and motion tracking. Furthermore, we highlight its capabilities in real-time simultaneous volume generation and tumor segmentation for visualizing dynamic volumes and 4D tumor tracking, showcasing its potential in image-guided radiation therapy.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"dgaofeng@cs.stonybrook.edu","name":"Gaofeng Deng"},{"affiliation":"Stony Brook University","email":"ari@cs.stonybrook.edu","name":"Arie Kaufman"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f84ebbf0-98bb-4a65-9640-3161c7449f60","image_caption":"This paper will be of interest to medical physicist, radiation oncologists, radiologist, and visualization practitioners working with dynamic anatomical data. In particular, clinicians and engineers involved in image-guided radiation therapy (IGRT), 4D CT/CBCT reconstruction, and tumor motion tracking could benefit from the presented work.","keywords":["Neural Representations","4D Reconstruction","Dynamic Volume Visualization","Real-Time Generation","Volume Rendering"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1161-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTE2MS1kb2MucGRmIiwiaWF0IjoxNzYwOTQzNzQzLCJleHAiOjE3OTI0Nzk3NDN9.di_sgq6h8fkFPYnphLave4nk3woBlBfC8cyj-RyxSY4","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1161","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f84ebbf0-98bb-4a65-9640","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"VolMoVis: Real-Time Volume Generation and Motion Visualization with Dynamic Tomographic Reconstruction","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"3d97e8c0-6d8b-4fe5-aa06-3ddf284b1a45","abstract":"Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I/O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization.","accessible_pdf":null,"authors":[{"affiliation":"University of Notre Dame","email":"syao2@nd.edu","name":"Siyuan Yao"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"3d97e8c0-6d8b-4fe5-aa06-3ddf284b1a45","image_caption":"Practitioners who would be interested in this paper:\n- Simulation scientists\n- Visualization researchers\n- Computer graphics and vision researchers\n- Medical imaging specialists\n- Data scientists working with 3D or time-varying data\n\nHow they can apply it:\n- Simulation scientists can visualize large, time-varying datasets in real time on low-end machines.\n- Visualization researchers can build on the segmentation and tracking methods for dynamic scenes.\n- Graphics and vision researchers can use the deformable 3D Gaussians for efficient scene representation.\n- Medical imaging specialists can adapt the segmentation methods for CT/MRI data.\n- Data scientists can apply the tracking tools to monitor changes in dynamic 3D datasets.","keywords":["Volume visualization","novel view synthesis","scene segmentation","segment tracking","deformable Gaussian splatting"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1535-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTUzNS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.EiKp6ljr8N-pX5aUmGDFoG3rW34Fei_dr1TCgGmEK-k","preprint_link":"https://arxiv.org/abs/2507.12667","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1535","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"3d97e8c0-6d8b-4fe5-aa06","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"cab72096-2336-4e27-a4e2-c416dd102259","abstract":"Within the nucleus of a cell, the genetic material (DNA) is condensed, supercoiled, and wrapped around proteins called histones, so to form chromatin. Unlike its traditional visualization, the organization of chromatin is 3D and can be measured with whole-genome sequencing technologies such as Hi-C sequencing, the results of which are often graphically represented as flat 2D heat-maps or a 1D curve that looks like a hairball. Here we discuss a novel approach to represent these and other genomic data by transforming the visualization of the genome from a 1D curve embedded in 3D into a structured 3D volume. This abstraction prioritizes 3D spatial proximity of chromatin and enables advanced techniques like clipping, thresholding, and contouring. Our method allows for multi-variable encoding, thereby enhancing our ability to interpret multivariate relationships, offering a more complete and effective representation of genomic structures, chromosome organization, and data relationships.","accessible_pdf":null,"authors":[{"affiliation":"Los Alamos National Laboratory","email":"bujack@lanl.gov","name":"Roxana Bujack"},{"affiliation":"Los Alamos National Laboratory","email":"dhr@lanl.gov","name":"David Rogers"},{"affiliation":"Los Alamos National Laboratory","email":"croth@lanl.gov","name":"Cullen Roth"},{"affiliation":"Los Alamos National Laboratory","email":"emsmall@lanl.gov","name":"Eric Small"},{"affiliation":"Los Alamos National Laboratory","email":"baners4@lanl.gov","name":"Banerjee Shounak"},{"affiliation":"Los Alamos National Laboratory","email":"vrindavenu7@gmail.com","name":"Vrinda Venu"},{"affiliation":"Los Alamos National Laboratory","email":"shawns@lanl.gov","name":"Shawn Starkenburg"},{"affiliation":"Los Alamos National Laboratory","email":"crsteadman@lanl.gov","name":"Steadman, Christina Rene"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"cab72096-2336-4e27-a4e2-c416dd102259","image_caption":"biologists, genomics researchers, epigenomics researchers","keywords":["Genome","volume","DNA","3D Hi-C","volumetric","abstraction."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1220-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyMjAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OCwiZXhwIjoxNzkyNDgyNTY4fQ.d0t5C-Qzdr24boBak8YXCmE2mMlvBC_fKz3HnMXo5dQ","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1220","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"cab72096-2336-4e27-a4e2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Volumetric Visualization of the Genome","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"f999c7ff-3a3e-4d54-a591-9d720a8d8244","abstract":"This paper introduces a timeline authoring tool that integrates Augmented Reality and tablet-based spatial interaction, enabling users to create large-scale non-linear timelines by leveraging spatial cognition and embodied interaction. In this system, the AR headset provides a large, immersive space for visualizing and interacting with a room-sized 3D timeline, while the tablet allows for precise sketching, annotation, and sculpting of timeline structures such as curves and branches. We conducted a design workshop to explore the user experience with this tool, which supports the creation of diverse expressive timelines for two datasets.","accessible_pdf":null,"authors":[{"affiliation":"University of Alabama in Huntsville","email":"ptv0002@uah.edu","name":"Veronica Vu"},{"affiliation":"UAH","email":"yr0011@uah.edu","name":"Yogesh Rai"},{"affiliation":"University of Alabama in Huntsville","email":"hc0021@uah.edu","name":"Haeyong Chung"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"f999c7ff-3a3e-4d54-a591-9d720a8d8244","image_caption":"Visualization and mixed-reality designers, developers, and researchers","keywords":["Timeline","authoring","mobile","immersive analytics."],"open_access_supplemental_link":null,"open_access_supplemental_question":"This work introduces a novel hybrid AR system that supports the authoring of large-scale, expressive timeline visualizations through tablet-based spatial interaction and immersive AR space.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1243-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEyNDMtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.wNaKGfC0vIqiqtRBzva-p09RluXyYcSSaNc180uqDso","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1243","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"f999c7ff-3a3e-4d54-a591","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Walking Through Time: A Hybrid Immersive System for Spatial and Expressive Timeline Authoring","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"ba87a101-1d66-45b7-91a0-0410f31e50fc","abstract":"Managing water resources within California\u2019s Central Valley is a complex task involving analyzing a myriad of factors e.g. climate, demand, and government policies. This results in opaque management decisions which are concentrated to a small group of people while the majority of the valley is left out of the decision-making process, leading to inequitable distribution. This paper assesses the design for Watering the Future, an application aiming to educate non-experts in water management about water distribution systems by illustrating regional water supplies and the factors behind them in a more understandable manner. We sought feedback from a domain expert in water management and conducted a user study to assess the effectiveness of our graph designs, of which results suggested increased levels of engagement while still communicating the required information, suggesting an enhanced level of accessibility to the unfamiliar field of water management.","accessible_pdf":null,"authors":[{"affiliation":"University of California, Davis","email":"sayuniar@ucdavis.edu","name":"Sarah Yuniar"},{"affiliation":"University of California, Davis","email":"ykawakami@ucdavis.edu","name":"Yuya Kawakami"},{"affiliation":"University of California, Davis","email":"log.gstang@gmail.com","name":"Logan Tang"},{"affiliation":"University of California, Davis","email":"ma@cs.ucdavis.edu","name":"Kwan-Liu Ma"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"ba87a101-1d66-45b7-91a0-0410f31e50fc","image_caption":"We believe data scientists and journalists who regularly design public-facing visualizations would be interested in our paper. Since complex data unfamiliar to the general public may be incorporated into visualizations designed by these practitioners, it would be paramount to collate strategies in visualizing such data in a way both informative and engaging to the audience. Our paper navigates the design process for such a visualization and discusses pitfalls as well as strategies in formulating a public-facing visualization for an unfamiliar field which we believe data scientists and journalists can integrate into their design workflow.","keywords":["Visualization design study","public-facing visualization","visual metaphor"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1310-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzMTAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2OSwiZXhwIjoxNzkyNDgyNTY5fQ.HndEOIFq4IkaDH4JxC4dW78x7feibgT6XkyM-7UPvRY","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1310","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"ba87a101-1d66-45b7-91a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Watering the Future: Assessing a Visualization Design for Accessible Comprehension of Water Management","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"b00813fe-c0fb-4bdb-8040-b6e338088247","abstract":"Humans inherently connect certain colors with particular concepts in semantically meaningful ways that facilitate visual communication. These colors are known as semantically resonant colors. For instance, we associate \u201csky\u201d and \u201cocean\u201d with shades of blue, and \u201ccherry\u201d with red. In this paper, we investigate how language models, including Word2Vec, RoBERTa, GPT-4o mini and the vision language model CLIP generate and represent nuanced semantically resonant colors for diverse concepts. To achieve this, we utilized a large dataset of color names and concepts, tailored models for the structure of each language model, and developed an interactive web interface, CONCEPT2COLOR, as a use case. Additionally, we conducted experiments and a detailed analysis to assess the ability of these models to generate meaningful colors. Through these experiments, we examined how factors such as model design, training data and context affect the color output. Our findings reveal the capabilities and limitations of language models in processing and generating semantically resonant colors for concepts, thus contributing insights into how they depict semantic color-concept connections. These insights have implications for data visualization, design, and human-computer interaction, where leveraging effective semantic color generation can enhance communication and user experience.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"ssalimaunti@cs.stonybrook.edu","name":"Shahreen Salim"},{"affiliation":"Stony Brook University","email":"tpial@cs.stonybrook.edu","name":"Tanzir Pial"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"b00813fe-c0fb-4bdb-8040-b6e338088247","image_caption":"This work will be of interest to anyone who needs to pick or evaluate color palettes for communication and design, including:\n\nInformation & Data Visualization Designers (in academia or industry), who want to automatically generate palettes that align with the semantics of their data categories.\n\nData Journalists & Reporters, who need on-the-fly, context-aware colors for charts and infographics to make stories more intuitive and engaging.\n\nUI/UX Designers & Front-end Developers, who can use semantic color mapping to theme components (alerts, statuses, notifications) based on message content or user sentiment.\n\nEducators & Instructional Designers, who create learning materials or e-learning modules and want color cues that reinforce key concepts or vocabulary.\n\nBranding & Marketing Professionals, who often start from keywords like \u201ctrust\u201d or \u201ceco-friendly\u201d and need to translate those into on-brand color palettes quickly.\n\nAccessibility & Assistive-Tech Specialists, who can augment communication aids (e.g., emotion cards, schedules) with semantically meaningful colors to reduce ambiguity for neurodivergent or visually impaired users.","keywords":["Tabular Data","Text/Document Data","Datasets","Methodologies","Software Prototype","Domain Agnostic","Color Machine Learning Techniques"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1506-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTUwNi1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM0LCJleHAiOjE3OTI0ODA1MzR9.RHtQ9p5gLNhMyIaq_A8fOEoBvyhNpndUme-jCO9KsVM","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1506","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"b00813fe-c0fb-4bdb-8040","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"What is the Color of Serendipity? Investigating the Use of Language Models for Semantically Resonant Color Generation","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"5cf45946-05e6-4d9c-876e-abeacdf5b8e1","abstract":"We investigate the perceived visual complexity (VC) in data visualizations using objective image-based metrics. We collected VC scores through a large-scale crowdsourcing experiment involving 349 participants and 1,800 visualization images. We then examined how these scores align with 12 image-based metrics spanning pixel-based and statistic-information-theoretic (clutter), color, shape, and our two new object-based metrics (meaningful-color-count (MeC) and text-to-ink ratio (TiR)). Our results show that both low-level edges and high-level elements affect perceived VC in visualization images; the number of corners and distinct colors are robust metrics across visualizations. Second, feature congestion, a statistical information-theoretic metric capturing color and texture patterns, is the strongest predictor of perceived complexity in visualizations rich in the same continuous color/texture stimuli; edge density effectively explains VC in node-link diagrams. Additionally, we observe a bell-curve effect for texts: increasing TiR initially reduces complexity, reaching an optimal point, beyond which further text increases VC. Our quantification model is also interpretable - enabling metric-based explanations - grounded in the VisComplexity2K dataset, bridging computational metrics with human perceptual responses. The preregistration is available at osf.io/5xe8a. osf.io/bdet6 has the dataset and analysis code.","accessible_pdf":null,"authors":[{"affiliation":"The Ohio State University","email":"chu.752@osu.edu","name":"Mengdi Chu"},{"affiliation":"The Ohio State University","email":"qiu.573@osu.edu","name":"Zefeng Qiu"},{"affiliation":"The Ohio State University","email":"ling.253@osu.edu","name":"Meng Ling"},{"affiliation":"The Ohio State University","email":"jiang.2126@osu.edu","name":"Shuning Jiang"},{"affiliation":"University of Nottingham","email":"robert.laramee@nottingham.ac.uk","name":"Robert Laramee"},{"affiliation":"University of Stuttgart","email":"michael.sedlmair@visus.uni-stuttgart.de","name":"Michael Sedlmair"},{"affiliation":"The Ohio State University","email":"chen.8028@osu.edu","name":"Jian Chen"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"5cf45946-05e6-4d9c-876e-abeacdf5b8e1","image_caption":"Visualization designers, data scientists, data journalists.","keywords":["Perceived visual complexity","image-based metrics","scene-like","text-ink-ratio","meaningful-color-count"],"open_access_supplemental_link":"https://osf.io/bdet6/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1919-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTkxOS1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0OTM5LCJleHAiOjE3OTI0ODA5Mzl9.Tc2niosZBCSLTjV5FD5akx4I48wRI0zLXenXlRI9AN8","preprint_link":"https://osf.io/preprints/osf/ypez4","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1919","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"5cf45946-05e6-4d9c-876e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"What Makes a Visualization Image Complex?","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"943a0aa8-b141-48cd-90a1-9a899d531ffe","abstract":"We describe a multidisciplinary collaboration to iteratively design an interactive exhibit for a public science center on paleoclimate, the study of past climates. We created a data physicalisation of mountains and ice sheets that can be tangibly manipulated by visitors to interact with a wind simulation visualisation that demonstrates how the climate of North America differed dramatically between now and the peak of the last ice age. We detail the system for interaction and visualisation plus design choices to appeal to an audience that ranges from children to scientists and responds to site requirements.","accessible_pdf":null,"authors":[{"affiliation":"University of Colorado Boulder","email":"dahu8258@colorado.edu","name":"David Hunter"},{"affiliation":"Mechanical Engineering","email":"pablo.botin@colorado.edu","name":"Pablo Botin"},{"affiliation":"UCAR","email":"emilysb@ucar.edu","name":"Emily Snode-Brenneman"},{"affiliation":"UCAR","email":"asteverm@ucar.edu","name":"Amy Stevermer"},{"affiliation":"UCAR|NCAR","email":"hatheway@ucar.edu","name":"Becca Hatheway"},{"affiliation":"NOAA","email":"dillon.amaya@noaa.gov","name":"Dillon Amaya"},{"affiliation":"Science Communicator","email":"eddie@eddiegoldstein.com","name":"Eddie Goldstein"},{"affiliation":"University of Colorado Boulder","email":"wayne.seltzer@colorado.edu","name":"Wayne Seltzer"},{"affiliation":"University of Colorado Boulder","email":"mdgross@colorado.edu","name":"Mark Gross"},{"affiliation":"CIRES","email":"kristopher.karnauskas@colorado.edu","name":"Kris Karnauskas"},{"affiliation":"Cornell University","email":"daniel.leithinger@cornell.edu","name":"Daniel Leithinger"},{"affiliation":"University of Colorado Boulder","email":"ellen.do@colorado.edu","name":"Ellen Yi-Luen Do"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"943a0aa8-b141-48cd-90a1-9a899d531ffe","image_caption":null,"keywords":["visualization; physicalization; tangible interaction; exhibit design;"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1111-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzExMTEtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1MiwiZXhwIjoxNzkyNDc1ODUyfQ.PVDuFK-rr05CCsI05WmbUxOZKll2OgXW8fn0ghEj8iI","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1111","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"943a0aa8-b141-48cd-90a1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Winds Through Time: Interactive Data Visualization and Physicalization for Paleoclimate Communication","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"4cf2c4a7-98c7-4d49-a5c2-410ecde134cb","abstract":"A growing body of work on visualization affordances highlights how specific design choices shape reader takeaways from information visualizations. However, mapping the relationship between design choices and reader conclusions often requires labor-intensive crowdsourced studies, generating large corpora of free-response text for analysis. To address this challenge, we explored alternative scalable research methodologies to assess chart affordances. We test four elicitation methods from human-subject studies: free response, visualization ranking, conclusion ranking, and salience rating, and compare their effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps. Overall, we find that while no method fully replicates affordances observed in free-response conclusions, combinations of ranking and rating methods can serve as an effective proxy at a broad scale. The two ranking methodologies were influenced by participant bias towards certain chart types and the comparison of suggested conclusions. Rating conclusion salience could not capture the specific variations between chart types observed in the other methods. To supplement this work, we present a case study with GPT-4o, exploring the use of large language models (LLMs) to elicit human-like chart interpretations. This aligns with recent academic interest in leveraging LLMs as proxies for human participants to improve data collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience rating methodology but suffered from severe constraints in other areas. Overall, the discrepancies in affordances we found between various elicitation methodologies, including GPT-4o, highlight the importance of intentionally selecting and combining methods and evaluating trade-offs.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of California Berkeley","email":"chase_stokes@berkeley.edu","name":"Chase Stokes"},{"affiliation":"Georgia Institute of Technology","email":"klin368@gatech.edu","name":"Kylie Lin"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"4cf2c4a7-98c7-4d49-a5c2-410ecde134cb","image_caption":"Practitioners who compare alternative designs (e.g., user experience researchers) would be interested in reading this paper, since it discusses the trade offs and different findings from different methodologies. For a similar reason, data scientists and similar roles may also be interested. They could extract important insights about specific methodological choices that can affect how participants respond to their designs.","keywords":["Information visualizations","affordances","methodology","conclusions","large-language models."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1507-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTUwNy1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0NTM1LCJleHAiOjE3OTI0ODA1MzV9.K9b2PJVEVo4Vjmg4ayxGDNPM7Po0RGo9ujSMQK5qxZA","preprint_link":"https://arxiv.org/abs/2507.17024","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1507","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"4cf2c4a7-98c7-4d49-a5c2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"258e8105-5eb0-4316-99bc-40522d2b6d15","abstract":"Causality helps people reason about and understand complex systems, particularly through what-if analyses that explore how interventions might alter outcomes. Although existing methods embrace causal reasoning using interventions and counterfactual analysis, they primarily focus on effects at the population level. These approaches often fall short in systems characterized by significant heterogeneity, where the impact of an intervention can vary widely across subgroups. To address this challenge, we present XplainAct, a visual analytics framework that supports simulating, explaining, and reasoning interventions at the individual level within subpopulations. We demonstrate the effectiveness of XplainAct through two case studies: investigating opioid-related deaths in epidemiology and analyzing voting inclinations in the presidential election.","accessible_pdf":"Accessible","authors":[{"affiliation":"Stony Brook University","email":"yanming.zhang@stonybrook.edu","name":"Yanming Zhang"},{"affiliation":"Stony Brook University","email":"khegde@cs.stonybrook.edu","name":"Krishnakumar Hegde"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":true,"has_pdf":true,"id":"258e8105-5eb0-4316-99bc-40522d2b6d15","image_caption":"1.Data Scientists and Machine Learning Engineers\nApplication: They can integrate XplainAct\u2019s ideas into their own pipelines to interpret and validate models more effectively, especially in applications where local explanations are as important as global patterns.\n\n2.Public Health Researchers and Epidemiologists\nApplication: Use XplainAct to identify region-specific or subgroup-specific causal drivers and simulate policy interventions (e.g., improving mental health resources) in a way that accounts for local heterogeneity.\n\n3.Political Analysts and Social Scientists\nApplication: Campaign strategists or sociopolitical researchers can simulate policy changes or demographic shifts and assess how they might influence election outcomes in targeted regions.","keywords":["Explainable AI","Causality","Visual Analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-short/1350-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LXNob3J0LzEzNTAtZG9jLnBkZiIsImlhdCI6MTc2MDk0NjU2NiwiZXhwIjoxNzkyNDgyNTY2fQ.gZ-RCVAme52nVWTFg3J5i_UXmyxP42BTj8QdBKLYPXU","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1350","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"258e8105-5eb0-4316-99bc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"XplainAct: Visualization for Personalized Intervention Insights","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"10595e17-8ad8-4932-b661-e5d6764c8082","abstract":"You Only Have Seven Seconds is a cinematic artistic visualization derived from an interactive AI art installation (ReCollection) that explores how machine intelligence can reassemble collective human memory through language input. Motivated by the artist\u2019s personal response to her grandmother\u2019s cognitive decline and informed by current research in Critical Dementia Studies that advocates for reimagining\u2014rather than repairing\u2014memory, the project collects and curates whispered, seven-second recollections and machine-generated image data from installation participants. These selected datasets are transformed into a generative visualization presented as\na cinematic art using custom-designed AI systems and experimental visualization strategies. This paper introduces the conceptual foundations and technical development of this project, with emphasis on data collection, experimental visualization, narrative construction, sound design, and intelligent system integration. Through the design of artificial memory, You Only Have Seven Seconds constructs a dynamic archive of shared, ephemeral recollections for storytelling.","accessible_pdf":null,"authors":[{"affiliation":"university of california, Santa Barbara","email":"zhangweidilydia@gmail.com","name":"weidi zhang"},{"affiliation":"The University of Sheffield","email":"lijiaozi.cheng@gmail.com","name":"Lijiaozi Cheng"},{"affiliation":"Zurich University of the Arts","email":"paulschmidt786@gmail.com","name":"Paul Schmidt"},{"affiliation":"Minus AI","email":"rodgerljl@msn.com","name":"Jieliang Luo"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"10595e17-8ad8-4932-b661-e5d6764c8082","image_caption":null,"keywords":["Generative Design","Experimental Data Visualization","Computer Animation","AI Art."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/a-visap/1076-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy9hLXZpc2FwLzEwNzYtZG9jLnBkZiIsImlhdCI6MTc2MDkzOTg1MywiZXhwIjoxNzkyNDc1ODUzfQ.PAyeHj_CBs3D65P57dr5Hl2UC1j7oWQ-ra8QSWbJURw","preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1076","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"10595e17-8ad8-4932-b661","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"You Only Have Seven Seconds: From Intimate Whispers to Shared Worlds in Participatory Data-Driven Cinematic Art","youtube_ff_id":null,"youtube_ff_url":null},{"UID":"1b3b1b15-e378-4c65-8984-717a7f7baa3a","abstract":"Systems relying on ML have become ubiquitous, but so has biased behavior within them. Research shows that bias significantly affects stakeholders\u2019 trust in systems and how they use them. Further, stakeholders of different backgrounds view and trust the same systems differently. Thus, how ML models\u2019 behavior is explained plays a key role in comprehension and trust. We survey explainability visualizations, creating a taxonomy of design characteristics. We conduct user studies to evaluate five state-of-the-art visualization tools (LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how taxonomy characteristics affect comprehension, bias perception, and trust for non-expert ML users. Surprisingly, we find an inverse relationship between comprehension and trust: the better users understand the models, the less they trust them. We investigate the cause and find that this relationship is strongly mediated by bias perception: more comprehensible visualizations increase people\u2019s perception of bias, and increased bias perception reduces trust. We confirm this relationship is causal: Manipulating explainability visualizations to control comprehension, bias perception, and trust, we show that visualization design can significantly (p < 0.001) increase comprehension, increase perceived bias, and reduce trust. Conversely, reducing perceived model bias, either by improving model fairness or by adjusting visualization design, significantly increases trust even when comprehension remains high. Our work advances understanding of how comprehension affects trust and systematically investigates visualization\u2019s role in facilitating responsible ML applications.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Massachusetts Amherst","email":"zhannakaufma@umass.edu","name":"Zhanna Kaufman"},{"affiliation":"University of Massachusetts Amherst","email":"mendres@umass.edu","name":"Madeline Endres"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"},{"affiliation":"University of Massachusetts","email":"brun@cs.umass.edu","name":"Yuriy Brun"}],"award":"","doi":null,"event_id":"","event_title":"","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1b3b1b15-e378-4c65-8984-717a7f7baa3a","image_caption":"Data scientists, developers who work with machine learning systems, and developers or researchers working in ML explainability would be interested in reading this paper. This paper provides insight into how people perceive information about the ML systems they interact with, and how different ways of presenting this information can influence their trust in the underlying model.","keywords":["Visualization design","explainability","trust","bias in machine learning."],"open_access_supplemental_link":"https://osf.io/c87xm/?view_only=31dfc1f2a7624f5cb20b0f07d3730df3","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":"https://data.tech.ieeevis.org/storage/v1/object/sign/vis2025/pdf-files/v-full/1446-doc.pdf?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJ2aXMyMDI1L3BkZi1maWxlcy92LWZ1bGwvMTQ0Ni1kb2MucGRmIiwiaWF0IjoxNzYwOTQ0MzQyLCJleHAiOjE3OTI0ODAzNDJ9.JVojo_lx3GSTyjShMn4ykEEeZ5jsNBZsovofjY_NEVg","preprint_link":"https://arxiv.org/abs/2508.00140","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1446","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"","session_room":"","session_room_id":null,"session_title":"","session_uid":"1b3b1b15-e378-4c65-8984","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":[],"time_stamp":"","title":"Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models","youtube_ff_id":null,"youtube_ff_url":null}]
